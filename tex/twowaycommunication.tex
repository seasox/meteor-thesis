\chapter{Adapting the Meteor Stegosystem for Two-Way Communication}
\label{chap:twowaycommunication}


In the Prisoners' Problem \cite{Simmons1983}, a unidirectional communication is described.
The sending party Alice embeds a hidden message in a covert channel, the receiving party Bob decodes the received message.
The Meteor stegosystem adopts this approach of receiving and sending parties.
But in most practical applications, a bidirectional communication channel between participants is needed.
For example, in client-server architectures such as HTTP or key exchange protocols such as Diffie-Hellman, the participants exchange messages bidirectionally.
Also, one of the most commonly used mediums for text messages is instant messaging, which is characterized by a continuous back-and-forth of messages between participants.

Since the Meteor stegosystem is constructed to be very adaptable to different underlying distributions (as long as they are RRRSS), we can modify Meteor to generate stegotext messages for use in the context of instant messaging.
In this chapter, we will introduce a simple protocol for two-way communication using Meteor and adapt Meteor to use DialoGPT \cite{Zhang2020}, a GPT-based generative model which is trained to generate text messages as they could appear in a chat.

\section{Protocol for Two-Way Communication}

In \autoref{fig:twowaycommunication}, we introduce a simple chat protocol between two participants Alice and Bob.
Here, Alice and Bob send messages back and forth.
Alice sends stegotext blocks, while Bob generates a sequence of responses, which can be stegotexts or handwritten messages.
After each message from Alice, Bob uses the Meteor stegosystem to decode $s_i$ to a message block $m_i$.
After $\ell$ iterations, the entire stegotext has been sent.
Now, Bob can recover the hiddentext $m$ by concatenating the decoded blocks $m_i$.


\begin{figure}[htbp]
  \centering
  \begin{msc}[instance distance=4cm,action width=5cm]{Two-Way Steganographic Communication Protocol}
    \declinst{alice}{}{Alice}
    \declinst{bob}{}{Bob}
    \condition{exchange key $k$ and history $h$}{alice,bob}
    \nextlevel[2]
    \action{split hiddentext $m$ in $\ell$ blocks $m_i$}{alice}
    \nextlevel[3]
    \condition{for $i \in \{ 1, 2, \dots, \ell \}$ do}{alice,bob}
    \nextlevel[2]
    \action{$s_i = Encode_{\mathcal{C}}^\beta(m_i, k, h)$}{alice}
    \nextlevel[3]
    \mess{$s_i$}{alice}{bob}
    \nextlevel
    \action{$m_i = Decode_{\mathcal{C}}^\beta(s_i, k, h)$}{bob}
    \nextlevel[2]
    \action{Generate response $r$}{bob}
    \nextlevel[3]
    \mess{$r$}{bob}{alice}
    \nextlevel
    \action{$h \leftarrow h || r$}{alice}
    \action{$h \leftarrow h || r$}{bob}
    \nextlevel[2]
    \condition{endfor}{alice,bob}
    \nextlevel[3]
    \action{$m = m_1 || m_2 || \dots || m_{\ell}$}{bob}
    \nextlevel
  \end{msc}
  \caption{
  A Two-Way Steganographic Communication Protocol for participants Alice and Bob.
  In this scheme, Alice encodes blocks of her hiddentext message $m$ into stegotexts $s_i$ and sends them to Bob, who decodes the blocks using the Meteor stegosystem.
  Bob then generates his response $r$ and sends it back.
  The response can be generated using a GNN, be (part of) a stegotext, or handwritten chat messages.
  After $\ell$ iterations, Bob can reconstruct the hiddentext $m$ by concatenating the $m_i$.
  }
  \label{fig:twowaycommunication}
\end{figure}

\section{Implementation of Two-Way Communication}
While we can generate stegotexts for two-way communication using the GPT-2 model, as the original Meteor stegosystem does, the texts generated with GPT-2 are not convincing as chat messages. 
In the following example, we see a conversation between Alice and Bob using the protocol described in \autoref{fig:twowaycommunication}.
The initial history consists of a short sequence of messages where both participants greet and ask each other how their respective day was.
Alice's messages encode the hiddentext ``Hi there!'' in blocks of 32 bits, while Bob's responses are generated manually.

\begin{example}[Meteor Conversation for hiddentext ``Hi there!'' using GPT-2 with 32 bit blocksize]
\leavevmode
\begin{itemize}
\item Alice: I've been trying to get my hands on a couple of these since they were first released last year, and it's been a fun time. It's been a long time since I've had the chance to play through
\item Bob: What games do you like to play?
\item Alice: "It's a long haul, but it's a long haul," said Steve Young, the former director of the FBI's counter
\item Bob: Are you affiliated with the FBI?
\item Alice: How To Use
\end{itemize}
\end{example}

As we can see, the generated text does not read as if it was an actual conversation between friends.
Also, the generated texts appear to lack context.
Alice's texts don't seem to take Bob's responses into account and cover quite different topics in each iteration.
This effect is due to the nature of GPT's training dataset called WebText, which is a large undisclosed collection of online newspaper articles, books, blog posts, and the like.
Therefore, it is very likely that GPT generates probability distributions for article-like texts.
When sampling from these distributions, it is likely to generate text that reads like a newspaper article as well.

Fortunately, the NLP community is very active in building different models for many use cases.
In 2019, DialoGPT, a pre-trained transformer trained on public multi-turn dialogues from Reddit threads, has been published \cite{Zhang2020}.
As the name suggests, DialoGPT is a derivation of GPT that is trained and fine-tuned for dialogue generation.
When repeating the same experiment as above, but with the DialoGPT model as a drop-in replacement for GPT-2, the results read more like what we would expect from a chat conversation between friends.
Again, Alice's text messages encode the hiddentext ``Hi there!'' with 32 bits of encoded data per chat message.
Bob's responses are handwritten.
While a little bit incoherent, the transcript is closer to an actual conversation between friends.

\begin{example}[Meteor Conversation for hiddentext ``Hi there!'' using DialoGPT with 32 bit blocksize]
\leavevmode
\begin{itemize}
\item Alice: I suppose it is . I'm a bit jealous of the weather today , though . It's nice , but it's nice to be in a nicer
\item Bob:   I don't know, it's too hot for me today to be honest :D
\item Alice: Yeah it is . I wish it was cooler though . I love it here . I wish I had another place to live though . We're all pretty cool here . Lol
\item Bob:   haha you're cool for sure
\item Alice: I love it here as
\item Bob:   Where are you currently?
\end{itemize}


\end{example}