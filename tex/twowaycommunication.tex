\chapter{Two-Way Communication}
\label{chap:twowaycommunication}

The Meteor Stegosystem is meant to generate text messages to send via an unsecure channel.
Usually, when two parties communicate, the communication consists of a back and forth between participants.
In the original Meteor project we are presented with a unidirectional communication.
The sending party generates a stegotext message and sends it to the receiving party.

Wouldn't it be great if we could adapt the Meteor stegosystem to generate messages as they would appear in an ordinary chat?
Since the Meteor stegosystem is constructed to be very adaptable to different underlying distributions (as long as they are a RRRSS scheme), this certainly is possible.
To achieve the task at hand, we can use Microsoft's DialoGPT \cite{Zhang2020} to generate stegotext messages using a GNN optimized for chat-like text generation.

\section{Protocol For Two-Way Communication}

In \autoref{fig:twowaycommunication}, we introduce a simple chat protocol between two participants Alice and Bob.
Here, Alice and Bob send messages back and forth.
Alice sends stegotext blocks, while Bob generates a sequence of responses, which can be stegotexts too or handwritten messages.
After each message from Alice, Bob uses the Meteor stegosystem to decode $s_i$ to a message block $m_i$.
After $\ell$ iterations, the entire stegotext has been sent.
Now, Bob can recover the hiddentext $m$ by concatenating the decoded blocks $m_i$.


\begin{figure}[htbp]
	\centering
	\begin{msc}[instance distance=4cm,action width=5cm]{Two-Way Steganographic Communication Protocol}
		\declinst{alice}{}{Alice}
		\declinst{bob}{}{Bob}
		\condition{exchange $k,~ \mathcal{H}$}{alice,bob}
		\nextlevel[2]
		\action{split hiddentext $m$ in $\ell$ blocks $m_i$ of size $b$}{alice}
		\nextlevel[3]
		\condition{for $i \in \{ 1, 2, \dots, \ell \}$ do}{alice,bob}
		\nextlevel[2]
		\action{$s_i = Encode_{\mathcal{M}}^\beta(m_i, k, \mathcal{H})$}{alice}
		\nextlevel[3]
		\mess{$s_i$}{alice}{bob}
		\nextlevel
		\action{$m_i = Decode_{\mathcal{M}}^\beta(s_i, k, \mathcal{H})$}{bob}
		\nextlevel[2]
		\action{Generate response $r$}{bob}
		\nextlevel[3]
		\mess{$r$}{bob}{alice}
		\nextlevel
		\action{$\mathcal{H} \leftarrow \mathcal{H} || r$}{alice}
		\action{$\mathcal{H} \leftarrow \mathcal{H} || r$}{bob}
		\nextlevel[2]
		\condition{endfor}{alice,bob}
		\nextlevel[3]
		\action{$m = m_1 || m_2 || \dots || m_{\ell}$}{bob}
		\nextlevel
	\end{msc}
	\caption{
	A Two-Way Steganographic Communication Protocol for participants Alice and Bob.
	In this scheme, Alice encodes blocks of their hiddentext message $m$ into stegotexts $s_i$ and sends them to Bob, who decodes the blocks using the Meteor stegosystem.
	Bob then generates their response $r$ and sends it back.
	The response can be generated using a GNN, be (parts of) a stegotext themselves or handwritten chat messages.
	After $\ell$ iterations, Bob can reconstruct the hiddentext $m$ by concatenating the $m_i$.
	}
	\label{fig:twowaycommunication}
\end{figure}

\section{Implementation Of Two-Way Communication}
While we can generate stegotexts for two-way communication using the GPT-2 model, as the original Meteor stegosystem does, the texts generated with GPT-2 are not really convincing as chat messages. 
In the following example we see a conversations between Alice, the encoding party, and Bob, the decoding party using the protocol described in \autoref{fig:twowaycommunication}.
The initial history consists of a short sequence of messages where both participants greet and ask each other how their day was.
Alice's messages encode the hiddentext ``Hi there!'' in blocks auf 32 bits. 
Bob's responses are generated by hand.

\begin{example}[Meteor Conversation for hiddentext ``Hi there!'' using GPT-2 with 32 bit blocksize]
\leavevmode
\begin{itemize}
\item Alice: I have a secret. I was working on an app for our Google
\item Bob:   Oh interesting! But are you really supposed to tell me? hah
\item Alice: "We're on the verge of a major shift in the way people think about climate change," says Steve McIntyre, an environmental economist and
\item Bob:   That's right, climate change is a huge problem
\item Alice: Description \\
\\
       This
\item Bob:   What?
\end{itemize}
\end{example}

As we can see, the generated text does not really read as if it were an actual conversation between two friends.
The generated text appears to lack context.
Also, Alice's texts don't seem to take Bob's responses into account and cover different topics in each iteration.

This effect is due to the nature of GPT's training dataset, which is a large collection of newspaper reports, books, articles and the like.
Therefore, it is very likely that GPT generates probability distributions for article-like texts.
When sampling from this distribution, it is likely to generate text that reads like a newspaper article as well.

Luckily, the NLP community is very active in developing different models for many use cases.
In 2020, Microsoft has developed and released the DialoGPT \cite{Zhang2020}, a pre-trained transformer trained on public multi-turn dialogues from Reddit threads.
As the name suggests, DialoGPT is a derivation of GPT.
When repeating the same experiment as above, but with the DialoGPT model, the results read more closely to what we would expect from a chat conversation between two friends.
Again, Alice's text messages encode the hiddentext ``Hi there!'', 32 bits of encoded data per chat message.
Bob's responses are handwritten.

\begin{example}[Meteor Conversation for hiddentext ``Hi there!'' using DialoGPT with 32 bit blocksize]
\leavevmode
\begin{itemize}
\item Alice: I suppose it is . I'm a bit jealous of the weather today , though . It's nice , but it's nice to be in a nicer
\item Bob:   I don't know, it's too hot for me today to be honest :D
\item Alice: Yeah it is . I wish it was cooler though . I love it here . I wish I had another place to live though . We're all pretty cool here . Lol
\item Bob:   haha you're cool for sure
\item Alice: I love it here as
\item Bob:   Where are you currently?
\end{itemize}


\end{example}