\chapter{Conclusion}
\label{chap:conclusion}


% In a German thesis write: \subsection{Zusammenfassung und Ausblick}


% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
% !!! Your action is needed here !!!
% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%
% Replace the following with your conclusion

We have discussed Meteor, an innovative stegosystem which utilizes the random sampling used in generative neural networks to embed a hiddentext into the cover channel.
It achieves that by replacing the randomness used for sampling with an encryption of the hiddentext.

If the encrypted hiddentext is indistinguishable from randomness, this process is secure against chosen hiddentext attackers.
In \autoref{chap:security}, we have seen that this is the case if an attacker is allowed to send up to one query to the SS-CHA game defined by \cite{Hopper2004}.
Unfortunately, Meteor's security guarantee does not hold for attackers which can send polynomial queries, because Meteor's output is deterministic.
To fix this, we have discussed a modification to the Meteor stegosystem which adds an initialization vector to the pseudorandom function used for encryption while maintaining indistinguishability from  randomness.

We have also modified Meteor for two-way communication as used in instant messaging.
While the texts generated by GPT-2 are not very convincing chat messages, we can easily adopt Meteor for other GNN models, as long as they satisfy the RRRSS property defined in \autoref{def:rrrss}.
We then have adopted Meteor to use the DialoGPT model.
The stegotexts generated with the DialoGPT model more closely resemble those found in an instant messaging.

We have also discussed correctness of the Meteor stegosystem and found that it sometimes generates stegotexts which cannot be decoded correctly, even if the decoding party knows the key used to generate the stegotext in \autoref{chap:correctness}.
These decoding errors happen more frequently the longer the hiddentext is.
This problem occurs due to subword tokenization, a feature of modern generative neural networks which greatly increases model performance.
We have discussed a modification to the Meteor stegosystem which fixes these decoding errors.
This modification first tries to detect an encoding error by using a checksum and, in case a decoding error occurs, generates every possible decomposition of the stegotext using the tokens associated with the model.

While the Meteor authors have discussed that the Meteor stegosystem will be adaptable to GPT-3 once it is released, OpenAI unfortunately did not yet decide to release the model publicly and only allows limited access to GPT-3 using a private (and paid) online API.
While it would technically still be possible to use this private API, its practical application in a stegosystem is limited.
Also, the occurring costs of US\$ 0.15 to US\$ 7.68 per megabyte of hiddentext (US\$ 0.0004 to US\$ 0.02 per 1000 tokens with an average of 3 bits of hiddentext per token) are a problem in practical use \cite{OpenAIPricing2022}.

Luckily, other researches have built and trained open-source alternatives to GPT-3.
One collective that stands out in the NLP community is EleutherAI who trained and released GPT-NeoX, a generative model with 20 billion parameters, which can arguably compete with GPT-3 in prediction quality \cite{GPTneo2022}.
Also, GPT-neoX seems to be compatible with GPT-2, since it uses the same encoder and token dictionary.
Further research could incorporate GPT-neoX as an alternative model for use with the Meteor stegosystem.
First experiments conducted for this thesis have concluded that text generation with large models, i.e. models with parameters an order of magnitude greater than GPT-2's 345 million parameters, requires a considerable amount of computational power which state-of-the-art laptop cannot deliver without hardware acceleration.

The Meteor stegosystem establishes a very promising approach for using generative neural networks for cryptographically secure steganographic communication.
Since it can be expected that computational performance in machine learning will continue to improve in the coming years, steganography on ML models is likely to remain a promising area of research.