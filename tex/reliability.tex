\chapter{Improving the Reliability of the Meteor Stegosystem}
\label{chap:reliability}

The Meteor authors have released an online demo of Meteor \cite{MeteorDemo2021}.
On the website, they state that ``due to issues with the GPT-2 algorithm interface, you sometimes may see extra output from a decoded stegotext. This does not impact the underlying security of the scheme''.
After experimentation, we conclude that the cause of this is not due to issues with the algorithm interface but to the way GNNs with subword tokenization such as GPT-2 tokenize text.

In this chapter, we will, after describing the issue at hand, show that the Meteor Stegosystem is indeed unreliable by providing a counterexample.
Afterwards, we will discuss an experiment that approximates the probability of incorrect decodings.
The experiment shows that for sufficiently long hiddentexts the probability of incorrect decodings increases to approximately 87 \%.
To improve reliability, we present a change to Meteor's $Decode$ algorithm to increase the probability of successfully recovering the hiddentext message using a DFS-based graph search while -- in the worst case -- introducing computational overhead exponential in stegotext length.

\section{Ambiguous Tokenization}
\label{sec:amb-tok}

In \autoref{sec:generative-neural-networks}, we discussed that GNNs use a set of tokens to represent text.
To improve the quality of the generated text, models (not only) in the GPT-n family use subword tokenization.
Subword tokenization is opposed to word tokenization, where text is split by spaces, and character-based tokenization, where text is tokenized as a sequence of characters.
Subword tokenization allows for more effective machine learning, as the model can easier connect related words.
This technique proved to be very effective in enhancing GNN performance in the last few years.

On the other hand, this allows for one word to have multiple tokenizations because subword tokens are usually not prefix-free.
For example, the word ``doesn't'' might be tokenized as ``do$||$es$||$n't'', but it could also be tokenized as ``does$||$n't'' or even ``d$||$o$||$e$||$s$||$n$||$'$||$t''.
This situation is what we call \emph{ambiguous tokenization}.
If Alice generates a stegotext that has an ambiguous tokenization and Bob choses a different tokenization than Alice, the decoding will fail because a wrong prefix of the sample used to generate a token will be recovered from the stegotext.

For party $P \in \{A,B\}$ we denote by $T_P$ the tokenizer used to generate (encoding party $A$) or parse (decoding party $B$) a stegotext $c \in \Delta^*$ as a sequence of tokens $T_P(c) = t_1||t_2||\dots||t_{|t|} \in \mathcal{T}^*$.
To measure the difference in tokenizations of a given stegotext between Alice and Bob, we define a distance function $D$ that measures the number of mismatching subsequences of two tokenizations $t = T_A(c),~ t' = T_B(c)$ as follows:

\begin{definition}[Tokenization Distance]
Let $t = t_1 || t_2 || \dots || t_{|t|} \in \mathcal{T}^*,~ t' = t'_1 || t'_2 || \dots || t'_{|t'|} \in \mathcal{T}^*$ be sequences of tokens where $|t| \geq |t'|$ (otherwise, swap $t$ and $t'$).
If $t_1 \neq t'_1$ or $t_{|t|} \neq t'_{|t'|}$ prepend or append, respectively, the \emph{empty token} $\epsilon$ to both $t$ and $t'$.
We define the \emph{tokenization distance of $t$ and $t'$} as a function $D \colon \mathcal{T}^* \times \mathcal{T}^* \rightarrow \mathbb{N}$ with
$$D(t, t') = \big| \big\{ i \in [1, |t|-1] \mid \exists i' > 0 \exists l > 0 \exists l' \geq l: t_{i} = t'_{i'} \land \left( \bigwedge_{j=1}^{l} t_{i+j} \neq t'_{i'+j} \right) \land t_{i+l+1} = t'_{i'+l'+1} \big\} \big|.$$
\end{definition}

\begin{example}[Tokenization Distance]
Let $c = \textrm{``hello''}$ be a text generated by Alice with tokenization $T_A(c) = \textrm{he}||\textrm{l}||\textrm{lo}$.
Let $T_B(c) = \textrm{he}||\textrm{llo}$ be the tokenization of $c$ generated by Bob.
The distance between $T_A(c)$ and $T_B(c)$ is 

$$
D(T_A(c), T_B(c))
= D(\textrm{he}||\textrm{l}||\textrm{lo}, \textrm{he}||\textrm{llo})
= D(\textrm{he}||\textrm{l}||\textrm{lo}||\epsilon, \textrm{he}||\textrm{llo}||\epsilon)
= \left| \left\{ 1 \right\} \right| = 1.
$$
\end{example}

\section{Reliability of the Meteor Stegosystem}

In \autoref{alg:decode}, we see that during decoding, the stegotext $c$ generated by \autoref{alg:encode} should be parsed as $c = c_0 ||c_1 || \dots || c_{\tau}$.
This task is performed by a tokenizer $T$.
Meteor's $Decode$ algorithm expects that $T$ can recover the tokens $c_i$ originally generated during encoding.
This unfortunately is, at least for models with subword tokenizations such as GPT-2, not the case for some combinations of hiddentext, key and history due to ambiguous tokenization described above.
In \autoref{sec:alg-rec-tok-candidates}, we will show how to fix these reliability issues.

\begin{theorem}
The Meteor stegosystem is not reliable.
\end{theorem}

\begin{proof}
For Meteor to be reliable, its unreliability must be negligible in $\lambda$, i.e.
$$UnRel_{\mathcal{S}}(\lambda) = \max_{\substack{k \in \{0,1\}^\lambda\\m \in \mathcal{U}^*\\ h \in \mathcal{H}}}\left\{ Pr[SD(k, SE(k,m,h), h) \neq m] \right\} < \mu(\lambda).$$
We show unreliability by finding a tuple $(k',m',h')$ where decoding always fails, i.e., $$Pr[SD(k', SE(k',m',h'), h') \neq m'] = 1.$$
The following counterexample has been generated by repeatedly encoding a fixed message $m'$ and history $h'$ with random keys $k' \leftarrowS U_{512}$.
The decoding failure is independent of the choice of $\lambda$.
We will later show that the decoding fails for many or most keys $k$, especially if the encoded messages $m$ are long ($|m| \geq 1024$ bytes).
Let 
\begin{lstlisting}[breaklines]
k' = 0xb95e03a1d01b304f11dcf2bc844e5fd3cbed41253b0506876004207b2c2a10e2
       d89c1a40e93530bfcfaaee54e66ae048d2d2a536615b0a81afe792883877d5b6
m' = "Hello world"
h' = "Despite a long history of research and wide-spread applications to censorship resistant systems, practical steganographic systems capable of embedding messages into realistic communication distributions, like text, do not exist.\n\n"
\end{lstlisting}
be the inputs to the encoder.
The stegotext when using the Meteor demo code at \cite{MeteorDemo2021} is
\begin{lstlisting}
c = '\nZeus communication system, controlled by anÃ†2 desktop mic with'
\end{lstlisting}
When passed to the standard GPT-2 tokenizer, the substring ``Zeus'' of $c$ is parsed as ``Z$||$eus'', while the encoding party has generated ``Zeus'' with token sequence ``Ze$||$us''.
Therefore, the stegotext cannot be successfully decoded to the original hiddentext ``Hello world'', which violates reliability.
\end{proof}

After we have seen that this problem appears at least once by finding a counterexample, another question arises:
How often does this happen?
If this happens only for a negligible number of combinations of message, key, and history (or maybe even only for this exact combination), this might be a non-issue.

To approach this question, we first define a random variable $X$ as the distance between tokenizations $T_A(c)$ and $T_B(c)$ generated by Alice and Bob, respectively, for a stegotext $c \in \Delta^*$, i.e., for randomly chosen $c$ we define $X = D(T_A(c), T_B(c))$.

As we have argued in \autoref{sec:amb-tok}, the decoding can only succeed if no tokenization mismatches happen, i.e., the tokenizations generated by Alice and Bob, respectively, are the same.
Therefore, for a successful decoding of a stegotext, the tokenization distance must be zero.
We can establish the tokenization distance as an upper bound on the probability of a successful decoding:

$$Pr[Decode_{\mathcal{C}}^\beta(k, Encode_{\mathcal{C}}^\beta(k,m,h), h)=m] \leq Pr[X=0].$$

Unfortunately, we cannot determine $Pr[X=0]$ exactly because there are infinitely many stegotexts $c \in \Delta^*$.
Instead, we will estimate $Pr[X=0]$ with an experimentally determined approximation $\hat{Pr}_n[X=0]$ using a finite number of samples $(k,m,h) \in S_n$ with keys $k \in \{0,1\}^\lambda$ for a fixed security parameter $\lambda$, messages $m$ from some set of text samples $M_n \subset \mathcal{U}^n$ for message lengths $n$ and histories $h \in \mathcal{H}$.
For each sample in $S_n$, we generate stegotexts $c = Encode_{\mathcal{C}}^\beta(k,m,h)$ and calculate the tokenization distances $D(T_A(c), T_B(c))$.
Afterwards, we can establish the approximation
$$\hat{Pr}_n[X=0] = \frac{|\{ (k,m,h) \in S_n \mid D(T_A(Encode_{\mathcal{C}}^\beta(k,m,h)), T_B(Encode_{\mathcal{C}}^\beta(k,m,h))) = 0 \}|}{|S_n|}.$$

For this experiment, we use William Shakespeare's drama Hamlet as the source for messages and histories.
For the message set $M_n$, we use blocks of length $n=128$ and $1024$ bytes from the script of Hamlet, respectively.
The entire script of Hamlet is 191726 bytes long.
That gives us $|M_{128}|=\left\lceil \frac{191726}{128} \right\rceil = 1498$ and $|M_{1024}|= \left\lceil \frac{191726}{1024} \right\rceil = 188$ samples for $m$.
Even though the sample size for blocks of 1024 bytes is rather small, the average stegotext length grows proportionally to the number of bits encoded, so we can expect about the same total length of token sequences for both experiments.
For each hiddentext message, we generate a random key $k \leftarrowS U_{512}$ as well as an initial history $h$ of length 128 bytes randomly chosen from the script of Hamlet.

After defining the sample data, we encode and subsequently decode each sample using a modified version of the Meteor demo code from \cite{MeteorDemo2021}.
After decoding, we compute the tokenization distances between Alice and Bob for each sample.

The experiment shows that most stegotexts have at least one mismatch if they are of significant length (\autoref{fig:meteor-stats-mismatch-count}).
When encoding 128 bytes of hiddentext, the success probability is $\hat{Pr}_{128}[X=0] \approx 0.57$.
With hiddentext lengths of 1024 bytes, it decreases even further to $\hat{Pr}_{1024}[X=0] \approx 0.13$.
It is reasonable to expect that the probability of a successful decoding converges to zero for even longer hiddentexts.

\begin{figure}[htbp]%
  \begin{subfigure}{.5\textwidth}%
    \centering%
      \resizebox{0.9\linewidth}{!}{%
      \input{fig_meteor_stats_mismatch_count_128.tikz}%
    }%
    \caption{128 bytes ($\hat{Pr}_{128}[X=0] = \frac{861}{1498} \approx 0.57$)}%
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}%
    \centering%
      \resizebox{0.9\linewidth}{!}{%
      \input{fig_meteor_stats_mismatch_count_1024.tikz}%
    }%
    \caption{1024 bytes ($\hat{Pr}_{1024}[X=0] = \frac{25}{188} \approx 0.13$)}%
  \end{subfigure}%
  \caption{
  Tokenization mismatch count probability when using Meteor to encode Shakespeare's Hamlet block-wise for different block sizes.
  We denote the random variable $X$ as the number of encoding mismatches in a stegotext.
  The x-axis shows the number of tokenization mismatches between the encoding and the decoding party for hiddentext blocks of size...}
  \label{fig:meteor-stats-mismatch-count} 
\end{figure}

%In the same experiment, we have also measured the mismatch length, which is the number of tokens which have to be skipped in the stegotext to get back in sync with the decoding party.
%For example, if a stegotext ``steganography is great'' was encoded with token sequence (steg, ano, graphy, \verbvisiblespace is, \verbvisiblespace great), but decoded as (ste, ga, no, graphy, \verbvisiblespace is, \verbvisiblespace great), the mismatch length is two, because the two tokens \emph{steg} and \emph{ano} have to be skipped on the encoded token sequence to get back in sync between encoding and decoding party when encounterint the token \emph{graphy}
%This measure will later help us estimate how far we should look ahead in the stegotext when trying to recover from decoding errors.

%\begin{figure}[htbp]%
% \begin{subfigure}{.5\textwidth}%
%   \centering%
%     \resizebox{0.8\linewidth}{!}{%
%     \input{fig_meteor_stats_mismatch_length_128.tikz}%
%   }%
%   \caption{128 bytes}%
% \end{subfigure}%
% \begin{subfigure}{.5\textwidth}%
%   \centering%
%     \resizebox{0.8\linewidth}{!}{%
%     \input{fig_meteor_stats_mismatch_length_1024.tikz}%
%   }%
%   \caption{1024 bytes}%
% \end{subfigure}%
% \caption{
% Token mismatch length statistics when using Meteor to encode Shakespear's Hamlet blockwise for different block sizes.
% The x-axis shows the lengths of tokenization mismatches of the decoding party for hiddentext blocks of size...}
% \label{fig:meteor-stats-mismatch-count} 
%\end{figure}

Therefore, we have to find a way to deal with these failed decodings, especially if we plan to encode hiddentext of significant length such as HTTP messages.
In \autoref{sec:alg-rec-tok-candidates}, we will present an approach to recover from decoding errors while introducing stegotext overhead linear in hiddentext length and -- in the worst case -- computational overhead exponential in stegotext length.
In \autoref{chap:twowaycommunication}, we will show how we can split the hiddentext message and use DialoGPT -- a derivation of GPT-2 -- to generate chat-like stegotext.
Splitting the hiddentext results in shorter stegotexts and thus decreases the probability of tokenization mismatches.



\section{Algorithmic Reconstruction of Tokenization Candidates}
\label{sec:alg-rec-tok-candidates}

Unfortunately, with subword tokenization, the decoding party cannot decide which tokens have been used to generate a given stegotext.
To allow successful decoding of ambiguously tokenized stegotexts, we will introduce modifications to the $Encode$ and $Decode$ algorithms to detect and fix wrong tokenizations.

Before encoding, split the message $m$ into blocks $m_i$ of length $\gamma$.
After each block $m_i$, add a checksum block $q_i = q(m_i)$ of length $\delta$ into the hiddentext, i.e., $q \colon \{ 0,1 \}^\gamma \rightarrow \{ 0,1 \}^\delta$.
This value helps the decoder to decide if the decoding is still correct up to block $i$.
By adding checksum blocks, we introduce $\delta \cdot \frac{|m|}{\gamma}$ bits of overhead to the hiddentext.
For the modified $MarkedEncode$ algorithm, see \autoref{alg:marked-encode}.

Now, the decoding party Bob has to verify that after each block $m_i$ of $\gamma$ bits, the checksum $q(m_i)$ of length $\delta$ appears.
If not, a tokenization mismatch occurred.
To recover from this, Bob has to roll back his decoding and generate all possible tokenizations of a substring of $c$ starting with the position in $c$ after his last successful checksum check.
The BPE tokenizer used in GPT-2 does not contain tokens that span word boundaries, i.e., no token contains inner spaces. Therefore, we can assume that a tokenization mismatch ends when the current word does.
When a tokenization with correct checksums was found, the decoder expects that the correct tokenization has been found and proceeds.
Note that this approach will not work with models that use language independent tokenizers such as SentencePiece \cite{SentencePiece2018}, which tokenizes strings as raw sentences and treats spaces as regular characters.
With language independent tokenizers, the entire suffix of $c$ starting with the block failing the checksum has to be considered, potentially introducing overhead exponential in stegotext length.

It is still possible that a wrong tokenization randomly yields a correct checksum.
In an actual implementation, one might want to check multiple blocks in advance before expecting that the correct tokenization has been found and choose $\gamma$ and $\delta$ depending on the security parameter $\lambda$ such that the probability of a false-positive checksum is negligible.
Determining specific values for $\gamma$ and $\delta$ is a trade-off between stegotext length and computational overhead in the case of a decoding error.
While longer checksums $\delta$ potentially decrease the risk of false-positive checksum checks, longer checksums cause longer stegotexts.
If we chose $\gamma = |m|$, only one checksum is generated over the entire message. 
If an error is detected after decoding the entire hiddentext, every possible tokenization for the stegotext $c$ has to be generated of which there are exponentially many in stegotext length.
If we choose a small $\gamma$, many checksums are inserted, drastically increasing stegotext length.
A modified $Decode$ algorithm can be found in \autoref{alg:marked-decode}.

\begin{Pseudocode}[caption={
$MarkedEncode$ algorithm.
The modification this algorithm introduces is that after every block $m_i$ of $\gamma$ bits a checksum $q(m_i)$ is inserted into the hiddentext.
This allows the recipient to check for decoding errors after decryption.
$q$ is a function $q \colon \{0,1\}^\gamma \rightarrow \{0,1\}^\delta$ that generates a checksum of length $\delta$ for a message block of length $\gamma$.
}, label={alg:marked-encode}]
algorithm $MarkedEncode_{\mathcal{C}}^{\beta, \gamma, \delta}(k_{prg}, m, h, q)$
  Output: Stegotext message $c$
  $c \leftarrow \epsilon,~ n \leftarrow 0,~ i \leftarrow 0$
  Parse $m$ as blocks $m_1||m_2||\dots||m_\xi$ of size $\gamma$
  $m^* \leftarrow m_1||q(m_1)||m_2||q(m_2)||\dots||m_\xi||q(m_\xi)$
  while $n < |m^*|$ do
    $mask \leftarrow PRG.Next(k_{prg})$
    $r \leftarrow m^*[n:n+\beta] \oplus mask$
    $c_i \leftarrow Sample_{\mathcal{C}}^\beta(h, r)$
    $\mathcal{R} = Recover_{\mathcal{C}}^\beta(h, c_i)$
    $n_i \leftarrow LenPrefix^\beta(\mathcal{R})$
    $c \leftarrow c || c_i, n \leftarrow n+n_i, h \leftarrow h||c_i,~ i \leftarrow i + 1$
  Output $c$
\end{Pseudocode}

\begin{Pseudocode}[caption={
$MarkedDecode$ algorithm.
In comparison to Meteor's $Decode$ algorithm, $MarkedDecode$ verifies the checksums $q(m_i)$ of blocks $m_i$.
If the checksum does not match, a decoding error occurred.
It then performs a lookbehind on the stegotext and generates all possible tokenizations $paths$ for a substring of $c$.
Afterwards, it rewinds the internal state and retry decoding with a path $p$ selected from $paths$.
}, label={alg:marked-decode}]
algorithm $MarkedDecode_{\mathcal{C}}^{\beta,\gamma,\delta}(k_{prg}, c, h, q)$
  Output: Plaintext message $m$
  $m^* \leftarrow \epsilon,~ n \leftarrow 0,~ j \leftarrow 0,~ \alpha^* \leftarrow 0$
  $paths \leftarrow \emptyset$
  Parse $c$ as $c_0 || c_1 || \dots || c_{\tau}$
  for $i \in \{ 0, 1, \dots, \tau \}$ do
    $\mathcal{R} = Recover_{\mathcal{C}}^\beta(h, c_i)$
    $n_i \leftarrow LenPrefix^\beta(\mathcal{R})$
    $n \leftarrow n + n_i$
    $j \leftarrow j + n_i$
    $m_i \leftarrow Prefix^\beta(\mathcal{R})$
    $mask \leftarrow PRG.Next(k_{prg})$
    $m^* \leftarrow m^* || (m_i \oplus mask[0: |m_i|])$
    $h \leftarrow h||c_i$
    if $j \geq \gamma + \delta$
      # calculate checksum
      Parse $m^*$ as $m^*_1||q_1||m^*_2||q_2||\dots||m^*_{\alpha^*}||q_{\alpha^*}||\dots||m^*_\alpha||q_\alpha$ ignoring suffix of length $< \gamma+\delta$
      if $q_{\alpha^*+1} \neq q(m^*_{\alpha^*+1}) \lor q_{\alpha^*+2} \neq q(m^*_{\alpha^*+2}) \lor \dots \lor q_\alpha \neq q(m^*_\alpha)$
        Find first index $j^* > i^*$ such that $c_{j^*}$ starts with space
        $c^* \leftarrow c_{i^*} || c_{i^*+1} || \dots || c_{j^*-1}$
        if $paths = \emptyset$
          $paths \leftarrow AllPaths(TokenizeCandidates_{\mathcal{T}}(c^*), c^*)$
        $p \leftarrow SelectPath(paths)$
        $paths \leftarrow paths~ \backslash~ p$
        replace $c_{i^*}||c_{i^*+1}||\dots||c_{j^*-1}$ in $c$ with $p$
        rewind PRG state and variables to state at $i^*$
        retry decoding starting at $m_{\alpha^*+1}$ with $p$
      else  # checksum verified, expect first $\alpha$ blocks and $i$ tokens to be correctly decoded
        $j \leftarrow j \mod (\gamma+\delta)$
        $paths \leftarrow \emptyset$
        $\alpha^* \leftarrow \alpha$
        $i^* \leftarrow i$
  Parse $m^*$ as $m^*_1||q(m^*_1)||m^*_2||q(m^*_2)||\dots||m^*_\xi||q(m^*_\xi)$
  Output $m \leftarrow m^*_1||m^*_2||\dots||m^*_\xi$
\end{Pseudocode}

For the modifications in $MarkedDecode$, we need helper algorithms $TokenizeCandidates_{\mathcal{T}}$, $AllPaths$ and $SelectPath$.

When passed a string $c$, $TokenizeCandidates_{\mathcal{T}}$ generates a directed acyclic graph, or DAG, $G = (V, E)$, representing the possible tokenizations of $c$ with tokens $\mathcal{T}$.
The nodes $V$ represent all reachable suffix strings of $c$ (including the empty string $\epsilon$) using tokens from $\mathcal{T}$ with $|V| \leq |c| + 1$.
The edges $E$ represent tokens used to transition between suffixes of $c$.
For an example graph for input ``hello'', see \autoref{fig:ex-graph-tokenize-candidates}.
For example, ``hello'' can be transformed to ``lo'' with token ``hel''.

$AllPaths$ takes a graph $G = (V, E)$ generated by $TokenizeCandidates_{\mathcal{T}}(c)$ for a stegotext $c$ and a start node $v_i \in V$ as inputs and returns a set of all possible paths between $v_i$ and a fixed sink $v_j = \epsilon$ (the empty suffix).
Each path in that list represents a possible tokenization of $c$.
But how many paths exist between $v_i$ and $v_j$?
A path between two vertices is a set $V' \subseteq V$ that contains both $v_i$ and $v_j$ and where there exist edges in $E$ from topologically greater vertices to topologically smaller vertices in $V'$.
In a complete DAG, there are up to $2^{|V|-2} \leq 2^{|c|-1}$ subsets of $V$ which contain both $v_i$ and $v_j$.
Therefore, the output size of $AllPaths$ is, in the worst case, exponential in input length.
For a DFS-based implementation of $AllPaths$ see \autoref{alg:all-paths}.

Lastly, $SelectPath$ selects a path from a list of $paths$ generated by $AllPaths$.
A simple strategy is to choose one element at random from $paths$.
With this approach, we will find the correct tokenization on average after $\frac{|paths|}{2} \leq \frac{2^{|c|-1}}{2} = 2^{|c|-2}$ attempts.
While there are more advanced strategies to select a tokenization, e.g. by selecting a tokenization according to the probability distribution generated by the ML model, this simple approach is still viable when using the English language since we only generate paths for single words that are relatively short.
As has been shown by analyzing books from different epochs, the average word length in the English language is about five characters \cite{BoShSo2012}.
Therefore, an average English word can have up to $2^4 = 16$ possible tokenizations.
There still are commonly used (and therefore generated) words with a length of up to 13 characters such as ``international'' or ``circumstances'', which could have up to $2^{12} = 4096$ possible tokenizations.
Our experiments have shown that when using the GPT-2 tokenizer, the graphs for common longer words in the English language are still relatively small with less than $2^{11}$ possible paths.
For an implementation of $SelectPath$ that chooses a path at random, see \autoref{alg:select-path-rnd}.



\begin{Pseudocode}[float,caption={
$TokenizeCandidates$ algorithm.
This generates a graph $G = (V, E)$ from a string $c$.
Vertices are substrings of $c$, each edge represents a token used to transform between substrings.
This algorithm can be sped up by using dynamic programming to cache results of invocations of $TokenizeCandidates$.
}, label={alg:tokenize-candidates}]
algorithm $TokenizeCandidates_{\mathcal{T}}(c)$
  Output: Graph $G = (V, E)$
  if $c = \epsilon$
    return $(\emptyset, \emptyset)$
  for $t \in \{ t' \in \mathcal{T}~ |~ t' \textrm{ is prefix of } c \}$ do
    $V \leftarrow V \cup \{ c[|t|{:}] \}$
    $E \leftarrow E \cup \{ (c, c[|t|{:}]) \}$
    $G \leftarrow G \cup TokenizeCandidates_{\mathcal{T}}(c[|t|{:}])$
  return $G$
\end{Pseudocode}

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}
    \node[block] (hello) {hello};
    \node[block, right=15mm of hello] (lo) {lo};
    \node[block, above=15mm of lo] (llo) {llo};
    \node[block, above=15mm of llo] (ello) {ello};
    \node[block, below=15mm of lo] (o) {o};
    \node[block, right=15mm of lo] (bot) {$\epsilon$};
    
    \draw[->] (hello) to node[above] {h} (ello);
    \draw[->] (hello) to node[above] {he} (llo);
    \draw[->] (hello) to node[above] {hel} (lo);
    \draw[->] (hello) to node[left] {hell} (o);
    \draw[->, bend right=90,looseness=2] (hello) to node[below] {hello} (bot);

    \draw[->] (ello) to node[right] {e} (llo);
    %\draw[->, bend left=30] (ello) to node[right] {el} (lo);
    %\draw[->, bend left=30] (ello) to node[above] {ell} (o);
    \draw[->] (ello) to node[right] {ello} (bot);

    \draw[->] (llo) to node[right] {l} (lo);
    %\draw[->, bend right=30] (llo) to node[above] {ll} (o);
    %\draw[->] (llo)   to node[above] {llo} (bot);

    \draw[->] (lo)    to node[right] {l} (o);
    \draw[->] (lo)    to node[above] {lo} (bot);

    \draw[->] (o)     to node[above] {o} (bot);
  \end{tikzpicture}
  \caption{
Tokenization graph generated by $TokenizeCandidates_{\mathcal{T}}(\textrm{``hello''})$ with tokens $\mathcal{T} = \{ h, e, l, o, he, lo,  hel, hell, ello, hello \}$.
The vertices represent substrings of $c = \textrm{``hello''}$ that are reachable by removing a prefix token $t \in \mathcal{T}$ from $c$.
The edges are labeled with the token $t \in \mathcal{T}$ used to transform the left-hand side vertex $v_i$ to the right-hand side vertex $v_j$, i.e., $v_i = t || v_j$.
The special node $\epsilon$ represents the empty string and is a sink in the tokenization graph.
The list of all possible paths between $c$ and $\epsilon$ are the possible tokenizations of $c$ using tokens $\mathcal{T}$.
}
  \label{fig:ex-graph-tokenize-candidates}
\end{figure}

\begin{Pseudocode}[float,caption={
DFS-based $AllPaths$ algorithm that generates a list of all possible paths between a root and a sink $\epsilon$ in a DAG.
This algorithm's performance can be sped up by using dynamic programming to cache results of invocations of $AllPaths$.
},label={alg:all-paths}]
algorithm $AllPaths(G, root)$
  Output: List of paths between $root$ and sink $\epsilon$
  if $root = \epsilon$ do
    return $\emptyset$
  $paths \leftarrow \emptyset$
  $hops \leftarrow OutEdges(G, root)$
  for $hop \in hops$ do
    $subpaths \leftarrow AllPaths(G, hop)$
    for $subpath \in subpaths$ do
      $paths \leftarrow paths \cup \{ root || subpath \}$
  return $paths$
\end{Pseudocode}

\begin{Pseudocode}[float, caption={
$SelectPath$ algorithm with random path selection strategy.
}, label={alg:select-path-rnd}]
algorithm $SelectPath(paths = \{ p_1, p_2, \dots, p_{|paths|} \})$
  Output: path $p \in paths$
  $i \leftarrowS \{1, 2, \dots, |paths| \}$
  $paths \leftarrow paths \backslash \{ p_i \}$
  return $p_i$
\end{Pseudocode}

%\begin{Pseudocode}[float, caption={
%Probabilistic Path Selection Strategy.
%}, label={alg:select-path-prob}]
%algorithm $SelectPath_{\mathcal{C}}(paths = \{ p_0, p_1, \dots, p_{|paths|-1} \}, h)$
% Output: path $p \in paths$
% $\mathcal{T}, \mathcal{P} \leftarrow Next_{\mathcal{C}}(h)$
% $p_i \leftarrow p_i \in paths$ such that $p_i$ starts with $t_j \in \mathcal{T}$ and $x_j \in \mathcal{P} = %max(\mathcal{P})$
  %$paths \leftarrow paths - \{ p_i \}$
  %return $p_i$
%\end{Pseudocode}
