\chapter{Correctness}
\label{chap:correctness}

The Meteor authors have released a demo of Meteor at \cite{MeteorDemo2021}. There, they state that ``due to issues with the GPT-2 algorithm interface, you sometimes may see extra output from a decoded stegotext. This does not impact the underlying security of the scheme''.
During experimentation, I have concluded that the cause of this is due to the way sub-word based GNNs, such as GPT-2, represent text and not due to issues with the algorithm interface.

After describing the issue at hand, I will show that the Meteor Stegosystem is actually incorrect with respect to Hopper's definition as defined in \autoref{def:correctness-hopper} by providing a counterexample.
To restore correctness, I then present a change to Meteor's $Recover$ algorithm to successfully reconstruct the original hiddentext message while inducing overhead exponential in stegotext length.

\section{Ambiguous Tokenization}

In \autoref{sec:generative-neural-networks}, we discuss that GPT-2 uses a token dictionary to represent text.
To improve text generation and better recognize relations between words, sub-word tokenization might can words into sub-words.
Sub-word tokenization is opposed to word tokenization, which splits a text by spaces, and character-based tokenization, which tokenizes text character by character.
Sub-word tokenization allows for more flexible deep learning, as the framework can easier connect related words to each other. 
The model might learn that, by adding the suffix ``n't'' to a verb, one can invert the meaning of that verb, e.g. ``haven't'' is the opposite of ``have'' and those words are in close relation to each other.
This technique proves to be very effective in enhancing GNN performance.

On the other hand, this allows for one word to have multiple representations in the ML model.
For example, the word ``doesn't'' might be tokenized by a tokenizer $\mathcal{T}$ as $\mathcal{T}(\textrm{``doesn't''}) = ( \textrm{``do''}, \textrm{``es''}, \textrm{``n't''} )$. It could also be tokenized as $\mathcal{T}(\textrm{``doesn't''}) = ( \textrm{``does''}, \textrm{``n't''} )$ or even $\mathcal{T}(\textrm{``doesn't''}) = ( \textrm{``d''}, \textrm{``o''}, \textrm{``e''}, \textrm{``s''}, \textrm{``n''}, \textrm{``'t''})$.
This effect is what we call ambiguous tokenization.

\begin{definition}[Ambiguous Tokenization]
	Let $\mathcal{M}$ be a GNN with tokens $\mathcal{T} = \{ t_1, t_2, \dots, t_n \}$.
	We say that $\mathcal{M}$ has an ambiguous tokenizations, if there exists at least two ordered list of indices $I = (i, j, k, \dots), J = (q, r, s, \dots), I \neq J$ with $T_1 = t_i || t_j || t_k || \dots$ and $T_2 = t_q || t_r || t_s || \dots$ such that $t_I = t_J$.
\end{definition}

\section{Correctness Of The Meteor Stegosystem}

In \autoref{alg:decode}, we see that during decoding, the stegotext $c$ generated by \autoref{alg:encode} should be parsed as $c = \{c_0,c_1, \dots, c_{|c|-1|}\}$.
This task is performed by a tokenizer $\mathcal{T}$ associated with the underlying model.
Here, the $Decode$ algorithm expects that the parsing of $c$ can recover the originally generated $c_i$.
This unfortunately is, at least for the GPT-2 model, not the case for some combinations of stegotext, key and history.
In \autoref{sec:alg-rec-tok-candidates}, I show how to fix these correctness issues.

\begin{theorem}
The Meteor Stegosystem is not correct.
\end{theorem}

\begin{proof}
For Meteor to be correct, it must fulfil \autoref{def:correctness-kaptchuk}, i.e. for any $k \leftarrow KeyGen(1^\lambda)$ and hiddentext $m$:

$$\mathop{Pr}[Decode_{\mathcal{D}}(k, Encode_{\mathcal{D}}(k, m, \mathcal{H}), \mathcal{H}) = m] \geq 1 - \mathop{negl}(\lambda)$$

We show incorrectness by giving a counterexample. 
Let

\begin{lstlisting}[breaklines]
k = 0xb95e03a1d01b304f11dcf2bc844e5fd3cbed41253b0506876004207b2c2a10e
    2d89c1a40e93530bfcfaaee54e66ae048d2d2a536615b0a81afe792883877d5b6
m = 'Hello world'
H = 'Despite a long history of research and wide-spread applications to censorship resistant systems, practical steganographic systems capable of embedding messages into realistic communication distributions, like text, do not exist.\n\n'
\end{lstlisting}

The stegotext for these inputs when using the Meteor demo code is

\begin{lstlisting}
c = '\nZeus communication system, controlled by an√Ü2 desktop mic with'
\end{lstlisting}

When passed to the standard GPT-2 Tokenizer, the substring ``Zeus'' of $c$ is parsed as \lstinline{['Z', 'eus']}, while the encoding process has generated ``Zeus'' with token sequence \lstinline{['Ze', 'us']}.
Therefore, the stegotext $c$ cannot be successfully decoded to the original hiddentext ``Hello world'', which violates correctness.
\end{proof}

After we have seen that this problem appears at least once by finding a counterexample, another question arises:
How often does this happen?
To approach this question, I have conducted an experiment which attempts to estimate the rate at which these incorrect decodings occur.
For that, I have repeatedly encoded the entire book of Hamlet in chunks of 1024 characters with random keys and histories of length 128 picked at random from Hamlet as well using a modified version of the Meteor demo code from \cite{MeteorDemo2021}.

The experiment shows that most stegotexts have at least one mismatch if they are of significant length.
Therefore, we have to find a way to deal with these failed decodings.
In \autoref{sec:alg-rec-tok-candidates}, I present approaches to recover from decoding errors while introducing computational overhead exponential in stegotext length.

\begin{figure}[htbp]
	\centering
	\input{fig_meteor_stats_mismatch_count_arithmetic.tikz}
	\caption{Binned mismatch count statistics. The x-axis shows the number of tokenization mismatches between the encoding and the decoding party}
	\label{fig:meteor-stats-mismatch-count}
\end{figure}
\todo{improve plot to increase readability}



\section{Algorithmic Reconstruction Of Token Candidates}
\label{sec:alg-rec-tok-candidates}

Unfortunately, with sub-word tokenization, the decoding party cannot decide how the stegotext has been tokenized by the encoder.
To allow successful decoding of ambiguously tokenized stegotexts, I will in this section introduce algorithms to detect and fix wrong tokenizations. 


For that we have to modify the encoding step of Meteor to detect and fix wrong tokenization.
Before encoding, split the message $m$ into blocks $m_i$ of length $\gamma$.
After each block $m_i$, add a marker or checksum $q_i = q(m_i)$ into the hiddentext where $q \colon \{ 0,1 \}^\gamma \rightarrow \{ 0,1 \}^\delta$, i.e. $q$ generates bitstrings of fixed length $\delta$.
This marker helps the decoder to decide if the decoding is still correct up to this point.
The marker $q_i$ can be a checksum of $m_i$ or a fixed marker.
By using markers, we introduce $\delta \cdot \frac{|m|}{\gamma}$ bits of overhead to the hiddentext.
We modify \autoref{alg:encode} as follows:

\begin{Pseudocode}[float, caption={Marked Encode Algorithm}, label={alg:marked-encode}]
algorithm $MarkedEncode_{\mathcal{M}}^{\beta, \gamma}(k_{prg}, m, q, \mathcal{H})$
	Output: Stegotext message $c$
	$c \leftarrow \epsilon,~ n \leftarrow 0$
	$m^* \leftarrow \epsilon,~ j \leftarrow 0$
	while $j < |m|$ do
		$m^* \leftarrow m^* || m_j$
		$j \leftarrow j + 1$
		if $j \equiv 0~ \pmod \gamma$
			$m^* \leftarrow m^* || q$
	while $n < |m|$ do
		$mask \leftarrow PRG.Next(k_{prg})$
		$r \leftarrow m[n:n+\beta] \oplus mask$
		$c_i \leftarrow Sample_{\mathcal{M}}^\beta(\mathcal{H}, r)$
		$\mathcal{R} = Recover_{\mathcal{M}}^\beta(\mathcal{H}, c_i)$
		$n_i \leftarrow LenPrefix^\beta(\mathcal{R})$
		$c \leftarrow c || c_i, n \leftarrow n+n_i, \mathcal{H} \leftarrow \mathcal{H}||c_i$
	Output $c$
\end{Pseudocode}

Now the decoding party has to verify that every $\lambda$ bits, the marker $q$ appears.
If not, a decoding error has occured in the stegotext.
We modify \autoref{alg:decode} as follows:


\begin{Pseudocode}[float, caption={Marked Decode Algorithm}, label={alg:marked-decode}]
algorithm $MarkedDecode_{\mathcal{M}}^\beta(k_{prg}, c, \mathcal{H})$
	Output: Plaintext message $m$
	$m \leftarrow \epsilon$
	!!TODO!!
	Parse $c$ as $\{ c_0, c_1, \dots, c_{|c|-1} \}$
	for $i \in \{0, 1, \dots, |c|-1 \}$ do
		$\mathcal{R} = Recover_{\mathcal{M}}^\beta(\mathcal{H}, c_i)$
		$m_i \leftarrow Prefix^\beta(\mathcal{R})$
		$mask \leftarrow PRG.Next(k_{prg})$
		$m \leftarrow m || (m_i \oplus mask[0: |m_i|])$
		$\mathcal{H} \leftarrow \mathcal{H}||c_i$
	Output $m$
\end{Pseudocode}
\todo{Modify Decode to MarkedDecode}

For the modification above, we need a helper algorithm which generates all possible tokenizations for a given stegotext.
We can represent these tokenization candidates in a graph using the ML models's tokens $\mathcal{T}$.

When passed a string $c$, the algorithm \emph{TokenizeCandidates} generates a directed, acyclic graph (DAG) $G = (V, E)$.
The nodes $v \in V$ represent all possible suffix strings of $c$ with $|V| = |c| + 1$.
The edges $e \in E$ represent possible tokens to use to transition between suffixes.
For example, the suffix string ``ello'' can be transformed to ``lo'' by removing the token ``el''.
An example graph for input ``hello'' can be found at \ref{fig:ex-graph-tokenize-candidates}.

Now that we have a graph for $c$, we can generate every possible sequence of tokens which $c$ can be parsed as.
For that, we use the algorithm $AllPaths$, which takes as input a graph $G = (V, E)$, a start node $v_i \in V$, a target node $v_j \in V$ and returns a set of all possible paths between $v_i$ and $v_j$.

But how many paths between $v_i$ and $v_j$ exist? A path between $v_i$ and $v_j$ is a subset of $E$ which contains both $v_i$ and $v_j$.
In the worst case, $G$ is a complete DAG with $v_i = max_{topological}(V)$ and $v_j = min_{topological}(V)$.
There are at most $2^{|V|-2} = 2^{|c|-1}$ subsets of $E$ which contain both $v_i$ and $v_j$, since $|V| = |c|+1$.
A DFS-based implementation for AllPaths can be found at \autoref{alg:all-paths}.

\begin{figure}[htbp]
	\centering
	\begin{tikzpicture}
		\node[block] (hello) {hello};
		\node[block, right=15mm of hello] (lo) {lo};
		\node[block, above=15mm of lo] (llo) {llo};
		\node[block, above=15mm of llo] (ello) {ello};
		\node[block, below=15mm of lo] (o) {o};
		\node[block, right=15mm of lo] (bot) {$\epsilon$};
		
		\draw[->] (hello) to node[above] {h} (ello);
		\draw[->] (hello) to node[above] {he} (llo);
		\draw[->] (hello) to node[above] {hel} (lo);
		\draw[->] (hello) to node[left] {hell} (o);
		\draw[->, bend right=120] (hello) to node[below] {hello} (bot);

		\draw[->] (ello) to node[right] {e} (llo);
		\draw[->, bend left=30] (ello) to node[right] {el} (lo);
		\draw[->, bend left=30] (ello) to node[above] {ell} (o);
		\draw[->] (ello) to node[right] {ello} (bot);

		\draw[->] (llo) to node[right] {l} (lo);
		\draw[->, bend right=30] (llo) to node[above] {ll} (o);
		\draw[->] (llo)   to node[above] {llo} (bot);

		\draw[->] (lo)    to node[right] {l} (o);
		\draw[->] (lo)    to node[above] {lo} (bot);

		\draw[->] (o)     to node[above] {o} (bot);
	\end{tikzpicture}
	\caption{Graph generated by TokenizeCandidates(``hello'')}
	\label{fig:ex-graph-tokenize-candidates}
\end{figure}
\todo{improve tikz graph}

\begin{lstlisting}[float, language=Python]
def tokenize_candidates(self, text):
    tokens = bucketize_tokens(self.encoder.items())
    tokenize_edges = {}
    for i in range(len(text)):
        _do_tokenize_candidates(self, text[-(i + 1):], tokens, tokenize_edges)
    return tokenize_edges
def _do_tokenize_candidates(self, text, tokens, tokenize_edges):  # parent: Union[str, Node]
    if text in tokenize_edges or text == '':
        return
    # for token, id in self.encoder.items():
    for token, id in tokens[text[0]]:
        if text.startswith(token):
            remainder = text[len(token):]
            if text not in tokenize_edges:
                tokenize_edges[text] = []
            if remainder not in tokenize_edges[text]:
                tokenize_edges[text] += [(remainder, token, id, -len(token))]
def bucketize_tokens(tokens):
    bins = {}
    ignored_token_ids = [
        220  # 'space' token
    ]
    for token, id in (t for t in tokens if t[1] not in ignored_token_ids):
        bins[token[0]] = bins.setdefault(token[0], []) + [(token, id)]
    return bins
\end{lstlisting}
\todo{rewrite as pseudocode}


\begin{lstlisting}[float, language={Python},caption={DFS-based algorithm which generates a list of all possible paths between between two nodes in a DAG},label={alg:all-paths}]
# TODO the performance of this is really bad
def all_paths(graph, root: tuple[str, str, int, int]) -> [[Optional[tuple[str, str, int, int]]]]:
    if root[0] == '':
        return [[None]]
    queue = []
    hops = graph[root[0]]
    queue += hops
    paths = []
    for hop in hops:
        subpaths = all_paths(graph, hop)
        for subpath in subpaths:
            paths += [[root] + subpath]
    return paths
\end{lstlisting}
\todo{rewrite as pseudocode, implement using dynamic programming}
