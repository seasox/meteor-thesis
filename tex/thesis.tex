\documentclass[english,version-2020-11]{uzl-thesis}

% Copy this file as a template for your thesis. You will have to take
% action at all places marked by
%
% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
% !!! Your action is needed here !!!
% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%
% The first place your action is needed is the first line of this
% document:
%
%
% Language of the thesis:
%
% You must use either 'german' or 'english' above, depending on the
% language used in the main text. This will automatically setup a lot
% of things in the background.
%
%
% Version of the class:
%
% You must specify which version of the thesis class is to be
% used. This is important in case the class style changes in later
% years, but we still want an older thesis to look the same, even when
% things are changed in the class.
%
% Do not change or remove the version-xxxx key.
%
%
% Text encoding:
%
% Your thesis *must* be encoded in utf8 (unicode), which is the
% default in most editors these days. Do *not* change this to latin8.



%%%
%
% Main setup:
%
%%%
%
% You must use the \UzLThesisSetup command to specify numerous things
% about your thesis. This includes the entries on the title page, the 
% abstracts, and the bibliography style. You do so by specifying
% so-called "values" for so-called "keys". For instance, 
% for the key "Autor" you must provide your name as the value. You do
% so by writing 'Autor = {Max Mustermann}', that is, the value is put
% into curly braces. You can use the \UzLThesisSetup command
% repeatedly and the order in which you provide the keys is not
% important. 
%
% Everything shown on the title page must be in German -- even
% if the thesis is written in English! Just insert German text for
% German keys and English text for English keys (like 'Abstract' needs
% English text, while 'Zusammenfassung' needs German text).

\UzLThesisSetup{
  %
  % !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  % !!! Your action is needed here !!!
  % !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  %
  % First, specify the institut or clinic at which the thesis was
  % written. You get the logo file from them (make sure it has the
  % correct size, namely the same as the example). If they do not have
  % a logo, the university's default logo is used.
  %
  % The 'verfasst' gets two arguments. Change the first to {an der}
  % for clinics, as in 'Verfasst = {an der}{Medizinischen Klinik I}'
  %
  Logo-Dateiname        = {uzl-thesis-logo-itcs.pdf},
  Verfasst              = {am}{Institut für Theoretische Informatik},
  %
  % The titles:
  %
  Titel auf Deutsch     = {
     Sichere Steganographie auf ML-Basierten Kanälen
  }, 
  Titel auf Englisch    = {
    Secure Steganography on ML-Based Channels
  },
  %
  % Author and supervisor:
  % 
  % Note that the 'Betreuer' or 'Betreuerin' is the supervisor, that
  % is, the professor who officially supervises the thesis. If there
  % is also an assistent of the professor who helped (typically a
  % lot), use 'Mit Unterstützung von' to thank that person. If the
  % thesis was mainly written 'externally' at some company or another
  % institute, point this out using 'Weitere Unterstützung'. 
  % 
  % For your own name, do *not* add things like "BSc" or "BSc
  % cand.". For the supervisor, you should normally include
  % "Prof. Dr." or "PD Dr." (ask your supervisor, what is
  % appropriate), but nothing more (so no
  % "Univ.-Prof. Dr. Dr. h.c. mult." unless your supervisor insists).  
  %
  Autor                 = {Jeremy Boy},
  Betreuer              = {Prof. Dr. Maciej Liśkiewicz},
  % 
  % Optional: Supporting persons and institutions. The text should be
  % in German, even for an English thesis.
  %
%  Mit Unterstützung von = {Harry Hilfreich},
  % 
  %   Weitere Unterstützung = {
  %     Die Arbeit ist im Rahmen einer Tätigkeit bei der Firma Muster GmbH
  %     entstanden.
  %   },
  %
  %
  % Your Degree Programm (Studiengang)
  %
  % Specify 'Bachelorarbeit' or 'Masterarbeit' and the degree
  % programme. Make sure the name of programme is correct and not
  % some abbreviation or some incorrect variant. For instance:
  % 'Medizinische Ingenierwissenschaft', but not 'MIW';
  % 'Medizinische Informatik', but not 'Medizin-Informatik';
  % 'Informatik', but not 'Informatik (SSE)'.
  %
  % Use German names for German programmes and English names for
  % English ones, so 'Infection Biology', not 'Infektionsbiologie'. 
  % For programmes that have a German bachelor and an English master,
  % use the German name for a bachelor thesis and the English name for
  % the master thesis.
  %
  Bachelorarbeit,
  Studiengang           = {IT-Sicherheit},
  %
  % Date on which the thesis is turned in German, formatted the
  % traditional German way:
  %
  Datum                 = {\today},
  %
  % The English abstract. You must always provide abstracts in German
  % and in English. 
  %
  Abstract              = {
    Governmental and commercial adversaries increasingly threaten the confidentiality of encrypted communication.
    Steganography alllows confidential communication even in environments where cryptography is actively prohibited.
    The Meteor Stegosystem extends classical steganographic primitives for the use on channels based on generative models, such as GPT-2.
    Here, the secret text is embedded in the model's sampling scheme.
    In this thesis, I perform a statistical analysis of Meteor's performance and compare it with the results presented in the original paper.
    Subsequently, I present and evaluate different strategies for embedding secret text.
    The strategies optimize the length of generated messages against reliability and computational ressources required to generate and decrypt stegotexts.
    The strategies improve the reliability significantly while keeping achievable information density and required computational ressources justifiable.
  },
  Zusammenfassung       = {
    Staatliche und privatwirtschaftliche Akteure bedrohen zunehmend die Vertraulichkeit verschlüsselter Kommunikation.
    Steganographie ermöglicht vertrauliche Kommunikation auch dann, wenn die Nutzung klassischer Kryptographie verhindert wird.
    Das Meteor-Stegosystem erweitert klassische steganographischer Primitive für die Verwendung auf Kanälen, die auf generativen Modellen basieren, wie etwa GPT-2.
    Hierbei wird der Geheimtext in das Sampling des generativen Modells eingebettet.
    In dieser Thesis führe ich eine statistische Analyse der Performance des Meteor-Stegosystems durch und vergleiche diese mit den im Paper präsentierten Ergebnissen.
    Anschließend präsentiere und evaluiere ich unterschiedliche optimierte Strategien zur Einbettung des Geheimtexts.
    Die Strategien optimieren die Länge der erzeugten Nachrichten gegen Zuverlässigkeit und Berechnungsaufwand beim Erzeugen sowie Entschlüsseln von Stegotexten.
    Die Strategien verbessern die Zuverlässigkeit signifikant, während die erzielbare Informationsdichte und benötigter Rechenaufwand vertretbar bleiben.

  },
  %
  % Optional: 'Danksagungen' (German) or 'Acknowledgements'
  % (English). Both keys are optional and both have the same effect of
  % adding an acknowledgements text after the abstracts and before the
  % table of contents.
  %
  %Acknowledgements      = {
  %  This is the place where you can thank people and institutions, do
  %  not try to do this on the title page. The only exception is in
  %  case you wrote your thesis while working or staying at a company or abroad. Then you
  %  should use the \Latex{Weitere Unterstützung} key to provide a text
  %  (in German) that acknowledges the company or foreign
  %  institute. For instance, you could use texts like »Die Arbeit
  %    ist im Rahmen einer Tätigkeit bei der Firma Muster GmbH
  %    entstanden« or »Die Arbeit ist im Rahmen eines
  %    Forschungsaufenthalts beim Institut für Dieses und Jenes an der
  %    Universität Entenhausen entstanden«. Do not name and thank
  %    individual persons from the company or foreign institute on the
  %    title page, do that here. 
  %},
  % Bibliography style: Choose between
  % 
  % 'Alphabetische Bibliographie'
  % for all degree programmes in the natural sciences 
  % 
  % 'Numerische Bibliographie'
  % alternative for all other degree programmes
  % 
  % Either will load biblatex and setup the citation methods and the
  % bibliography styles correctly. You should not mess with them.
  % 
  Alphabetische Bibliographie,
  % Alternatively:
  % Numerische Bibliographie
}


\addbibresource{thesis.bib}


%%%%%%%%%%%%%%%%%%%%
%
% Styling the thesis
%
%%%%%%%%%%%%%%%%%%%%
%
% Creating a visually pleasing layout and choosing fonts is not
% easy. Furthermore, different people have different preferences. Of
% course, for the University of Lübeck, the dean of studies could just
% force everyone to use one specific layout and font, but that seems a
% bit drastic and, also, it seems nice that thesis by different people
% have an individual style even though they all stick to the same
% overall structure.
%
% For these reasons, I (Till Tantau) have spend quite some time on
% designing a flexible layout and styling mechanism for theses.
%
% Basically, the overall structure of the thesis is fixed by the
% thesis class and so are many structural elements. For instance, you
% cannot change the order in which the abstract and table of contents
% are shown, you cannot move the bibliography elsewhere, indeed, the
% bibliography style is also fixed. Likewise, the text on the title
% page is fixed.
%
% Although many things are fixed, you *can* change several other
% things. For instance, you can change the font used for the main
% text, you can change which font is used for titles and headings or
% you can change whether titles and headlines are centered or flushed
% left.
%
% There are many LaTeX packages for changing such things. You are
% kindly asked *not to use them*. Rather, use (only) the options
% offered by the thesis class. All possible choices and combinations
% there have been tested by me and produce nice results; what happens
% with other packages no one knows and might no longer conform to what
% is expected by the university. As you will see, you still have a
% lot of options.
%
%
% Technical note: All styling is done via the command
%
% \UzLStyle{...}
%
% where ... is a key-value list just as for \UzLThesisSetup. The
% difference is just that everything having to do with styling as
% controlled by \UzLStyle, while the more “formal” setup keys are
% controlled by \UzLThesisSetup.
%
%%%
%
% Designs
%
%
% A \emph{design} is a whole set of font and layout options bundled
% together. They have been chosen in such a way that a visually
% pleasing “overall appearance” results.
%
%
% \UzLStyle{computer modern oldschool design}
%
% The look of this design mimics the “classical” way a paper or report
% created with \LaTeX\ looks like: The Computer Modern font is used,
% bold face fonts are used for headlines, only black and white are
% used as colors. This design reminds me of older scientific
% documents, especially from the computer science community where
% \LaTeX\ was used very early.
%
%
% \UzLStyle{computer modern basic design}
%
% A slightly less “oldschool” version of the previous design. It is
% still a classic design in the sense that it uses the Computer Modern
% font and that it still has this “good old \LaTeX” look, but some
% more modern aspects (like colors!) have been added.
%
% Note that this design uses Myriad for the title page (one of the
% “modern aspect”), which means that his font must be installed.
%
%
% \UzLStyle{computer modern scholary design}
%
% In my opinion, this is the ultimate “scholary design”: The thesis
% will look like it has been typeset by hand some 150 years ago and
% then printed by a university press. There is really nothing “modern”
% about it and the word in the name of the design is just part of the
% name of the “Computer Modern” font.
%
%
% \UzLStyle{pagella basic design}
%
% A, well, basic design that uses the Pagella font rather than the
% Computer Modern font. Especially the bold face version of this font
% looks nicer than the Computer Modern counterpart. Also, Pagella,
% while still having a “bookish” look, still feels a bit fresher than
% Computer Modern. 
%
%
% \UzLStyle{pagella centered design}
%
% A variant of the basic Pagella design that centers all
% headlines. A nice alternative to the basic version.
%
%
% \UzLStyle{pagella contrast design}
%
% This design tries to create some visual friction by contrasting the
% sans serif headline font (in bold!) with the main text. I find it a
% visually very interesting combination.
%
%
% \UzLStyle{alegrya basic design}
%
% The third variant of the basic design, this time using the Alegrya
% font. 
%
%
% \UzLStyle{alegrya scholary design}
%
% The Alegrya version of the previous “scholary” design. Unlike the
% Computer Modern version, this design does not look old, but more
% fresh -- while still creating the impression that the text must be
% about a very scientific subject. 
%
%
% \UzLStyle{alegrya stylish design}
%
% The design is quite similar to the scholary version for the Alegrya
% font, but with even more modern additions. “Stylish” is the word
% that comes to my mind.
%
%
\UzLStyle{alegrya modern design}
%
% A design that uses the sans serif version of the Alegrya font for
% the headlines. This is a nice modern overall design.
%
%%%




%%%%%%%%
%
% Now, include the package you need here using \usepackage. 
%
% However, many standard packages are already loaded by the class:
%
% amsmath, amssymb, amsthm, babel, biblatex, csquotes, etoolbox,
% filecontents, fontspec, geometry, hyperref, tikz (with libraries
% arrows.meta, positioning and shapes), varioref, url 
%
% Indeed, in many cases you will not need any extra packages.
%
%%%%%%%

\usepackage[inline]{enumitem}
\usepackage{todonotes}
\usepackage{pgfplots}

\usepgfplotslibrary{groupplots}

\newcommand{\definitionautorefname}{Definition}
\newcommand{\lstlistingautorefname}{Algorithm}




\begin{document}

%
% The title page and table of contents will be inserted automatically
% here. 
%

% In a German thesis write: \chapter{Einleitung}


% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
% !!! Your action is needed here !!!
% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%
% Replace with your own introduction:

\chapter{Introduction}

When governmental and private-sector adversaries try to breach the confidentiality of private communication, civil society requires means to defend their privacy.
Especially in authoritorian states, law enforcement continuously attempts to break or prohibit effective means of cryptography, especially when used for censorship-resistant communication.

\section{Motivation}

TODO

\section{Structure Of This Thesis}

Chapter 2 discusses previous works.
First, we will discuss steganography and provably secure steganography, introduced in \cite{Hopper04}.
Afterwards, we discuss how generative neural networks (GNNs) such as GPT-2 produce high-quality text iteratively.
Concluding, I will present the Meteor stegosystem, a provably secure steganographic system over ML-based channels as introduced by XYZ.

Chapter 3 provides an analysis of the correctness of the Meteor stegosystem. 
We will see that the Meteor stegosystem is not correct because of the way tokenization in GNNs work.
Afterwards, I will present algorithms which ensure correctness of Meteor on the receiver site.
Unfortunately, these algorithms pose overhead exponential in the length of the generated stegotext.

In chapter 4, I present a small modification to the Meteor stegosystem which allows two-way communication.
There, we embed a fixed amount of bits in each message, using the DialoGPT model developed by Microsoft \cite{Zhang2020} to generate dialog-like messages.
This modification makes sure that the algorithms from chapter 3 can be applied more conveniently, since the exponential overhead is negligible for shorter messages.

In chapter 5, we will perform a security analysis of the Meteor stegosystem using Hopper's definition of secure stegosystem.
We will compare the definition of security used by the Meteor authors with the definition of security by Hopper.
We then will show that the Meteor stegosystem is not universally secure in Hopper's definition.

Finally, in chapter 6, I will discuss the results in a conclusion and give ideas for further research.

\chapter{Previous Work}

\section{Provably Secure Steganography}
\label{sec:prov-sec-steg}

Steganography extends the concepts of cryptography to hide the mere existence of a message.
This makes it robust against censorship and prohibition.
While cryptography can be used to hide the contents of a confidential message, its use is easily discoverable by adversaries.
Steganography is usually modelled using the Prisoner's Problem, as introduced by \cite{Simmons1983}.
There, two prisoners Alice and Bob are imprisoned.
While they can exchange messages, their contents are surveilled by a Warden.
Alice and Bob now want to craft and exchange escape plans.
Warden observes messages exchanged between Alice and Bob.
He tries to distinguish the exchange of the escape plan from innocous messages.

In steganography, a message is represented by a sequence of documents from an underlying distribution.
Most approaches to steganography use photographs as underlying distribution and hide information in the images' noise.
If the original photograph is unknown to Warden, Alice can encode hidden text in the original photograph

In his doctoral thesis ``Toward a theory of Steganography'', \cite{Hopper2004} establishes a cryptographic point of view on steganography. 
This allows us to do cryptanalysis on steganographic protocols.
In section 3, he defines a stegosystem as follows:

\begin{definition}[Stegosystem, Hopper]
A \emph{steganographic protocol} $\mathcal{S}$, or \emph{stegosystem}, is a pair of probabilistic algorithms:

\begin{itemize}
	\item $\mathcal{S}$.Encode (abbreviated $SE$) takes as input a key $K \in \{0,1\}*$, a string $m \in \{0,1\}*$ (the hiddentext), and a message history $h$.
		SE(K, m, h) returns a sequence of documents $s_1||s_2||\dots||s_l$ (the stegotext) from the support of $\mathcal{C})h^l$.
	\item $\mathcal{S}$.Decode (abbreviated $SD$) takes as input a key $K$, a sequence of documents $s_1||s_2||\dots||s_l$, and a message history $h$.
		SD(K, s, h) returns a hiddentext $m \in \{0,1\}*$
\end{itemize}
\end{definition}

We want our stegosystem to be correct.
To achieve that, the decoding of a given stegotext $s$, given the correct key $k$ and history $h$, should yield the original hiddentext $m$.
Hopper defines the correctness of a stegosystem as follows:

\begin{definition}[Correctness, Hopper]
\label{def:correctness-hopper}
A stegosystem $\mathcal{S}$ is \emph{correct} if for every polynomial $p(k)$, there exists a negligible function $\mu(k)$ such that $SE$ and $SD$ also satisfy the relationship:

$$\forall m \in \{0,1\}^{p(k)}, h \in D^* \colon Pr(SD(K, SE(K, m, h), h) = m) \geq 1 - \mu(k)$$

where the randomization is over the key $K$ and any coin tosses of $SE$, $SD$, and the oracles accessed by $SE$, $SD$.
\end{definition}

The introduction of a \emph{negligible} polynomial $\mu(k)$ allows the stegosystem to sometimes yield wrong results, but only with probability negligible in $k$.
While stegosystem with $\mu(k) = 0$ can exist, the introduction of negligible polynomials proves helpful for reductions on established cryptographic primitives. 

Since we want our stegosystem to be prone to analysis by warden, we also need a definition of security.
We want any warden to be unable to efficiently decode our stegotext $s$ to the original message $m$ without knowledge of our secret key $k$.
As described above in the Prisoners' Game, we don't want any warden to be able to distinguish a stegotext $s := SE(K, m, h)$ from a covertext $c$ randomly sampled from the underlying distribution $\mathcal{C}$.
If the warden is allowed to choose the hiddentext $m$ used to generate $s$, we call warden a \emph{chosen hiddentext attacker}. 

If the warden can distinguish the stegotext from a covertext, they have an advantage against the stegosystem in the distingushing game.
The insecurity of a stegosystem is the maximum possible advantage any attacker can have.
The attackers are limited by running time $t(k)$, number of queries $q(k)$ and total length $l(k)$ of hiddentexts.
Formally, Hopper defined a chosen hiddentext attack as follows:

\begin{definition}[Chosen Hiddentext Attack, Hopper]
In a chosen hiddentext attack, $W$ is given access to a ``mystery oracle'' $M$ which is chosen from one of the following distributions:

\begin{enumerate}
\item $ST$: The oracle $ST$ has a uniformly chosen key $K \leftarrow U_k$ and responds to queries $(m, h)$ with a StegoText drawn from $SE(K, m, h)$.
\item $CT$: The oracle $CT$ has a uniformly chosen $K$ as well, and responds to queries $(m, h)$ with a CoverText of length $\mathcal{l} = |SE(K, m, h)|$ drawn from $\mathcal{C}_h^{\mathcal{l}}$
\end{enumerate}

$W^M(1^k)$ outputs a bit which represents its guess about the type of $M$.

We define $W$'s advantage against a stegosystem $\mathcal{S}$ for channel $\mathcal{C}$ by

$$Adv_{\mathcal{S}, \mathcal{C}, W}^{ss}(k) = |Pr[W^{ST}(1^k)=1] - Pr[W^{CT}(1^k)=1]|$$

where the probability is taken over the randomness of $ST$, $CT$, and $W$. 

Define the insecurity of $\mathcal{S}$ with respect to channel $\mathcal{C}$ by

$$InSec_{\mathcal{S}, \mathcal{C}}^{ss}(t, q, l, k) = \max_{W \in \mathcal{W}}(t, q, l) \left\{ Adv_{\mathcal{S}, \mathcal{C}, W}^{ss}(k) \right\}$$

where $\mathcal{W}(t, q, l)$ denotes the set of all adversaries which make at most $q(k)$ queries totaling at most $l(k)$ bits (of hiddentext) and running in time at most $t(k)$.
\end{definition}

After defining what insecurity (or $InSec$) of a stegosystem is, Hopper establishes a definition for steganographic secrecy.

\begin{definition}[Steganographic Secrecy, Hopper]
A Stegosystem $\mathcal{S}_k$ is ca	lled $(t,q,l,\epsilon)$ steganographically secret against chosen hiddentext attack for the channel $\mathcal{C}$ ($(t,q,l,\epsilon)$-SS-CHA-$\mathcal{C}$) if $InSec_{\mathcal{S},\mathcal{C}}^{ss}(t,q,l,k) \leq \epsilon$
\end{definition}

Ideally, we would like to achieve \emph{universal} steganographic secrecy, i.e. steganographic secrecy over any always-informative channel $\mathcal{C}$.

\begin{definition}[Universal Steganographic Secrecy, Hopper]
A Stegosystem $\mathcal{S}$ is called $(t, q, l, \epsilon)$-universally steganographic secret agains chosen hiddentext attack ($(t,q,l,k,\epsilon)$-SS-CHA if it is $(t,q,l,\epsilon)$-SS-CHA-$\mathcal{C}$ for every always-informative channel $\mathcal{C}$
\end{definition}

Lastly, we call a Stegosystem $\mathcal{S}_k$ \emph{universally steganographic secret against chosen hiddentext attacks} (USS-CHA) if for every channel $\mathcal{C}$ and for every PPT $W \in \mathcal{W}(t,q,l,k)$, $Adv_{\mathcal{S},\mathcal{C},W}(k)$ is negligible in k.


\section{Generative Neural Networks}
\label{sec:generative-neural-networks}

Generative Neural Networks (GNN) establish a model which approximate realistic distributions such as natural language.
In the last few years, the artificial intelligence community achieved tremendous progress in building powerful machine learning based models.
The Generative Pretrained Transformer 2 (GPT-2) by OpenAPI is one of the industry-standard approaches to (not only) text generation.
GPT-2 has been introduced in \cite{OpenAI2019}.
This model works on a set of tokens, which are words or sub-words of the approximated distribution.
To generate a sentence, i.e. a sequence of tokens, the model requires a history as input and outputs a probability distribution for the next token, see \autoref{fig:generative-network}.

On a high level, we can understand the ML model as a black box.
When given a history, the ML model first generates a logits vector for tokens $t_i$, more formally a list of 2-tuples $((z_1, t_1), (z_2, t_2), \dots, (z_n, t_n)),~ z_i \in ( -\infty, \infty )$.
In the next step, we convert the logits vector $z := (z_i)^{1 \leq i \leq n}$ to a probability distribution $(p_i)^{1 \leq i \leq n}$ with $\sum_{i=1}^n p_i = 1$ using the softmax function $\sigma$:

$$\sigma \colon \mathbb{R}^n \rightarrow (0,1)^n,$$
$$\sigma(z) = \left(\frac{e^{z_i}}{\sum_{j=1}^n e^{z_j}}\right)^{1 \leq i \leq n}$$

When combined, we get the black box for our ML model in \autoref{fig:generative-network}.

\begin{figure}[htpb]
	\centering
	\begin{tikzpicture}
            \node[rectangle, draw=black, minimum width=4cm, minimum height=4cm] (oracle) {ML Model};
            \node[left=of oracle] (history) {history};
            \node[above right=-5mm and 1cm of oracle] (t1) {$t_1$};
            \node[below=5mm of t1] (t2) {$t_2$};
            \node[below=5mm of t2] (t3) {$t_3$};
            \node[below=5mm of t3] (t4) {$\dots$};
            \node[below=5mm of t4] (tn) {$t_n$};
            \draw[->] (history) -- (oracle);
            \draw[->] (oracle) -- node[midway,below] {$p_1$} (t1);
            \draw[->] (oracle) -- node[midway,below] {$p_2$} (t2);
            \draw[->] (oracle) -- node[midway,below] {$p_3$} (t3);
            %\draw[->] (oracle) -- (t4);
            \draw[->] (oracle) -- node[midway,below] {$p_n$} (tn);
	\end{tikzpicture}
	\caption{Generative ML Model as black box function, taking a history as input.}
	\label{fig:generative-network}
\end{figure}
To generate the next token in the sequence, there are basically two approaches:
\begin{enumerate*}[label=(\roman*)] \item \label{enum:gnn-most-likely} selecting the most likely token from the distribution or \item \label{enum:gnn-sample} sampling according to the generated distribution \end{enumerate*}.
While \ref{enum:gnn-most-likely} creates deterministic output, the generated sequences tend to be repetitive.
On the other hand, approach \ref{enum:gnn-sample} is not deterministic, but usually creates higher quality output.
For a comparison of GNN model outputs, see examples \ref{example:gpt2-output} and \ref{example:gpt2-output-sample}.

To use GPT-2 with the Python programming language, we can utilize the Huggingface Transformers library together with PyTorch.
In the most simple form, the pipeline API generates texts using not only GPT-2 but many other text generation models.
A Python program which generates the outputs in examples \ref{example:gpt2-output} and \ref{example:gpt2-output-sample} can be found at \autoref{code:gpt2-demo}.

While the use of the pipeline API is very convenient to develop simple text generation or question answering programs, it abstracts some steps that we will need to intercept to use in a stegosystem.
A more sophisticated way to generate text using GPT-2 can be found in \autoref{code:gpt2-logits-demo}.
The output of \autoref{code:gpt2-logits-demo} resembles those shown in \autoref{example:gpt2-output-sample}. 
This code example shows how we can:

\begin{itemize}
	\item use a tokenizer to convert strings to a vector of tokens
	\item use the model to get a logits vector
	\item transform the logits vector into a probability distribution using the softmax function
	\item sample the next token from the distribution
	\item convert a vector of tokens to a string using the tokenizer
\end{itemize}


\lstinputlisting[language=Python, caption={Python program which generates text using the Huggingface Transformers Pipeline API}, label={code:gpt2-demo}]{code/pipeline_gpt2.py}

\lstinputlisting[language=Python, caption={Python program which generates text using the Huggingface Transformers}, label={code:gpt2-logits-demo}]{code/logits_demo_gpt2.py}



\begin{example}[Example of GPT-2 model output without sampling]
	Hello, I'm a language model, not a programming language. I'm a language model. I'm a language model. I'm a language model. I'm a language model. I'm a language model.
	\label{example:gpt2-output}
\end{example}

\begin{example}[Example of GPT-2 model output with sampling]
	Hello, I'm a language model, I'm a problem solver in languages."

	At the same time, she said we can understand an idea like "reactive programming," because programming is what you create
	\label{example:gpt2-output-sample}
\end{example}

\todo{write about BPE, sub-word tokens etc.}

\section{Meteor Stegosystem}
\label{sec:meteor}
In their paper \cite{Meteor2021} Kaptchuk et al. present the Meteor stegosystem, a provably secure steganographic system for realistic distributions as approximated by ML models.
The central idea behind Meteor is that the encoding party Alice manipulates the randomness used in the underlying sampling scheme to embed hidden text in the generated token sequence.
The decoding party Bob receives the tokens from Alice and recovers a prefix of the random value used in sampling.

While this is easily possible for the encoding party (because they control the randomness used in sampling), it is unclear wether or not the decoding party can extract (a prefix of) the randomness in the decoding process to recover parts of the hiddentext message.
Luckily, many generative neural networks, such as GPT-2, possess this property. 
The Meteor authors have introduced the concept of a Ranged Randomness Recoverable Sampling Scheme, or RRRSS, see \autoref{def:rrrss}.
This formally models the behaviour described above.

\begin{definition}[Ranged Randomness Recoverable Sampling Scheme, Kaptchuk et al.]
	\label{def:rrrss}
	We call 
	$(Sample_{\mathcal{D}}^\beta, Recover_{\mathcal{D}}^\beta, \mathcal{D}, \beta, \mathcal{H})$ 
	with a ppt. algorithm 
	$Sample_{\mathcal{D}}^\beta \colon (\mathcal{H}, r) \rightarrow s$ 
	and a ppt. algorithm 
	$Recover_{\mathcal{D}}^\beta \colon (\mathcal{H}, s) \rightarrow \mathcal{R}$ 
	a Ranged Randomness Recoverable Sampling Scheme (RRRSS) over distribution $\mathcal{D}$ with precision $\beta$, if:
	
	\begin{enumerate}
		\item $Sample_{\mathcal{D}}^\beta(h, r)$ on history $h \in \mathcal{H}$ and randomness $r \in \{0,1\}^\beta$, sample an output $s$ from its underlying distribution $\mathcal{D}$
		\item $Recover_{\mathcal{D}}^\beta(h, s)$ on history $h \in \mathcal{H}$ and sample s, output the set $\mathcal{R} = \{ r \in \{0,1\}^\beta | Sample_{\mathcal{D}}^\beta(\mathcal{H}, r) = s$ of possible values for $r$
	\end{enumerate}
\end{definition}

Now, with the definition of an RRRSS, the authors define sampling and recovery algorithms for the GPT-2 model:

\begin{Pseudocode}
algorithm $Sample_{\mathcal{GPT}}^\beta(	h, r)$
	Output: Token $t$
	$\mathcal{T}, \mathcal{P} \leftarrow Next_{\mathcal{GPT}}(h)$
	$cuml \leftarrow 0$
	for $i \in \{ 0, 1, \dots, | \mathcal{T} - 1 | \}$ do
		$cuml \leftarrow cuml + \mathcal{P}[i]$
		if $cuml > r$ then
			Output $t \leftarrow \mathcal{T}[i]$
	Output $t \leftarrow \mathcal{T}[|\mathcal{T}|-1]$
\end{Pseudocode}

\begin{Pseudocode}
algorithm $Recover_{\mathcal{GPT}}^\beta(h, r)$
	Output: Randomness set $\mathcal{R}$
	$\mathcal{T}, \mathcal{R} \leftarrow Next_{\mathcal{GPT}}(h)$
	$cuml \leftarrow 0$
	for $i \in \{ 0, 1, \dots, | \mathcal{T} - 1 | \}$ do
		if $\mathcal{T}[i] = s$ then
			Output $\mathcal{R} \leftarrow \{ r \in \{ 0, 1\}^\beta | cuml \leq r < cuml + \mathcal{P}[i] \}$
		$cuml \leftarrow cuml + \mathcal{P}[i]$
	Output $\mathcal{R} \leftarrow \emptyset$
\end{Pseudocode}

Here, $Next_{\mathcal{GPT}}$ is the algorithm which generates the probability distribution for the next token, as shown in \autoref{fig:generative-network}.
The tokens are denoted by $\mathcal{T}$ and probabilities are denoted by $\mathcal{P}$.
The $Sample_{\mathcal{GPT}}^\beta$ algorithm cumulates the token probabilities $p_i$ into $cuml$ until the threshold $r$ has been reached.
Then the current token $t_i$ is the token to be sampled from the distribution.
We can see that the order of tokens is relevant for the recovery to succeed, so we expect $Next_{\mathcal{GPT}}^\beta$ to return token probabilities in a somehow ordered fashion.

Now that a RRRSS scheme for GPT-2 is established, we can commence with the definition of the stegosystem $\mathcal{S} = \left( Encode_{\mathcal{M}}^\beta, Decode_{\mathcal{M}}^\beta \right)$.
Kaptchuk et al. described those algorithms as follows while introducing three helper functions $LenPrefix^\beta$, $Prefix^\beta$ and $KeyGen_{\mathcal{M}}^\beta$:


\begin{Pseudocode}[caption={Meteor Encode Algorithm}, label={alg:encode}]
algorithm $Encode_{\mathcal{M}}^\beta(k_{prg}, m, \mathcal{H})$
	Output: Stegotext message $c$
	$c \leftarrow \epsilon,~ n \leftarrow 0$
	while $n < |m|$ do
		$mask \leftarrow PRG.Next(k_{prg})$
		$r \leftarrow m[n:n+\beta] \oplus mask$
		$c_i \leftarrow Sample_{\mathcal{M}}^\beta(\mathcal{H}, r)$
		$\mathcal{R} = Recover_{\mathcal{M}}^\beta(\mathcal{H}, c_i)$
		$n_i \leftarrow LenPrefix^\beta(\mathcal{R})$
		$c \leftarrow c || c_i, n \leftarrow n+n_i, \mathcal{H} \leftarrow \mathcal{H}||c_i$
	Output $c$
\end{Pseudocode}

$Encode$ uses the PRG to sample a pseudorandom mask and applies that mask to the next $\beta$ bits of $m$ into $r$.
Since $mask$ is pseudorandom, this makes $r$ pseudorandom as well.
We then use $r$ to sample from the RRRSS distribution to get the next token $c_i$.
Afterwards, $Recover_{\mathcal{M}}^\beta$ and $LenPrefix^\beta$ are queried to get the number of bits $n_i$ recoverable from $r$.
This procedure is repeated until $|m|$ bits of hiddentext are encoded.
Consider that the output of $Encode$ is deterministic, i.e. for a given $k_{prg}$ and $\mathcal{H}$, the same stegotext $c$ will be generated.


\begin{Pseudocode}[caption={Meteor Decode Algorithm}, label={alg:decode}]
algorithm $Decode_{\mathcal{M}}^\beta(k_{prg}, c, \mathcal{H})$
	Output: Plaintext message $m$
	$m \leftarrow \epsilon$
	Parse $c$ as $\{ c_0, c_1, \dots, c_{|c|-1} \}$
	for $i \in \{0, 1, \dots, |c|-1 \}$ do
		$\mathcal{R} = Recover_{\mathcal{M}}^\beta(\mathcal{H}, c_i)$
		$m_i \leftarrow Prefix^\beta(\mathcal{R})$
		$mask \leftarrow PRG.Next(k_{prg})$
		$m \leftarrow m || (m_i \oplus mask[0: |m_i|]$
		$\mathcal{H} \leftarrow \mathcal{H}||c_i$
	Output $m$
\end{Pseudocode}

$Decode$ decodes the stegotext to the hiddentext message left-to-right. 
It repeatedly uses $Recover_{\mathcal{M}}^\beta$ and $Prefix^\beta$ to recover bits from the pseudorandom value used to sample from the distribution.
It then generates a mask from the PRG (which will be the same mask generated in the encoding step) to decrypt the original message.

\begin{Pseudocode}[caption={Meteor LenPrefix Algorithm}]
algorithm $LenPrefix^\beta(\mathcal R = \{ r_1, \dots r_n \})$
	Output: Length $l$
	$l \leftarrow 1$
	while $l < \beta$ do
		if $\exists i, j \in \{ 1, \dots, n \}$ such that $r_i[0: l] \neq r_j[0:	l]$ then
			Output $l-1$
		$l \leftarrow l+1$
	Output $l$
\end{Pseudocode}

\begin{Pseudocode}[caption={Meteor Prefix Algorithm}]
algorithm $Prefix^\beta(	\mathcal R = \{ r_1, \dots r_n \})$
	Output: Bit String $s$
	Output $r_1[0: LenPrefix^\beta(\mathcal{R})]$
\end{Pseudocode}

\begin{Pseudocode}[caption={Meteor KeyGen Algorithm}]
algorithm $KeyGen_{\mathcal{M}}^\beta(1^\lambda)$
	Output: Key $k_{prg}$
	Output $k_{prg} = PRG.Setup(1^\lambda)$
\end{Pseudocode}

For correctness, the authors argue that a steganographic protocol is correct if encoded messages can be recovered using the $Decode$ algorithm except for negligible in key length.
They introduce another formal definition, see Definition \ref{def:correctness-kaptchuk}.
In \autoref{chap:correctness}, we will argue how correctness is actually not achieved using GPT-2 and how we can modify the stegosystem described here to achieve correctness while introducing exponential overhead.

\begin{definition}[Correctness, Kaptchuk et al.]
\label{def:correctness-kaptchuk}
A steganographic protocol must be correct, i.e. except with negligible probability an encoded message can be recovered using the decode algorithm. Formally, for any $k \leftarrow KeyGen(1^\lambda)$,

$$\mathop{Pr}[Decode_{\mathcal{D}}(k, Encode_{\mathcal{D}}(k, m, \mathcal{H}), \mathcal{H}) = m] \geq 1 - \mathop{negl}(\lambda)$$
\end{definition}

Meteor's security proof is also outlined by the authors.
They argue that a steganographic protocol is secure if no ppt. adversary $\mathcal{A}$ is able to distinguish the output of the stegosystem from the output of a covertext distribution, i.e. an oracle $\mathcal{O}$ sampling from the distribution. 
More formally, they define security in \autoref{def:sec-kaptchuk}. 
Furthermore, they argue that the Meteor stegosystem is secure by reduction to the PRG real-or-random game. 
In \autoref{chap:security}, we will discuss how \autoref{def:sec-kaptchuk} is weaker than Hopper's definition of universally steganographic secrecy introduced in \autoref{sec:prov-sec-steg}.
We will also discuss how an attacker can distinguish Meteor's output from a random oracle $\mathcal{O}_{\mathcal{D}}$ using polynomial queries, time and message length.

\begin{definition}[Steganographic Secrecy, Kaptchuk et al.]
	\label{def:sec-kaptchuk}
	We say that a steganographic scheme $\Sigma_{\mathcal{D}}$ is secure against chosen hiddentext attacks if for all ppt. adversaries $\mathcal{A}_{\mathcal{D}}$,
	
	$$k \leftarrow KeyGen_{\mathcal{D}}(1^\lambda)$$
	$$\left| \mathop{Pr}\left[ \mathcal{A}_{\mathcal{D}}^{Encode_{\mathcal{D}}(k, \cdot, \cdot)}=1 \right] - \mathop{Pr}\left[ \mathcal{A}_{\mathcal{D}}^{\mathcal{O}_{\mathcal{D}}(\cdot, \cdot)}=1 \right] \right| < \mathop{negl}(\lambda)$$
\end{definition}


\chapter{Correctness}
\label{chap:correctness}

The Meteor authors have released a demo of Meteor at \cite{MeteorDemo2021}. There, they state that ``due to issues with the GPT-2 algorithm interface, you sometimes may see extra output from a decoded stegotext. This does not impact the underlying security of the scheme''.
During experimentation, I have concluded that the cause of this is due to the way sub-word based GNNs, such as GPT-2, represent text and not due to issues with the algorithm interface.

After describing the issue at hand, I will show that the Meteor Stegosystem is actually incorrect with respect to Hopper's definition as defined in \autoref{def:correctness-hopper} by providing a counterexample.
To restore correctness, I then present a change to Meteor's $Recover$ algorithm to successfully reconstruct the original hiddentext message while inducing overhead exponential in stegotext length.

\section{Ambiguous Tokenization}

In \autoref{sec:generative-neural-networks}, we discuss that GPT-2 uses a token dictionary to represent text.
To improve text generation and better recognize relations between words, sub-word tokenization might can words into sub-words.
Sub-word tokenization is opposed to word tokenization, which splits a text by spaces, and character-based tokenization, which tokenizes text character by character.
Sub-word tokenization allows for more flexible deep learning, as the framework can easier connect related words to each other. 
The model might learn that, by adding the suffix ``n't'' to a verb, one can invert the meaning of that verb, e.g. ``haven't'' is the opposite of ``have'' and those words are in close relation to each other.
This technique proves to be very effective in enhancing GNN performance.

On the other hand, this allows for one word to have multiple representations in the ML model.
For example, the word ``doesn't'' might be tokenized by a tokenizer $\mathcal{T}$ as $\mathcal{T}(\textrm{``doesn't''}) = ( \textrm{``do''}, \textrm{``es''}, \textrm{``n't''} )$. It could also be tokenized as $\mathcal{T}(\textrm{``doesn't''}) = ( \textrm{``does''}, \textrm{``n't''} )$ or even $\mathcal{T}(\textrm{``doesn't''}) = ( \textrm{``d''}, \textrm{``o''}, \textrm{``e''}, \textrm{``s''}, \textrm{``n''}, \textrm{``'t''})$.
This effect is what we call ambiguous tokenization.

\begin{definition}[Ambiguous Tokenization]
	Let $\mathcal{M}$ be a GNN with tokens $\mathcal{T} = \{ t_1, t_2, \dots, t_n \}$.
	We say that $\mathcal{M}$ has an ambiguous tokenizations, if there exists at least two ordered list of indices $I = (i, j, k, \dots), J = (q, r, s, \dots), I \neq J$ with $T_1 = t_i || t_j || t_k || \dots$ and $T_2 = t_q || t_r || t_s || \dots$ such that $t_I = t_J$.
\end{definition}

\section{Correctness Of The Meteor Stegosystem}

In \autoref{alg:decode}, we see that during decoding, the stegotext $c$ generated by \autoref{alg:encode} should be parsed as $c = \{c_0,c_1, \dots, c_{|c|-1|}\}$.
This task is performed by a tokenizer $\mathcal{T}$ associated with the underlying model.
Here, the $Decode$ algorithm expects that the parsing of $c$ can recover the originally generated $c_i$.
This unfortunately is, at least for the GPT-2 model, not the case for some combinations of stegotext, key and history.
In \autoref{sec:alg-rec-tok-candidates}, I show how to fix these correctness issues.

\begin{theorem}
The Meteor Stegosystem is not correct.
\end{theorem}

\begin{proof}
For Meteor to be correct, it must fulfil \autoref{def:correctness-kaptchuk}, i.e. for any $k \leftarrow KeyGen(1^\lambda)$ and hiddentext $m$:

$$\mathop{Pr}[Decode_{\mathcal{D}}(k, Encode_{\mathcal{D}}(k, m, \mathcal{H}), \mathcal{H}) = m] \geq 1 - \mathop{negl}(\lambda)$$

We proof incorrectness by providing a counterexample. 
Let

\begin{lstlisting}[breaklines]
k = 0xb95e03a1d01b304f11dcf2bc844e5fd3cbed41253b0506876004207b2c2a10e
    2d89c1a40e93530bfcfaaee54e66ae048d2d2a536615b0a81afe792883877d5b6
m = 'Hello world'
H = 'Despite a long history of research and wide-spread applications to censorship resistant systems, practical steganographic systems capable of embedding messages into realistic communication distributions, like text, do not exist.\n\n'
\end{lstlisting}

The stegotext for these inputs when using the Meteor demo code is

\begin{lstlisting}
c = '\nZeus communication system, controlled by anÆ2 desktop mic with'
\end{lstlisting}

When passed to the GPT-2 Tokenizer, the substring ``Zeus'' of $c$ is parsed as \lstinline{['Z', 'eus']}, while the encoding process has generated ``Zeus'' with token sequence \lstinline{['Ze', 'us']}.
Therefore, the stegotext $c$ cannot be successfully decoded to the original hiddentext ``Hello world'', which violates correctness.
\end{proof}

After we have seen that this problem appears at least once by finding a counterexample, another question arises:
How often does this happen?
To approach this question, I have conducted an experiment which attempts to estimate the rate at which these incorrect decodings occur.
For that, I have encoded the entire book of Hamlet in chunks of 1024 characters with random keys and histories of length 128 picked at random from Hamlet as well using a modified version of the Meteor demo code from \cite{MeteorDemo2021}.

The experiment shows that most stegotexts have at least one mismatch if they are of significant length.
Therefore, we have to find a way to deal with these failed decodings.
In \autoref{sec:alg-rec-tok-candidates}, I present approaches to recover from decoding errors while introducing exponential computational overhead.


\begin{figure}[htbp]
	\centering
	\input{fig_meteor_stats_mismatch_count_arithmetic.tikz}
	\caption{Binned mismatch count statistics. The x-axis shows the number of tokenization mismatches between the encoding and the decoding party}
	\label{fig:meteor-stats-mismatch-count}
\end{figure}
\todo{improve plot to increase readability}



\section{Algorithmic Reconstruction Of Token Candidates}
\label{sec:alg-rec-tok-candidates}

Unfortunately, with sub-word tokenization, the decoding party cannot decide how the stegotext has been tokenized by the encoder.
To allow successful decoding of ambiguously tokenized stegotexts, I will in this section introduce algorithms to detect and fix wrong tokenizations. 


For that we have to modify the encoding step of Meteor to allow detection of wrong tokenization.
Before encoding, add a recurring marker $q$ into the hiddentext every $\gamma$ bits.
This marker helps the decoder to decide if the decoding is still correct up to this point.
By using markers, we introduce $|q| \cdot \frac{|m|}{\gamma}$ bits of overhead to the hiddentext.
We modify \autoref{alg:encode} as follows:

\begin{Pseudocode}[caption={Marked Encode Algorithm}, label={alg:marked-encode}]
algorithm $MarkedEncode_{\mathcal{M}}^{\beta, \gamma}(k_{prg}, m, q, \mathcal{H})$
	Output: Stegotext message $c$
	$c \leftarrow \epsilon,~ n \leftarrow 0$
	$m^* \leftarrow \epsilon,~ j \leftarrow 0$
	while $j < |m|$ do
		$m^* \leftarrow m^* || m_j$
		$j \leftarrow j + 1$
		if $j \equiv 0~ \pmod \gamma$
			$m^* \leftarrow m^* || q$
	while $n < |m|$ do
		$mask \leftarrow PRG.Next(k_{prg})$
		$r \leftarrow m[n:n+\beta] \oplus mask$
		$c_i \leftarrow Sample_{\mathcal{M}}^\beta(\mathcal{H}, r)$
		$\mathcal{R} = Recover_{\mathcal{M}}^\beta(\mathcal{H}, c_i)$
		$n_i \leftarrow LenPrefix^\beta(\mathcal{R})$
		$c \leftarrow c || c_i, n \leftarrow n+n_i, \mathcal{H} \leftarrow \mathcal{H}||c_i$
	Output $c$
\end{Pseudocode}

Now the decoding party has to verify that every $\lambda$ bits, the marker $q$ appears.
If not, a decoding error has occured in the stegotext.
We modify \autoref{alg:decode} as follows:


\begin{Pseudocode}[caption={Marked Decode Algorithm}, label={alg:marked-decode}]
algorithm $MarkedDecode_{\mathcal{M}}^\beta(k_{prg}, c, \mathcal{H})$
	Output: Plaintext message $m$
	$m \leftarrow \epsilon$
	!!TODO!!
	Parse $c$ as $\{ c_0, c_1, \dots, c_{|c|-1} \}$
	for $i \in \{0, 1, \dots, |c|-1 \}$ do
		$\mathcal{R} = Recover_{\mathcal{M}}^\beta(\mathcal{H}, c_i)$
		$m_i \leftarrow Prefix^\beta(\mathcal{R})$
		$mask \leftarrow PRG.Next(k_{prg})$
		$m \leftarrow m || (m_i \oplus mask[0: |m_i|]$
		$\mathcal{H} \leftarrow \mathcal{H}||c_i$
	Output $m$
\end{Pseudocode}
\todo{Modify Decode to MarkedDecode}

For the modification above, we need a helper algorithm which generates all possible tokenizations for a given stegotext.
We can represent these tokenization candidates in a graph using the ML models's tokens $\mathcal{T}$.
When passed a string $c$, the algorithm $TokenizeCandidates$ generates a directed, acyclic graph $G = (V, E)$.


\begin{figure}[htbp]
	\centering
	\begin{tikzpicture}
		\node[block] (hello) {hello};
		\node[block, right=15mm of hello] (lo) {lo};
		\node[block, above=15mm of lo] (llo) {llo};
		\node[block, above=15mm of llo] (ello) {ello};
		\node[block, below=15mm of lo] (o) {o};
		\node[block, right=15mm of lo] (bot) {$\epsilon$};
		\draw[->] (hello) to node[above] {h} (ello);
		\draw[->] (hello) to node[above] {he} (llo);
		\draw[->] (hello) to node[above] {hel} (lo);
		\draw[->] (hello) to node[above] {hell} (o);
		\draw[->] (ello)  to node[above] {e} (llo);
		\draw[->] (ello)  edge[bend left=30,->] (lo) node[midway,above] {el};
%		\draw[->] (ello)  edge[bend left=30,->] (o)  node[above] {ell};
		\draw[->] (ello)  to node[above] {ello} (bot);
		\draw[->] (llo)   to node[above] {l} (lo);
%		\draw[->] (llo)   edge[bend right=30,->] to node[above] {ll} (o);
		\draw[->] (llo)   to node[above] {llo} (bot);
		\draw[->] (lo)    to node[above] {l} (o);
		\draw[->] (lo)    to node[above] {lo} (bot);
		\draw[->] (o)     to node[above] {o} (bot);
%		\draw[->] (hello) edge[bend right=120,->] to node[above] {hello} (bot);
	\end{tikzpicture}
	\caption{Example graph generated by TokenizeCandidates(``hello'')}
	\label{fig:ex-graph-tokenize-candidates}
\end{figure}

\begin{lstlisting}[language=Python]
def tokenize_candidates(self, text):
    tokens = bucketize_tokens(self.encoder.items())
    tokenize_edges = {}
    for i in range(len(text)):
        _do_tokenize_candidates(self, text[-(i + 1):], tokens, tokenize_edges)
    return tokenize_edges
def _do_tokenize_candidates(self, text, tokens, tokenize_edges):  # parent: Union[str, Node]
    if text in tokenize_edges or text == '':
        return
    # for token, id in self.encoder.items():
    for token, id in tokens[text[0]]:
        if text.startswith(token):
            remainder = text[len(token):]
            if text not in tokenize_edges:
                tokenize_edges[text] = []
            if remainder not in tokenize_edges[text]:
                tokenize_edges[text] += [(remainder, token, id, -len(token))]
def bucketize_tokens(tokens):
    bins = {}
    ignored_token_ids = [
        220  # 'space' token
    ]
    for token, id in (t for t in tokens if t[1] not in ignored_token_ids):
        bins[token[0]] = bins.setdefault(token[0], []) + [(token, id)]
    return bins
\end{lstlisting}
\todo{rewrite as pseudocode}




\chapter{Two-Way-Communication}

As discussed in \autoref{sec:alg-rec-tok-candidates}, it is possible 

\chapter{Security Analysis}
\label{chap:security}

TODO

\section{Comparison Of Meteor's And Hopper's Security Game}

TODO

\chapter{Conclusion}


% In a German thesis write: \subsection{Zusammenfassung und Ausblick}


% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
% !!! Your action is needed here !!!
% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%
% Replace the following with your conclusion

TODO write a conclusion.
\end{document}

%  LocalWords:  LaTeX tex moretexcs Lübeck pdf uzl lualatex bibtex th
%  LocalWords:  TechReport Kernighan Lamport's Tantau's Tantau cls kZ
%  LocalWords:  Mustermann emacs oldschool pdflatex texmf utf biber
%  LocalWords:  biblatex Alphabetische Bibliographie Numerische VIIa
%  LocalWords:  varioref german Einleitung Beiträge dieser Arbeit xml
%  LocalWords:  Ergebnisse Verwandte Arbeiten Aufbau nucleotide VIIc
%  LocalWords:  ensembl amino phylogenetic Alexa Siri decrypt versa
%  LocalWords:  cryptographic pre nondeterministic deterministically
%  LocalWords:  Beutelspacher Untersuchungen zum genetischen sep llcc
%  LocalWords:  Beispiel tikz jpg png Alegrya Kasimir Malewitsch PGF
%  LocalWords:  Lamport Institut für Theoretische Informatik zu url
%  LocalWords:  Universität Springer DowneyF Downey Parameterized doi
%  LocalWords:  BibLaTeX Kime Philipp urldate Mittelbach hyperref Lua
%  LocalWords:  Rahtz Oberdiek Heiko Braams Bezos López fontspec Das
%  LocalWords:  Arseneau amsmath ist Tipps und zur Formulierung
%  LocalWords:  mathematischer Gedanken Mathematik Studienanfänger
%  LocalWords:  Albrecht Vieweg Teubner Verlag
