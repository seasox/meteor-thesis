\chapter{Previous Work}
\label{chap:previous-work}

\section{Provably Secure Steganography}
\label{sec:prov-sec-steg}

Steganography extends the concepts of cryptography to hide the mere existence of a message.
This makes it robust against censorship and prohibition.
While cryptography can be used to hide the contents of a confidential message, its use is easily discoverable by adversaries.


Steganography is usually modelled using the Prisoners' Problem, as introduced by \cite{Simmons1983}.
The participants Alice and Bob are imprisoned.
While they can exchange messages, their contents are surveilled by a Warden.
Alice and Bob now want to craft and exchange escape plans.
Warden observes the messages exchanged between Alice and Bob.
He tries to distinguish the exchange of the escape plan from innocuous messages.
An illustration of the Prisoners' Problem can be found at \autoref{fig:prisonersgame}.

\begin{figure}[hb]
\centering
\begin{tikzpicture}
	\node[block] (alice) {Alice};
	\node[block, right=3cm of alice] (warden) {Warden};
	\node[block, right=3cm of warden] (bob) {Bob};
	\node[left=of alice] (m) {m};
	\node[above=of alice] (k) {k};
	\node[below=of alice] (h) {history $h$};
	\node[right=of bob] (mb) {m};
	\node[above=of bob] (kb) {k};
	\node[below=of bob] (hb) {history $h$};
	\node[block, below=of warden] (steganalysis) {Steganalysis: is $s$ stegotext?};
	\node[below left=of steganalysis] (isstego) {suppress message};
	\node[below right=of steganalysis] (notstego) {forward message to recipient};
	\draw[->] (alice) -- node[above] {$s$} (warden);
	\draw[->] (warden) -- node[above] {$s$} (bob);
	\draw[->] (m) -- (alice);
	\draw[->] (k) -- (alice);
	\draw[->] (h) -- (alice);
	\draw[->] (bob) -- (mb);
	\draw[->] (kb) -- (bob);
	\draw[->] (hb) -- (bob);
	\draw[->] (warden) -- (steganalysis);
	\draw[->] (steganalysis) -- node[below] {Yes} (isstego);
	\draw[->] (steganalysis) -- node[below] {No} (notstego);
\end{tikzpicture}
\caption{
An illustration of The Prisoners' Problem, as described in \cite{Simmons1983}.
Two accomplices in a crime (Alice and Bob) are imprisoned in widely separated cells.
They want to escape and try to coordinate their escape plans.
They are surveilled by a warden who monitors their communication.
If Warden detects a hidden message by successfully distinguishing it from an innocuous message, they suppress the message.
Otherwise, the message is forwarded.
It is expected that Alice and Bob have been able to exchange a common secret key $k$.
They also have access to the same initial history $h$, which can be public.
}
\label{fig:prisonersgame}
\end{figure}

In steganography, a message is represented by a sequence of documents from an underlying distribution.
Most approaches to steganography use photographs as underlying distribution and hide information in the images' noise.
If the original photograph is unknown to Warden, Alice can encode hidden text in the original photograph.

In his dissertation ``Toward a theory of Steganography'', \cite{Hopper2004} establishes a cryptographic point of view on steganography. 
This allows us to do cryptanalysis on steganographic protocols.
Since we work on sequences of documents, Hopper introduces a notion of a channel.
The channel we define our stegosystem on contains all possible sequences of documents which Alice might send to Bob.

\begin{definition}[Channel, \cite{Hopper2004}]
Let $D$ be an efficiently recognizable, prefix-free set of strings, or documents.
A \emph{channel} $\mathcal{C}$ is a distribution on sequences $s \in D^\Omega$.
\end{definition}

A sequence $s \in D^\Omega$ is generated by repeatedly sampling from $D$.
While Alice could generate this sequence in one go, it might come in handy to generate it iteratively.
To do that, Alice draws a single document repeatedly and uses the previously drawn documents~ --~ the history $h$~ --~ as condition to channel $\mathcal{C}$.

\begin{definition}[Marginal Channel Distribution, \cite{Hopper2004}]
Let $\mathcal{C}$ be a channel over distribution $D$ and $h \in D^n$ a history of documents of length n.
We let $C_h$ denote the \emph{marginal channel distribution on a single document from $D$} conditioned on the history $h$ of already drawn documents.
We let $C_h^l$ denote the \emph{marginal channel distribution on a sequence of $l$ documents} from $D$ conditioned on the history $h$ of already drawn documents.
\end{definition}

Later on when we introduce the concept of steganographic secrecy, we will need the notion of an \emph{always-informative} channel. An always-informative channel is a channel with bounded minimum-entropy.

\begin{definition}[Support]
Let $\mathcal{D}$ be a probability distribution.
We denote the \emph{support of $\mathcal{D}$} as $\mathop{supp}(\mathcal{D}) = \{ x \in \mathcal{D} \colon Pr_{\mathcal{D}}[x] \neq 0 \}$.
\end{definition}

\begin{definition}[Entropy, \cite{Hopper2004}]
Let $\mathcal{D}$ be a distributon with finite support $X$.
We define the \emph{minimum entropy of} $\mathcal{D}$ as

	$$H_{\infty}(D) = \min_{x \in \mathcal{X}}\left\{ \log_2 \frac{1}{ Pr_{\mathcal{D}}[x] } \right\}$$
\end{definition}

\begin{definition}[Always-Informative Channel, \cite{Hopper2004}]
	Let $L > 0,~ \alpha > 0, \beta > 0$. 
	We call a channel \emph{$\mathcal{C}$ $(L, \alpha, \beta)$-informative} if for all $h \in D^L$,~ $\mathcal{C}$ satisfies $Pr_{\mathcal{C}}[h] = 0$ or $H_{\infty}(C_{\mathcal{h}}^\beta) \geq \alpha$.
	
	If a channel $\mathcal{C}$ is $(\alpha, \beta)$-informative for all $L > 0$, we call $\mathcal{C}$ \emph{$(\alpha, \beta)$-always informative} or simply \emph{always informative}.
\end{definition}

Now that we have established the notion of channels and always-informative channels, we can commence with the definiton of a stegosystem.
In section 3 of his thesis, Hopper defines a stegosystem as follows:

\begin{definition}[Stegosystem, \cite{Hopper2004}]
\label{def:stegosystem}
A \emph{steganographic protocol} $\mathcal{S}$, or \emph{stegosystem}, is a pair of probabilistic algorithms:

\begin{itemize}
	\item $\mathcal{S}$.Encode (abbreviated $SE$) takes as input a key $k \in \{0,1\}^\lambda$, a string $m \in \{0,1\}^*$ (the hiddentext), and a message history $h \in D^*$.
		$SE(k, m, h)$ returns a sequence of documents $s_1||s_2||\dots||s_l$ (the stegotext) from the support of $\mathcal{C}_h^l$.
	\item $\mathcal{S}$.Decode (abbreviated $SD$) takes as input a key $k \in \{0,1\}^\lambda$, a sequence of documents $s_1||s_2||\dots||s_l$, and a message history $h \in D^*$.
		$SD(k, s, h)$ returns a hiddentext $m \in \{0,1\}^*$
\end{itemize}
\end{definition}

We want our stegosystem to be correct.
To achieve that, the decoding of a given stegotext $s$, given the correct key $k$ and history $h$, should yield the original hiddentext $m$.
Hopper defines the correctness of a stegosystem as follows:

\begin{definition}[Correctness, \cite{Hopper2004}]
\label{def:correctness-hopper}
A stegosystem $\mathcal{S}$ is \emph{correct} if for every polynomial $p(\lambda)$, there exists a negligible function $\mu(\lambda)$ such that $SE$ and $SD$ also satisfy the relationship:

$$\forall m \in \{0,1\}^{p(\lambda)}, h \in D^* \colon Pr(SD(k, SE(k, m, h), h) = m) \geq 1 - \mu(\lambda)$$

where the randomization is over the key $k$ and any coin tosses of $SE$, $SD$, and the oracles accessed by $SE$, $SD$.
\end{definition}

The introduction of a \emph{negligible} polynomial $\mu(\lambda)$ allows the stegosystem to sometimes yield wrong results, but only with probability negligible in the security parameter $\lambda$.
While stegosystem with $\mu(\lambda) = 0$ can exist, the introduction of negligible polynomials proves helpful for reductions on established cryptographic primitives. 

Since we want our stegosystem to be prone to analysis by warden, we also need a definition of security.
We want any warden to be unable to efficiently detect the use of our stegosystem without knowledge of our secret key $k$.
As described above in the Prisoners' Game, we don't want any warden to be able to distinguish a stegotext $s := SE(k, m, h)$ from a covertext $c$ randomly sampled from the underlying distribution $\mathcal{C}$.
If the warden is allowed to choose the hiddentext $m$ used to generate $s$, we call warden a \emph{chosen hiddentext attacker}. 

If the warden can distinguish the stegotext from a covertext, he has an advantage against the stegosystem in the distinguishing game.
The insecurity of a stegosystem is the maximum possible advantage any attacker can have.
The attackers are limited by running time $t(\lambda)$, number of queries $q(\lambda)$ and total length $l(\lambda)$ of hiddentexts.
Formally, Hopper defined a chosen hiddentext attack as follows:

\begin{definition}[Chosen Hiddentext Attack, \cite{Hopper2004}]
In a chosen hiddentext attack, $W$ is given access to a ``mystery oracle'' $M$ which is chosen from one of the following distributions:

\begin{enumerate}
\item $ST$: The oracle $ST$ has a uniformly chosen key $k \leftarrow U_\lambda$ and responds to queries $(m, h)$ with a StegoText drawn from $SE(k, m, h)$.
\item $CT$: The oracle $CT$ has a uniformly chosen $k$ as well, and responds to queries $(m, h)$ with a CoverText of length $\mathcal{l} = |SE(k, m, h)|$ drawn from $\mathcal{C}_h^{\mathcal{l}}$
\end{enumerate}

$W^M(1^\lambda)$ outputs a bit which represents its guess about the type of $M$.

We define $W$'s advantage against a stegosystem $\mathcal{S}$ for channel $\mathcal{C}$ by

$$Adv_{\mathcal{S}, \mathcal{C}, W}^{ss}(\lambda) = |Pr[W^{ST}(1^\lambda)=1] - Pr[W^{CT}(1^\lambda)=1]|$$

where the probability is taken over the randomness of $ST$, $CT$, and $W$. 

Define the insecurity of $\mathcal{S}$ with respect to channel $\mathcal{C}$ by

$$InSec_{\mathcal{S}, \mathcal{C}}^{ss}(t, q, l, \lambda) = \max_{W \in \mathcal{W}(t, q, l)} \left\{ Adv_{\mathcal{S}, \mathcal{C}, W}^{ss}(\lambda) \right\}$$

where $\mathcal{W}(t, q, l)$ denotes the set of all adversaries which make at most $q(\lambda)$ queries totalling at most $l(\lambda)$ bits (of hiddentext) and running in time at most $t(\lambda)$.
\end{definition}

\begin{definition}[Steganographic Secrecy, \cite{Hopper2004}]
\label{def:sec-hopper}
A Stegosystem $\mathcal{S}_\lambda$ is called $(t,q,l,\epsilon)$ steganographically secret against chosen hiddentext attack for the channel $\mathcal{C}$ ($(t,q,l,\epsilon)$-SS-CHA-$\mathcal{C}$) if $InSec_{\mathcal{S},\mathcal{C}}^{ss}(t,q,l,\lambda) \leq \epsilon$
\end{definition}

\begin{definition}[Universal Steganographic Secrecy, \cite{Hopper2004}]
A Stegosystem $\mathcal{S}$ is called $(t, q, l, \epsilon)$-universally steganographic secret agains chosen hiddentext attack ($(t,q,l,\epsilon)$-SS-CHA if it is $(t,q,l,\epsilon)$-SS-CHA-$\mathcal{C}$ for every always-informative channel $\mathcal{C}$
\end{definition}

Lastly, we call a Stegosystem $\mathcal{S}$ \emph{universally steganographic secret against chosen hiddentext attacks} (USS-CHA) if for every channel $\mathcal{C}$ and for any ppt. adversary $W \in \mathcal{W}(t,q,l,\lambda)$, $Adv_{\mathcal{S},\mathcal{C},W}^{ss}(\lambda)$ is negligible in $\lambda$.

\section{Generative Neural Networks}
\label{sec:generative-neural-networks}

Generative Neural Networks (GNN) establish a model which approximate realistic distributions such as natural language.
In the last few years, the artificial intelligence community achieved tremendous progress in building powerful deep learning based models.

One of the milestones in deep learning based generative neural networks certainly is the development of the Generative Pretrained Transformer, or GPT, by OpenAI.
The second iteration of GPT, called GPT-2, has been introduced in \cite{OpenAI2019} and became one of the industry-standard models in the natural language processing (NLP) community.
While it is mostly known for its capabilities in text generation, it is not specifically designed to generate blog posts or newspaper articles.
It can also be used for a variety of other NLP tasks, such as question answering, sentiment detection or automatic text summarization.

We can think of GPT-2 as a function $\mathcal{M} \colon \mathbb{N}^k \rightarrow \mathbb{N}^\ell$. 
It takes as input a vector of dimension $k$ and transforms it into another vector of dimension $\ell$. 
The input vector is a numeric representation of a history $h$, which is a string.
The output is a $\ell$-dimensional logits vector with components $l_i$ for each token $t_i$ in the model's token dictionary, which we can transform to a probability distribution.

Since we work with text, we need to find a way to transform our history $h$ into the realm of numbers and vectors.
To do that, we use a set of tokens $\mathcal{T} = \{ t_1, t_2, \dots, t_\ell \}$ specific to the model.
In the simplest case, the token set can be the characters or words of the english language.
We can then encode a string $s$ with $|s| \leq k$ into a vector $v = \left(t_i\right)^T_{1 \leq i \leq |s|}, ~t_i \in \mathcal{T}$, fill the remaining $k - |s|$ entries in the vector with a special padding token $\bot$ and pass it to $\mathcal{M}$.
	
While the character or word based approach works, it is hard for the model to learn the relationship between words solely on the sequence of characters, which badly influences the quality of outputs.
Are ``do'' and ``don't'' more closely related then ``do'' and ``to''?
Probably yes.
But from the model's perspective, the vectors $(d, o, \bot, \bot, \bot)^T$ and $(t, o, \bot, \bot, \bot)^T$ are very close to each other (the only differ in one dimension), while $(d, o, \bot, \bot, \bot)^T$ and $(d, o, n, ', t)^T$ differ in all but two dimensions.

To tackle this challenge, most modern text generation models use subword tokenization.
With subword tokenization, a token may not only be a single character or a full word, but also a substring of a word.
This allows the models to better predict relationships between words.
For example, the words ``do'' and ``don't'' are probably closely related.
With subword tokens, we might tokenize ``do'' as $(do, \bot)^T$ and ``don't'' as $(do, n't)^T$.
These vectors only differ in one dimension.
A deep learning model might learn that n't is used to negate a word.
While this approach makes it easier for the model to learn relationships between different words, there exist multiple tokenizations for some words.

After the tokenizer has encoded the history as a vector of subwords -- with each subword token having a unique numeric identifier -- this vector is passed as input to the generative model.

The model's output is a logits vector.
A logits vector is a vector of unweighted activations of the model's underlying layers, where a higher value represents a stronger activation.
Ultimately, we want to have a probability distribution to sample from.
To transform the logits vector to a probability distribution $p = (p_i)_{1 \leq i \leq \ell}$ with $\sum_{i=1}^{\ell} p_i = 1$, we can use the softmax function $\sigma$:

$$\sigma \colon \mathbb{N}^{\ell} \rightarrow [0,1]^{\ell},$$
$$\sigma(z) = \left(\frac{e^{z_i}}{\sum_{j=1}^{\ell} e^{z_j}}\right)^{1 \leq i \leq \ell}$$

When we combine all the previous steps, we get the black box for our ML model as illustrated in \autoref{fig:generative-network}.

\begin{figure}[htpb]
	\centering
	\begin{tikzpicture}
            \node[rectangle, draw=black, minimum width=4cm, minimum height=4cm] (oracle) {ML Model};
            \node[right=of oracle, rectangle, draw=black, minimum width=2cm, minimum height=4cm] (sigmoid) {$\sigma$};
            \node[left=of oracle, rectangle, draw=black, minimum width=2cm, minimum height=2cm] (transform) {Tokenizer};
            \node[left=of transform] (history) {$h$};
            \node[above right=-5mm and 1cm of sigmoid] (t1) {$t_1$};
            \node[below=5mm of t1] (t2) {$t_2$};
            \node[below=5mm of t2] (t3) {$t_3$};
            \node[below=5mm of t3] (t4) {$\dots$};
            \node[below=5mm of t4] (tn) {$t_n$};
            \draw[->] (transform) -- (oracle);
            \draw[->] (history) -- (transform);
            \draw[->] (oracle) -- (sigmoid);
            \draw[->] (sigmoid) -- node[midway,below] {$p_1$} (t1);
            \draw[->] (sigmoid) -- node[midway,below] {$p_2$} (t2);
            \draw[->] (sigmoid) -- node[midway,below] {$p_3$} (t3);
            %\draw[->] (oracle) -- (t4);
            \draw[->] (sigmoid) -- node[midway,below] {$p_n$} (tn);
	\end{tikzpicture}
	\caption{
Generative ML Model as black box function, taking a history as input.
The history $h$ is a string of text which is transformed to a vector $v \in \mathbb{N}^k$ (with model-specific $k$) of token indices.
The model returns a logits vector $z \in \mathbb{N}^{\ell}$ with unscaled outputs.
The logits vector is then transformed to a probability distribution $p = \left(p_i\right)_{1 \leq i \leq \ell}$ with $\sum_{i=1}^{\ell} p_i = 1$ using the softmax function.
	The $p_i$ represent the probability of token $t_i$ to be the next token in the sequence $s$ given a history $h$: $Pr[s_{k+1}=t_i|s=h]$.
}
	\label{fig:generative-network}
\end{figure}

To generate the next token in the sequence, we can sample according to the generated distribution.
After sampling from the distribution, we append the sampled token to the history and repeat the whole process until sufficiently many tokens have been generated.
We then convert the vector to a string to get the generated text.
For an example of GNN model outputs, see \exampleref{example:gpt2-output-sample}.

\begin{example}[Example of GPT-2 model output]
	Hello, I'm a language model, I'm a problem solver in languages."

	At the same time, she said we can understand an idea like "reactive programming," because programming is what you create
	\label{example:gpt2-output-sample}
\end{example}

To use GPT-2 with the Python programming language, we can utilize the Huggingface Transformers library together with PyTorch.
PyTorch is a machine learning framework which implements high performance tensor operations.
The Huggingface Transformers library is built upon PyTorch and gives an API for easy access to several pretrained machine learnings models and allows training of machine learning models.
In the most simple form, the Transformers Pipeline API generates texts using not only GPT-2 but many other text generation models.
A Python program which generates the outputs in \exampleref{example:gpt2-output-sample} can be found at \autoref{alg:gpt2-demo}.


While the use of the pipeline API is very convenient to develop simple text generation or question answering programs, it abstracts some steps that we will need to intercept to use in a stegosystem.
A more sophisticated way to generate text using GPT-2 can be found in \autoref{alg:gpt2-logits-demo}. \todo{replace algorithms refs with listing}
The output of \autoref{alg:gpt2-logits-demo} resembles those shown in \exampleref{example:gpt2-output-sample}. 
This code example shows how we can:

\begin{itemize}
	\item use a tokenizer to convert strings to a vector of tokens
	\item use the model to get a logits vector
	\item transform the logits vector into a probability distribution using the softmax function
	\item sample the next token from the distribution
	\item convert a vector of tokens to a string using the tokenizer
\end{itemize}

As we can see in \autoref{alg:gpt2-logits-demo}, a pseudorandom value \emph{sample} is generated to select the next token \emph{selection} from the distribution.
This is what the Meteor stegosystem will abuse.
There, the sending party will replace the randomness with a crafted value to embed a hidden message.
While that value seems random to an observer, a recipient who possesses a secret key is able to recover the hidden message.


\lstinputlisting[style=code, language=Python, caption={Python program which generates text using the Huggingface Transformers Pipeline API}, label={alg:gpt2-demo}]{code/pipeline_gpt2.py}

\lstinputlisting[style=code, language=Python, caption={Python program which generates text using the Huggingface Transformers}, label={alg:gpt2-logits-demo}]{code/logits_demo_gpt2.py}


\section{Meteor Stegosystem}
\label{sec:meteor}
In their paper \cite{Meteor2021}, the authors present the Meteor stegosystem, a provably secure steganographic system for realistic distributions as approximated by ML models.
The central idea behind Meteor is that the encoding party Alice manipulates the randomness used in the underlying sampling scheme to embed hidden text in the generated token sequence.
The decoding party Bob receives the generated stegotext from Alice and recovers a prefix of the random value used in sampling for each token.

While this is easily possible for the encoding party because they control the randomness used in sampling, it is intuitively unclear whether the decoding party can extract a prefix of the randomness in the decoding process to recover parts of the hiddentext message.
Luckily, many generative neural networks, such as GPT-2, possess this property.
The Meteor authors have introduced the concept of Ranged Randomness Recoverable Sampling Schemes, or RRRSS, see \autoref{def:rrrss}.
This definition formally models the desired behaviour described above.

\begin{definition}[Ranged Randomness Recoverable Sampling Scheme, \cite{Meteor2021}]
	\label{def:rrrss}
	We call 
	$(Sample_{\mathcal{D}}^\beta, Recover_{\mathcal{D}}^\beta, \mathcal{D}, \beta, \mathcal{H})$ 
	with ppt. algorithms
	$Sample_{\mathcal{D}}^\beta \colon \mathcal{H} \times [0,1]^\beta \rightarrow S$ 
	and 
	$Recover_{\mathcal{D}}^\beta \colon \mathcal{H} \times S \rightarrow \mathcal{P}([0,1]^\beta)$ 
	a Ranged Randomness Recoverable Sampling Scheme (RRRSS) over distribution $\mathcal{D}$ with precision $\beta \in \mathbb{N}$ and histories $\mathcal{H}$, if:
	
	\begin{enumerate}
		\item $Sample_{\mathcal{D}}^\beta(h, r)$ on history $h \in \mathcal{H}$ and randomness $r \in [0,1]^\beta$, sample an output $s$ from its underlying distribution $\mathcal{D}$
		\item $Recover_{\mathcal{D}}^\beta(h, s)$ on history $h \in \mathcal{H}$ and sample $s$ returned from $Sample_{\mathcal{D}}^\beta$, output the set $\mathcal{R} = \{ r \in \{0,1\}^\beta | Sample_{\mathcal{D}}^\beta(\mathcal{H}, r) = s \}$ of possible values for $r$
	\end{enumerate}
\end{definition}

Now, with the definition of an RRRSS, the authors define sampling and recovery algorithms for the GPT-2 model:

\begin{Pseudocode}[caption={
RRRSS Sample Algorithm for GPT, \cite{Meteor2021}.
Sample produces, given a history $h$ and a value $r$, the next token sampled according to $r$ from the distribution for the next token generated by $Next_{\mathcal{GPT}}$.
}]
algorithm $Sample_{\mathcal{GPT}}^\beta(	h, r)$
	Output: Token $t$
	$\mathcal{T}, \mathcal{P} \leftarrow Next_{\mathcal{GPT}}(h)$
	$cuml \leftarrow 0$
	for $i \in \{ 0, 1, \dots, | \mathcal{T} - 1 | \}$ do
		$cuml \leftarrow cuml + \mathcal{P}[i]$
		if $cuml > r$ then
			Output $t \leftarrow \mathcal{T}[i]$
	Output $t \leftarrow \mathcal{T}[|\mathcal{T}|-1]$
\end{Pseudocode}

\begin{Pseudocode}[caption={
RRRSS Recover Algorithm for GPT, \cite{Meteor2021}.
Given a history $h$ and a sample $s$, return a set of possible random values used to generated $s$ according to $h$
}]
algorithm $Recover_{\mathcal{GPT}}^\beta(h, s)$
	Output: Randomness set $\mathcal{R}$
	$\mathcal{T}, \mathcal{P} \leftarrow Next_{\mathcal{GPT}}(h)$
	$cuml \leftarrow 0$
	for $i \in \{ 0, 1, \dots, | \mathcal{T} - 1 | \}$ do
		if $\mathcal{T}[i] = s$ then
			Output $\mathcal{R} \leftarrow \{ r \in \{ 0, 1\}^\beta | cuml \leq r < cuml + \mathcal{P}[i] \}$
		$cuml \leftarrow cuml + \mathcal{P}[i]$
	Output $\mathcal{R} \leftarrow \emptyset$
\end{Pseudocode}

Here, $Next_{\mathcal{GPT}}$ is the algorithm which generates the probability distribution for the next token, as shown in \autoref{fig:generative-network}.
The tokens are denoted by $\mathcal{T}$ and probabilities are denoted by $\mathcal{P}$.
The $Sample_{\mathcal{GPT}}^\beta$ algorithm cumulates the token probabilities $p_i$ into $cuml$ until the threshold $r$ has been reached.
Then the current token $t_i$ is the token to be sampled from the distribution.
We can see that the order of tokens is relevant for the recovery to succeed, so we expect $Next_{\mathcal{GPT}}^\beta$ to return probabilities in a somehow ordered fashion.

Now that a RRRSS scheme for GPT-2 is established, we can commence with the definition of the stegosystem $\mathcal{S} = \left( Encode_{\mathcal{M}}^\beta, Decode_{\mathcal{M}}^\beta \right)$.
The authors described those algorithms as follows while introducing three helper functions $LenPrefix^\beta$, $Prefix^\beta$ and $KeyGen_{\mathcal{M}}^\beta$:


\begin{Pseudocode}[caption={Meteor Encode Algorithm, \cite{Meteor2021}}, label={alg:encode}]
algorithm $Encode_{\mathcal{M}}^\beta(k_{prg}, m, h)$
	Output: Stegotext message $c$
	$c \leftarrow \epsilon,~ n \leftarrow 0$
	while $n < |m|$ do
		$mask \leftarrow PRG.Next(k_{prg})$
		$r \leftarrow m[n:n+\beta] \oplus mask$
		$c_i \leftarrow Sample_{\mathcal{M}}^\beta(h, r)$
		$\mathcal{R} = Recover_{\mathcal{M}}^\beta(h, c_i)$
		$n_i \leftarrow LenPrefix^\beta(\mathcal{R})$
		$c \leftarrow c || c_i, n \leftarrow n+n_i, h \leftarrow h||c_i$
	Output $c$
\end{Pseudocode}
\todo{explain slicing notation}

$Encode$ uses the PRG to sample a pseudorandom mask and applies that mask to the next $\beta$ bits of $m$ into $r$.
Since $mask$ is pseudorandom, this makes $r$ pseudorandom as well.
We then use $r$ to sample from the RRRSS distribution to get the next token $c_i$.
Afterwards, $Recover_{\mathcal{M}}^\beta$ and $LenPrefix^\beta$ are queried to get the number of bits $n_i$ recoverable from $r$.
This procedure is repeated until $|m|$ bits of hiddentext are encoded.
Consider that the output of $Encode$ is deterministic, i.e. for a given $k_{prg}$ and $\mathcal{H}$, the same stegotext $c$ will be generated, which will be important when we discuss security in \autoref{chap:security}.


\begin{Pseudocode}[float, caption={Meteor Decode Algorithm, \cite{Meteor2021}}, label={alg:decode}]
algorithm $Decode_{\mathcal{M}}^\beta(k_{prg}, c, \mathcal{H})$
	Output: Plaintext message $m$
	$m \leftarrow \epsilon$
	Parse $c$ as $c_0 || c_1 || \dots || c_{|c|-1}$
	for $i \in \{0, 1, \dots, |c|-1 \}$ do
		$\mathcal{R} = Recover_{\mathcal{M}}^\beta(\mathcal{H}, c_i)$
		$m_i \leftarrow Prefix^\beta(\mathcal{R})$
		$mask \leftarrow PRG.Next(k_{prg})$
		$m \leftarrow m || (m_i \oplus mask[0: |m_i|]$
		$\mathcal{H} \leftarrow \mathcal{H}||c_i$
	Output $m$
\end{Pseudocode}

$Decode$ decodes the stegotext to the hiddentext message left-to-right. 
It repeatedly uses $Recover_{\mathcal{M}}^\beta$ and $Prefix^\beta$ to recover bits from the pseudorandom value used to sample from the distribution.
It then generates a mask from the PRG (which will be the same mask generated in the encoding step) to decrypt the original message.

\begin{Pseudocode}[caption={Meteor LenPrefix Algorithm, \cite{Meteor2021}}]
algorithm $LenPrefix^\beta(\mathcal R = \{ r_1, \dots r_n \})$
	Output: Length $l$
	$l \leftarrow 1$
	while $l < \beta$ do
		if $\exists i, j \in \{ 1, \dots, n \}$ such that $r_i[0: l] \neq r_j[0:	l]$ then
			Output $l-1$
		$l \leftarrow l+1$
	Output $l$
\end{Pseudocode}%
%
\begin{Pseudocode}[caption={Meteor Prefix Algorithm, \cite{Meteor2021}}]
algorithm $Prefix^\beta(	\mathcal R = \{ r_1, \dots r_n \})$
	Output: Bit String $s$
	Output $r_1[0: LenPrefix^\beta(\mathcal{R})]$
\end{Pseudocode}%
%
\begin{Pseudocode}[caption={Meteor KeyGen Algorithm, \cite{Meteor2021}}]
algorithm $KeyGen_{\mathcal{M}}^\beta(1^\lambda)$
	Output: Key $k_{prg}$
	Output $k_{prg} = PRG.Setup(1^\lambda)$
\end{Pseudocode}

For correctness, the authors argue that a steganographic protocol is correct if encoded messages can be recovered using the $Decode$ algorithm except for negligible in key length.
They introduce another formal definition, see Definition \autoref{def:correctness-kaptchuk}.
In \autoref{chap:correctness}, we will argue how correctness is actually not achieved using GPT-2 as an RRRSS scheme and how we can modify the stegosystem described here to achieve correctness while introducing exponential overhead.

\begin{definition}[Correctness, \cite{Meteor2021}]
\label{def:correctness-kaptchuk}
A steganographic protocol should be correct, i.e. except with negligible probability an encoded message can be recovered using the decode algorithm. Formally, for any $k \leftarrow KeyGen(1^\lambda)$, message $m \in \{0,1\}^*$ and history $h \in \mathcal{H}$,

$$\mathop{Pr}[Decode_{\mathcal{D}}^\beta(k, Encode_{\mathcal{D}}^\beta(k, m, h), h) = m] \geq 1 - \mathop{negl}(\lambda)$$
\end{definition}

Meteor's security proof is also outlined by the authors.
They argue that a steganographic protocol is secure if no ppt. adversary $\mathcal{A}$ is able to distinguish the output of the stegosystem from the output of an oracle $\mathcal{O}$ sampling from the distribution.
More formally, they define steganographic secrecy as in \autoref{def:sec-kaptchuk}.
Furthermore, they argue that the Meteor stegosystem is secure by reduction to the PRG real-or-random game. 
In \autoref{chap:security}, we will discuss how \autoref{def:sec-kaptchuk} is weaker than Hopper's definition of steganographic secrecy against chosen hiddentext attackers introduced in \autoref{sec:prov-sec-steg}.
We will also discuss how an attacker can distinguish Meteor's output from a random oracle $\mathcal{O}_{\mathcal{D}}$ using two queries, polynomial time and message length.

\begin{definition}[Steganographic Secrecy, \cite{Meteor2021}]%
	\label{def:sec-kaptchuk}%
	We say that a steganographic scheme $\Sigma_{\mathcal{D}}$ is secure against chosen hiddentext attacks if for all ppt. adversaries $\mathcal{A}_{\mathcal{D}}$
	
	$$k \leftarrow KeyGen_{\mathcal{D}}(1^\lambda)$$
	$$\left| \mathop{Pr}\left[ \mathcal{A}_{\mathcal{D}}^{Encode_{\mathcal{D}}(k, \cdot, \cdot)}=1 \right] - \mathop{Pr}\left[ \mathcal{A}_{\mathcal{D}}^{\mathcal{O}_{\mathcal{D}}(\cdot, \cdot)}=1 \right] \right| < \mathop{negl}(\lambda)$$
\end{definition}
