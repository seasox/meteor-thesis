\chapter{Background of the Meteor Stegosystem}
\label{chap:previous-work}

\section{Provably Secure Steganography}
\label{sec:prov-sec-steg}

Steganography extends the concepts of cryptography to hide the mere existence of a message.
This makes it robust against censorship and prohibition.
While cryptography can be used to hide the contents of a confidential message, its use is easily discoverable by adversaries.


Steganography is usually modelled using the Prisoners' Problem, as introduced in \cite{Simmons1983}.
The participants Alice and Bob are imprisoned.
While they can exchange messages, their contents are surveilled by a warden who we will call Eve.
Alice and Bob now want to craft and exchange escape plans while Eve observes the messages exchanged between Alice and Bob.
Since Eve does not want Alice and Bob to leave the prison prematurely, she tries to distinguish the exchange of the escape plan from innocuous messages.
If Eve succeeds, she suppresses the message.
For an illustration of the Prisoners' Problem see \autoref{fig:prisonersgame}.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
	\node[block] (alice) {Alice};
	\node[block, right=3cm of alice] (warden) {Eve};
	\node[block, right=3cm of warden] (bob) {Bob};
	\node[left=of alice] (m) {$m$};
	\node[above=of alice] (k) {$k$};
	\node[below=of alice] (h) {history $h$};
	\node[right=of bob] (mb) {$m$};
	\node[above=of bob] (kb) {$k$};
	\node[below=of bob] (hb) {history $h$};
	\node[block, below=of warden] (steganalysis) {Steganalysis: is $s$ stegotext?};
	\node[below left=of steganalysis] (isstego) {suppress message};
	\node[below right=of steganalysis] (notstego) {forward message to recipient};
	\draw[->] (alice) -- node[above] {$s$} (warden);
	\draw[->] (warden) -- node[above] {$s$} (bob);
	\draw[->] (m) -- (alice);
	\draw[->] (k) -- (alice);
	\draw[->] (h) -- (alice);
	\draw[->] (bob) -- (mb);
	\draw[->] (kb) -- (bob);
	\draw[->] (hb) -- (bob);
	\draw[->] (warden) -- (steganalysis);
	\draw[->] (steganalysis) -- node[below] {Yes} (isstego);
	\draw[->] (steganalysis) -- node[below] {No} (notstego);
\end{tikzpicture}
\caption{
An illustration of The Prisoners' Problem, as described in \cite{Simmons1983}.
Two accomplices in a crime (Alice and Bob) are imprisoned in widely separated cells.
They want to escape and try to coordinate their escape plans.
They are surveilled by a warden Eve who monitors their communication.
If Eve detects a hidden message by successfully distinguishing it from an innocuous message, he suppresses the message.
Otherwise, the message is forwarded.
It is expected that Alice and Bob have been able to exchange a common secret key $k$.
They also have access to the same initial history $h$, which can be public.
}
\label{fig:prisonersgame}
\end{figure}

While the Prisoners' Problem has already been established in the 1980s, the cryptographic community was lacking a strict definition of steganographic secrecy until the early 2000s.
In \cite{HLA2002}, a steganographic protocol based on rejection sampling is presented, the first known example of a general provably secure steganographic system.
On the way, the authors also establish a cryptographic point of view on steganographic channels and steganographic protocols as well as the notion of steganographic secrecy against chosen hiddentext attackers.

\begin{definition}[Channel, \cite{Hopper2004}, \cite{Wölfel2011}]
Let $\Sigma = \{0,1\}^\sigma$ be a finite set of bit-strings, or \emph{documents}, of length $\sigma$.
$\Sigma^\ell$ denotes the set of \emph{sequences} of length $\ell$ of bit-strings over $\Sigma$, and $\Sigma^*$ the set of sequences of finite length over $\Sigma$.

A \emph{channel} $\mathcal{C}$ is a function that takes a history $h \in \Sigma^*$ as input and produces a probability distribution $\mathcal{D}_{\mathcal{C},h}$ on $\Sigma$.
We denote the set of histories for $\mathcal{C}$ as $\mathcal{H}$.
\end{definition}

A sequence $s = s_1||s_2||\dots||s_\ell \in \Sigma^\ell$ of documents can be generated by repeatedly generating the distribution $\mathcal{D}_{\mathcal{C},h||s_1||s_2||\dots||s_i}$ by querying $\mathcal{C}$ with some initial history $h$.
For different values of $\ell$, we will now formally introduce the notion of \emph{marginal channel distribution}, which we will later need for the definition of steganographic secrecy.

\begin{definition}[Marginal Channel Distribution, \cite{Hopper2004}]
Let $\mathcal{C}$ be a channel over $\Sigma$ and $h \in \mathcal{H}$ a history.
We let $\mathcal{C}_h = \mathcal{D}_{\mathcal{C},h}$ denote the \emph{marginal channel distribution on a single document from $\Sigma$} conditioned on the history $h$ of already drawn documents.
We let $\mathcal{C}_h^\ell$ denote the \emph{marginal channel distribution on a sequence of $\ell$ documents} from $\Sigma$ conditioned on the history $h$ of already drawn documents.
\end{definition}

We will also need the notion of an \emph{always-informative} channel.
An always-informative channel is a channel with bounded minimum-entropy.

\begin{definition}[Support]
Let $D$ be a distribution.
We denote the \emph{support of $D$} as $\mathop{supp}(D) = \{ x \in D \colon Pr_{D}[x] \neq 0 \}$.
\end{definition}

\begin{definition}[Minimum-Entropy, \cite{Hopper2004}]
Let $D$ be a distributon with finite support $X$.
We define the \emph{minimum entropy of $D$} as

	$$H_{\infty}(D) = \min_{x \in X}\left\{ \log_2 \frac{1}{ Pr_{D}[x] } \right\}$$
\end{definition}

\begin{definition}[Always-Informative Channel, \cite{Hopper2004}]
	Let $L > 0,~ \alpha > 0, \beta > 0$. 
	We call a channel \emph{$\mathcal{C}$ $(L, \alpha, \beta)$-informative} if for all $h \in \Sigma^L$,~ $\mathcal{C}$ satisfies $Pr_{\mathcal{C}}[h] = 0$ or $H_{\infty}(C_{\mathcal{h}}^\beta) \geq \alpha$.
	
	If a channel $\mathcal{C}$ is $(\alpha, \beta)$-informative for all $L > 0$, we call $\mathcal{C}$ \emph{$(\alpha, \beta)$-always informative} or simply \emph{always informative}.
\end{definition}

Now that we have established the notion of channels and always-informative channels, we can commence with the definiton of a stegosystem.

\begin{definition}[Stegosystem, \cite{Hopper2004}]
\label{def:stegosystem}
A \emph{steganographic protocol} $\mathcal{S}$, or \emph{stegosystem}, with security parameter $\lambda$ is a pair of probabilistic algorithms:

\begin{itemize}
	\item $\mathcal{S}$.Encode (abbreviated $SE$) takes as input a key $k \in \{0,1\}^\lambda$, a string $m \in \{0,1\}^*$ (the hiddentext), and a message history $h \in \mathcal{H}$.
		$SE(k, m, h)$ returns a sequence of documents $s_1||s_2||\dots||s_l$ (the stegotext) from the support of $\mathcal{C}_h^l$.
	\item $\mathcal{S}$.Decode (abbreviated $SD$) takes as input a key $k \in \{0,1\}^\lambda$, a sequence of documents $s_1||s_2||\dots||s_l$, and a message history $h \in \mathcal{H}$.
		$SD(k, s, h)$ returns a hiddentext $m \in \{0,1\}^*$
\end{itemize}
\end{definition}

Now that we have defined what a steganographic protocol is, we need to define some properties we would like a stegosystem to have.
First of all, we want a stegosystem to be correct.
The decoding of a given stegotext $s$, given the correct key $k$ and history $h$, should yield the original hiddentext $m$.
Hopper defines the correctness of a stegosystem as follows:

\begin{definition}[Correctness, \cite{Hopper2004}]
\label{def:correctness-hopper}
A stegosystem $\mathcal{S}$ is \emph{correct} if for every polynomial $p(\lambda)$, there exists a negligible function $\mu(\lambda)$ such that $SE$ and $SD$ satisfy the following relationship, where the randomization is over the key $k$ and any coin tosses of $SE$, $SD$, and the oracles accessed by $SE$, $SD$.

$$\forall m \in \{0,1\}^{p(\lambda)},~ h \in \mathcal{H} \colon Pr[SD(k, SE(k, m, h), h) = m] \geq 1 - \mu(\lambda)$$

\end{definition}

The introduction of a \emph{negligible} function $\mu(\lambda)$ allows the stegosystem to sometimes yield wrong results, but only with probability negligible in the security parameter $\lambda$.
While stegosystem with $\mu(\lambda) = 0$ can exist, the introduction of a negligible polynomial proves helpful for reductions on established cryptographic primitives.

Since we naturally want our stegosystem to be prone to analysis by Eve, we also need a definition of security.
Intuitively, we want Eve to be unable to efficiently detect the use of a stegosystem $\mathcal{S}$ without knowledge of the secret key $k$.
To show that Eve cannot detect the use of $\mathcal{S}$, we establish an oracle $\mathcal{O}$ which does not actually use $m$ and $k$ to encode a hidden message, but instead draws a random sequence of documents of length $|SE(k,m,h)|$ from $\mathcal{C}$.
The actual encoding party Alice generates a stegotext $s = SE(k, m, h)$.
If Eve is allowed to choose the hiddentext $m$ and history $h$, we call her a \emph{chosen hiddentext attacker}.

We can model this attack as a distinguishing game between a challenger and a challengee Eve.
First, the challenger samples a key $k \leftarrowS U_\lambda$.
Next, Eve sends a tuple $(m, h)$ to the challenger.
The challenger now secretly flips a coin.
If the coin shows head, the challenger actually uses $\mathcal{S}$ to generate a stegotext $s = SE(k,m,h)$.
Otherwise, the challenger uses $\mathcal{O}$ to generate a covertext $s$.
Either way, the challenger sends the message $s$ back to Eve.
If Eve is able to guess the result of the coin flip, she must have been able to distinguish a stegotext from a covertext.

If Eve can distinguish a stegotext from a covertext, we say that she has an advantage against the stegosystem in the distinguishing game.
The naïve approach to win this game would be to randomly guess the coin flip with a success rate of $\frac{1}{2}$.
To rule out this, the challenger and Eve repeat this game $q(\lambda)$ times with up to $l(\lambda)$ bits of hiddentext.
Also, Eve is limited computationally by time $t(\lambda)$.
The insecurity of a stegosystem is the maximum possible advantage any attacker can have.
Formally, Hopper defined a chosen hiddentext attack as follows:

\begin{definition}[Chosen Hiddentext Attack, \cite{Hopper2004}]
In a chosen hiddentext attack, $W$ is given access to a ``mystery oracle'' $M$ which is chosen from one of the following distributions:

\begin{enumerate}
\item $ST$: The oracle $ST$ has a uniformly chosen key $k \leftarrowS U_\lambda$ and responds to queries $(m, h)$ with a stegotext drawn from $SE(k, m, h)$.
\item $CT$: The oracle $CT$ has a uniformly chosen $k$ as well, and responds to queries $(m, h)$ with a covertext of length $\mathcal{l} = |SE(k, m, h)|$ drawn from $\mathcal{C}_h^{\mathcal{l}}$
\end{enumerate}

$W^M(1^\lambda)$ outputs a bit which represents its guess about the type of $M$.

We define $W$'s advantage against a stegosystem $\mathcal{S}$ for channel $\mathcal{C}$ by

$$Adv_{\mathcal{S}, \mathcal{C}, W}^{ss}(\lambda) = |Pr[W^{ST}(1^\lambda)=1] - Pr[W^{CT}(1^\lambda)=1]|$$

where the probability is taken over the randomness of $ST$, $CT$, and $W$. 

Define the insecurity of $\mathcal{S}$ with respect to channel $\mathcal{C}$ by

$$InSec_{\mathcal{S}, \mathcal{C}}^{ss}(t, q, l, \lambda) = \max_{W \in \mathcal{W}(t, q, l)} \left\{ Adv_{\mathcal{S}, \mathcal{C}, W}^{ss}(\lambda) \right\}$$

where $\mathcal{W}(t, q, l)$ denotes the set of all adversaries which make at most $q(\lambda)$ queries totalling at most $l(\lambda)$ bits (of hiddentext) and running in time at most $t(\lambda)$.
\end{definition}

\begin{definition}[Steganographic Secrecy, \cite{Hopper2004}]
\label{def:sec-hopper}
A Stegosystem $\mathcal{S}$ with security parameter $\lambda$ is called $(t,q,l,\epsilon)$ steganographically secret against chosen hiddentext attack for the channel $\mathcal{C}$ ($(t,q,l,\epsilon)$-SS-CHA-$\mathcal{C}$) if $InSec_{\mathcal{S},\mathcal{C}}^{ss}(t,q,l,\lambda) \leq \epsilon$
\end{definition}

\begin{definition}[Universal Steganographic Secrecy, \cite{Hopper2004}]
A Stegosystem $\mathcal{S}$ is called $(t, q, l, \epsilon)$-universally steganographic secret against chosen hiddentext attack ($(t,q,l,\epsilon)$-SS-CHA if it is $(t,q,l,\epsilon)$-SS-CHA-$\mathcal{C}$ for every always-informative channel $\mathcal{C}$

We call a Stegosystem $\mathcal{S}$ \emph{universally steganographic secret against chosen hiddentext attacks} (USS-CHA) if for every channel $\mathcal{C}$ and for any ppt. adversary $W \in \mathcal{W}(t,q,l,\lambda)$, $Adv_{\mathcal{S},\mathcal{C},W}^{ss}(\lambda)$ is negligible in $\lambda$.
\end{definition}

In the past two decades, many researchers built upon those cryptographic definitions of steganographic protocols.
In \cite{DIRR2008}, complexity limitations of black-box steganography were investigated.
In black-box steganography, the sending party Alice knows nothing about the underlying except a bound to its min-entropy and the ability to sample from it (but does not have to know the generated distribution), while Bob needs no access to the channel at all.
While a black-box construction provides a definition with the least required knowledge of the channel, exponential sampling complexity is required for the encoding party with respect to the number of message bits.

In contrast, white-box steganography describes the other extreme, where Alice either knows the channel distribution beforehand or can efficiently compute it.
Both black-box and white-box steganography are unrealistic in practice, because Alice is possibly able to gain some knowledge or approximation about the channel, but most of the time cannot perfectly describe the channel's distribution.
Therefore, grey-box steganography has been introduced \cite{LRW2013}.
Grey-box steganography tries to resolve these deficiencies by expecting that Alice has some initial knowledge of the covertext channel.
Alice then uses machine learning techniques to learn about the covertext distribution and generate suitable stegotexts.
This allows the use of steganography on efficiently learnable covertext channels and lowers the required complexity to sample stegotext documents.

\section{Generative Neural Networks}
\label{sec:generative-neural-networks}

Generative Neural Networks (GNN) establish a model which approximate realistic distributions such as natural language.
In the last few years, the artificial intelligence community achieved tremendous progress in building powerful models based on deep learning \cite{DeepLearning2015}.

One of the milestones in deep learning based generative models certainly is the development and release of the Generative Pretrained Transformer, or GPT \cite{OpenAI2018}.
Its successors GPT-2 \cite{OpenAI2019} and GPT-3 \cite{OpenAI2020} quickly became two of the industry-standard models in the natural language processing (NLP) community.
While the GPT-n family is mostly known for its capabilities in text generation, it is not specifically designed to generate blog posts or newspaper articles.
It can also be used for a variety of other NLP tasks, such as question answering, sentiment detection or automatic text summarization.
While GPT-3 can only be used via a paid online API, GPT-2 has been open-sourced in 2019 \cite{GPTReleasePlan2019}.
This makes GPT-2 ideal for use in research.

All members of the GPT-n family are attention-based transformer models, which were introduced in \cite{Vaswani2017}.
Before transformer models, state of the art machine learning models were often built with recurrent neural networks (RNNs) such as the long-short-term memory model, or LSTM \cite{Hochreiter1997}.

In comparison to RNNs, transformer models are able to achieve significantly higher training and prediction performance.
While both RNNs and transformer models parse their inputs as a vector of so-called tokens, they differ in the way they use it to generate an output.
In RNN, the input tokens are parsed one word at a time, while transformer models are able to parse all input tokens at the same time.
This is due to the architecture of RNNs.
RNNs consist of a loop structure which parses input tokens together with a hidden state vector and outputs a hidden state vector for the next iteration.

A transformer model, on the other hand, uses attention and refrains from using a hidden state vector like RNNs do.
Instead, at each token of the output, the transformer model weighs its attention on any token of the input, where the sum of attention weights $\omega_i$ over an input of length $n$ must be sum up to $\sum_i \omega_i = 1$.
This technique mimics the way human attention works.
For example, when translating a sentence from one language to another, the translator puts their attention to different parts of the input to determine the correct output word at a given position.

\begin{example}[Example of LSTM and attention in translation]
	When translating the english sentence ``The cat likes to eat bananas very much.'' to german ``Die Katze isst sehr gerne Bananen.'', the structure of the output differs significantly from that of the input.
	In an RNN, the model has to remember the token ``eat'' until the token ``isst'' is generated.
	This information has to be encoded in the hidden state vector while parsing the input and be carried throughout the output generation.
	Since each output in the RNN alters the hidden state, the translation has to performed in a serial manner.
		
	In contrast, a transformer model uses attention at each step of output generation.
	When generating the token ``isst'', the models attention will predominantly be weighed to the input token ``eat'', but some attention will also be placed on ``likes'' and ``the cat'' to produce a grammatically correct sentence.
	Meanwhile other tokens such as ``very'' will have low attention since that word is not relevant to generate ``isst''.
\end{example}



When we use GPT-2 in a stegosystem as we will with Meteor, we can abstract its prediction functionality as a function $\mathcal{GPT} \colon \mathcal{T}^\zeta \rightarrow \mathbb{Z}^{|\mathcal{T}|}$.
It takes an input vector $v \in \mathcal{T}^\zeta$ of tokens (with $\zeta$ the maximum history length supported by the model) and transforms it into a logits vector of unweighted model outputs (the next token prediction) of dimension $|\mathcal{T}|$.
The input vector $v=T(h)$ is a representation of a history $h$ generated using a tokenizer $T$ over tokens $\mathcal{T}$.
The output is logits vector $z \in \mathbb{Z}^{|\mathcal{T}|}$ with components $z_i$ for each token $t_i$ in the model's token set.
Afterwards, we can transform the output $z$ to a probability distribution.

Since we work with text, we need to find a way to transform our history $h$ into the realm of numbers and vectors.
To do that, we use a set of tokens $\mathcal{T} = \{ t_1, t_2, \dots, t_\ell \}$ specific to the model.
In the simplest case, the token set can be the characters or words of the english language.
We can then encode a string $m$ with $|m| \leq \zeta$ into a vector $v = T(h) = \left(t_i\right)^T_{1 \leq i \leq |m|}, ~t_i \in \mathcal{T}$, fill the remaining $\zeta - |m|$ entries in the vector with a special padding token $\bot$ and pass it to $Next_{\mathcal{GPT}}$.
	
While the character based approach could work, it is hard for the model to learn the relationship between words solely on the sequence of characters, which badly influences the quality of outputs.
Are ``do'' and ``don't'' more closely related then ``do'' and ``to''?
Probably yes.
But on the character level, ``do'' and ``to'' are very close to each other as they only differ in one character.
Meanwhile, ``do'' and ``don't'' share a common prefix, but differ in three characters (if we pad ``do'' with filler characters in the end).
As we can see, using character based tokenization creates long token sequences and makes each token for itself meaningless.

With word tokens on the other hand, each word is represented by a unique token, causing large token sets.
It is also unlikely that a token set can be exhaustive for all kinds of texts, especially if we take typing errors into account.
Word based tokenization makes it hard for a machine learning algorithm to learn relationships between words.
Similar words -- such as the examples ``do'' and ``to'' from above -- can have vastly different meanings.

To tackle this challenge, most modern text generation models use subword tokenization.
Subword tokenization uses encoding techniques to find common sequences of characters.
One of the commonly used encoding techniques in NLP (and coincidentally the one used in the GPT-n family) is byte-pair encoding (BPE), which was originally introduced as a data compression algorithm in \cite{BPE1994} and proposed as an encoding algorithm in neural machine translation in \cite{BPENMT2016}.
With subword tokenization, a token may not only be a single character or a full word of a given language, but also a substring of a word.
The use of subword tokens allows a model to better predict relationships between words.
For example, the words ``do'' and ``don't'' are probably closely related.
With subword tokens, a tokenizer can parse ``don't'' as ``do|n't''. 
An NLP model might then learn that the suffix ``n't'' is used to negate certain verbs.
While this approach makes it easier for the model to learn relationships between different words, there may exist multiple tokenizations for some words.

After the tokenizer has encoded the history as a vector of subwords -- with each subword token having a unique numeric identifier -- this vector is passed as input to the generative model.

The model's output is a logits vector.
A logits vector is a vector of unweighted activations of the model's underlying layers, where a higher value represents a stronger activation.
Ultimately, we want to have a probability distribution to sample from.
To transform the logits vector $z \in \mathbb{Z}^\ell$ to a probability distribution $p = (p_i)_{1 \leq i \leq \ell}$ with $\sum_{i=1}^{\ell} p_i = 1$, we can use the softmax function $\sigma$:

$$\sigma \colon \mathbb{Z}^{\ell} \rightarrow [0,1]^{\ell},$$
$$\sigma(z) = \left(\frac{e^{z_i}}{\sum_{j=1}^{\ell} e^{z_j}}\right)_{1 \leq i \leq \ell}$$

When we combine all the previous steps, we get the black box for our ML model as illustrated in \autoref{fig:generative-network}.

\begin{figure}[htpb]
	\centering
	\begin{tikzpicture}
            \node[rectangle, draw=black, minimum width=4cm, minimum height=4cm] (oracle) {$\mathcal{GPT}$};
            \node[right=of oracle, rectangle, draw=black, minimum width=2cm, minimum height=4cm] (sigmoid) {$\sigma$};
            \node[left=of oracle, rectangle, draw=black, minimum width=2cm, minimum height=2cm] (transform) {Tokenizer};
            \node[left=of transform] (history) {$h$};
            \node[above right=-5mm and 1cm of sigmoid] (t1) {$t_1$};
            \node[below=5mm of t1] (t2) {$t_2$};
            \node[below=5mm of t2] (t3) {$t_3$};
            \node[below=5mm of t3] (t4) {$\dots$};
            \node[below=5mm of t4] (tn) {$t_\ell$};
            \node[draw,dashed,inner sep=2mm,label={$Next_{\mathcal{GPT}}$},fit=(sigmoid) (oracle) (transform)] {};
            \draw[->] (transform) -- node[midway,above] {$v$} (oracle);
            \draw[->] (history) -- (transform);
            \draw[->] (oracle) -- node[midway,above] {$z$} (sigmoid);
            \draw[->] (sigmoid) -- node[midway,below] {$p_1$} (t1);
            \draw[->] (sigmoid) -- node[midway,below] {$p_2$} (t2);
            \draw[->] (sigmoid) -- node[midway,below] {$p_3$} (t3);
            \draw[->] (sigmoid) -- node[midway,below] {$p_\ell$} (tn);
	\end{tikzpicture}
	\caption{
Generative ML Model as a pipeline $Next_{\mathcal{GPT}}$ as it is used in the Meteor stegosystem, which takes as input a history $h \in \mathcal{H}$ and outputs a tuple $(\mathcal{T}, \mathcal{P})$, where $\mathcal{P} = \{p_1, p_2, \dots, p_\ell\}$ is a probability distribution over tokens $\mathcal{T} = \{ t_1, t_2, \dots, t_\ell\}$. 
The history $h$ is a string of text which is transformed to a vector $v \in \mathcal{T}^\zeta$ (with model-specific $\zeta$) of token indices using a tokenizer $T$.
The model takes as input said vector $v$ and returns a logits vector $z \in \mathbb{N}^{\ell}$ with unscaled outputs.
This logits vector is then transformed to a probability distribution $\mathcal{P} = \left(p_i\right)_{1 \leq i \leq \ell}$ with $\sum_{i=1}^{\ell} p_i = 1$ using the softmax function.
	The $p_i$ represent the probability of token $t_i \in \mathcal{T}$ to be the next token in the sequence $s$ given the history $v$ of length $\zeta$: $p_i = Pr[s_{\zeta+1}=t_i|s=v]$.
}
	\label{fig:generative-network}
\end{figure}

To generate the next token in the sequence, we can sample according to the generated distribution.
After sampling from the distribution, we append the sampled token to the history and repeat the whole process until sufficiently many tokens have been generated.
We then convert the model outputs to a string to get the generated text.
For an example of GNN model outputs, see \exampleref{example:gpt2-output-sample}.

\begin{example}[Example of GPT-2 model output]
	Hello, I'm a language model, I'm a problem solver in languages."

	At the same time, she said we can understand an idea like "reactive programming," because programming is what you create
	\label{example:gpt2-output-sample}
\end{example}

To use GPT-2 with the Python programming language, we can use the Hugging Face Transformers library together with PyTorch.
PyTorch is a machine learning framework which implements high performance operations needed for machine learning with hardware acceleration.
The Hugging Face Transformers library is built upon PyTorch and gives an API for easy access to several pretrained machine learning models.
In its simplest form, the pipeline API generates texts using not only GPT-2 but many other generative models.
A Python program which generates the outputs in \exampleref{example:gpt2-output-sample} can be found in \autoref{alg:gpt2-demo}.

While the use of the pipeline API is very convenient to develop simple text generation or question answering programs, it abstracts some steps that we will need to intercept to use in a stegosystem.
A more sophisticated way to generate text using GPT-2 can be found in \autoref{alg:gpt2-logits-demo}.
Its output resembles that shown in \exampleref{example:gpt2-output-sample}. 
This code example shows how we can:

\begin{itemize}
	\item use a tokenizer to convert strings to a vector of tokens
	\item use the model to get a logits vector
	\item transform the logits vector into a probability distribution using the softmax function
	\item sample the next token from the distribution
	\item convert a vector of tokens back to a string using the tokenizer
\end{itemize}

As we can see in \autoref{alg:gpt2-logits-demo}, a pseudorandom value \emph{sample} is generated to select the next token \emph{selection} from the distribution.
This is what the Meteor stegosystem will abuse.
There, the sending party will replace the randomness with a crafted value to embed a hidden message.
While that value seems random to an observer, a recipient who possesses a secret key is able to recover the hidden message.


\lstinputlisting[style=code, language=Python, caption={Python program which generates text using the Huggingface Transformers Pipeline API}, label={alg:gpt2-demo}]{code/pipeline_gpt2.py}

\lstinputlisting[style=code, language=Python, caption={Python program which generates text using the Huggingface Transformers}, label={alg:gpt2-logits-demo}]{code/logits_demo_gpt2.py}

\newpage

\section{Meteor Stegosystem}
\label{sec:meteor}
In their paper \cite{Meteor2021}, the authors present the Meteor stegosystem, a provably secure steganographic system for realistic distributions as approximated by ML models.
The central idea behind Meteor is that an encoding party Alice manipulates the randomness used in the underlying sampling scheme to embed hidden text in the generated token sequence.
The decoding party Bob receives the generated stegotext from Alice and recovers a prefix of the random value used in sampling for each token.

While this is easily possible for the encoding party because they control the randomness used in sampling, it is intuitively unclear whether the decoding party can extract a prefix of the randomness in the decoding process to recover parts of the hiddentext message.
Luckily, many generative neural networks, such as GPT-2, possess this property.
The Meteor authors have introduced the concept of Ranged Randomness Recoverable Sampling Schemes, or RRRSS.
This definition formally models the desired behaviour described above.

\begin{definition}[Ranged Randomness Recoverable Sampling Scheme, \cite{Meteor2021}]
	\label{def:rrrss}
	We call 
	$(Sample_{\mathcal{D}}^\beta, Recover_{\mathcal{D}}^\beta, \mathcal{D}, \beta, \mathcal{H})$ 
	with ppt. algorithms
	$Sample_{\mathcal{D}}^\beta \colon \mathcal{H} \times \{0,1\}^\beta \rightarrow \Sigma$ 
	and 
	$Recover_{\mathcal{D}}^\beta \colon \mathcal{H} \times \Sigma \rightarrow \mathcal{P}(\{0,1\}^\beta)$ 
	a Ranged Randomness Recoverable Sampling Scheme (RRRSS) over distribution $\mathcal{D}$ with precision $\beta \in \mathbb{N}$ and histories $\mathcal{H}$, if:
	
	\begin{enumerate}
		\item $Sample_{\mathcal{D}}^\beta(h, r)$ on history $h \in \mathcal{H}$ and randomness $r \in \{0,1\}^\beta$, sample an output $s$ from the underlying distribution $\mathcal{D}$
		\item $Recover_{\mathcal{D}}^\beta(h, s)$ on history $h \in \mathcal{H}$ and sample $s \in \Sigma$ returned from $Sample_{\mathcal{D}}^\beta$, output the set $\mathcal{R} = \{ r \in \{0,1\}^\beta | Sample_{\mathcal{D}}^\beta(\mathcal{H}, r) = s \}$ of possible values for $r$
	\end{enumerate}
\end{definition}

With the definition of an RRRSS, the authors define sampling and recovery algorithms for the GPT-2 model.
While they define the $Sample$ and $Recover$ algorithms for GPT-2 only, they argue that many generative models satisfy the RRRSS property.

\begin{Pseudocode}[caption={
RRRSS Sample Algorithm for GPT, \cite{Meteor2021}.
$Sample$ produces, given a history $h$ and a value $r$, the next token sampled according to $r$ from the distribution for the next token generated by $Next_{\mathcal{GPT}}(h)$.
}]
algorithm $Sample_{\mathcal{GPT}}^\beta(	h, r)$
	Output: Token $t$
	$\mathcal{T}, \mathcal{P} \leftarrow Next_{\mathcal{GPT}}(h)$
	$cuml \leftarrow 0$
	for $i \in \{ 0, 1, \dots, | \mathcal{T} - 1 | \}$ do
		$cuml \leftarrow cuml + \mathcal{P}[i]$
		if $cuml > r$ then
			Output $t \leftarrow \mathcal{T}[i]$
	Output $t \leftarrow \mathcal{T}[|\mathcal{T}|-1]$
\end{Pseudocode}

\begin{Pseudocode}[caption={
RRRSS Recover Algorithm for GPT, \cite{Meteor2021}.
Given a history $h$ and a sample $s$, return a set of possible random values used to generated $s$ according to $h$
}]
algorithm $Recover_{\mathcal{GPT}}^\beta(h, s)$
	Output: Randomness set $\mathcal{R}$
	$\mathcal{T}, \mathcal{P} \leftarrow Next_{\mathcal{GPT}}(h)$
	$cuml \leftarrow 0$
	for $i \in \{ 0, 1, \dots, | \mathcal{T} - 1 | \}$ do
		if $\mathcal{T}[i] = s$ then
			Output $\mathcal{R} \leftarrow \{ r \in \{ 0, 1\}^\beta | cuml \leq r < cuml + \mathcal{P}[i] \}$
		$cuml \leftarrow cuml + \mathcal{P}[i]$
	Output $\mathcal{R} \leftarrow \emptyset$
\end{Pseudocode}

Here, $Next_{\mathcal{GPT}}$ is the algorithm which generates the probability distribution for the next token, as shown in \autoref{fig:generative-network}.
The tokens are denoted by $\mathcal{T}$ and probabilities are denoted by $\mathcal{P}$.
The $Sample_{\mathcal{GPT}}^\beta$ algorithm cumulates the token probabilities $p_i$ into $cuml$ until the threshold $r$ has been reached.
Then the current token $t_i$ is the token to be sampled from the distribution.
We can see that the order of tokens is relevant for the recovery to succeed, so we expect $Next_{\mathcal{GPT}}$ to return probabilities in a somehow ordered fashion.

Now that a RRRSS scheme for GPT-2 is established, we can commence with the definition of the stegosystem $\mathcal{S} = \left( Encode_{\mathcal{M}}^\beta, Decode_{\mathcal{M}}^\beta \right)$.
The authors described those algorithms as follows while introducing three helper functions $LenPrefix^\beta$, $Prefix^\beta$ and $KeyGen_{\mathcal{M}}^\beta$:


\begin{Pseudocode}[caption={Meteor Encode Algorithm \cite{Meteor2021}}, label={alg:encode}]
algorithm $Encode_{\mathcal{M}}^\beta(k_{prg}, m, h)$
	Output: Stegotext message $c$
	$c \leftarrow \epsilon,~ n \leftarrow 0$
	while $n < |m|$ do
		$mask \leftarrow PRG.Next(k_{prg})$
		$r \leftarrow m[n:n+\beta] \oplus mask$
		$c_i \leftarrow Sample_{\mathcal{M}}^\beta(h, r)$
		$\mathcal{R} = Recover_{\mathcal{M}}^\beta(h, c_i)$
		$n_i \leftarrow LenPrefix^\beta(\mathcal{R})$
		$c \leftarrow c || c_i, n \leftarrow n+n_i, h \leftarrow h||c_i$
	Output $c$
\end{Pseudocode}

$Encode$ uses the PRG to sample a pseudorandom mask and applies that mask to the next $\beta$ bits of $m$ into $r$.
Since $mask$ is pseudorandom, this makes $r$ pseudorandom as well.
We then use $r$ to sample from the RRRSS distribution to get the next token $c_i$.
Afterwards, $Recover_{\mathcal{M}}^\beta$ and $LenPrefix^\beta$ are queried to get the number of bits $n_i$ recoverable from $r$.
This procedure is repeated until $|m|$ bits of hiddentext have been encoded.
Consider that the output of $Encode$ is deterministic, i.e. for a given $m$, $k_{prg}$ and $h$, the same stegotext $c$ will be generated, which will be important when we discuss security in \autoref{chap:security}.


\begin{Pseudocode}[float, caption={Meteor Decode Algorithm \cite{Meteor2021}}, label={alg:decode}]
algorithm $Decode_{\mathcal{M}}^\beta(k_{prg}, c, h)$
	Output: Plaintext message $m$
	$m \leftarrow \epsilon$
	Parse $c$ as $c_0 || c_1 || \dots || c_{\tau}$
	for $i \in \{ 0, 1, \dots, \tau \}$ do
		$\mathcal{R} = Recover_{\mathcal{M}}^\beta(h, c_i)$
		$m_i \leftarrow Prefix^\beta(\mathcal{R})$
		$mask \leftarrow PRG.Next(k_{prg})$
		$m \leftarrow m || (m_i \oplus mask[0: |m_i|]$
		$h \leftarrow h||c_i$
	Output $m$
\end{Pseudocode}

$Decode$ takes a stegotext $c$ and decodes it to the hiddentext message.
It repeatedly uses $Recover_{\mathcal{M}}^\beta$ and $Prefix^\beta$ to recover bits from the pseudorandom value used to sample from the distribution.
It then generates a mask from the PRG (which will be the same mask generated in the encoding step) to decrypt the original message.

\begin{Pseudocode}[caption={Meteor LenPrefix Algorithm, \cite{Meteor2021}}]
algorithm $LenPrefix^\beta(\mathcal R = \{ r_1, \dots r_n \})$
	Output: Length $l$
	$l \leftarrow 1$
	while $l < \beta$ do
		if $\exists i, j \in \{ 1, \dots, n \}$ such that $r_i[0: l] \neq r_j[0:	l]$ then
			Output $l-1$
		$l \leftarrow l+1$
	Output $l$
\end{Pseudocode}%
%
\begin{Pseudocode}[caption={Meteor Prefix Algorithm, \cite{Meteor2021}}]
algorithm $Prefix^\beta(	\mathcal R = \{ r_1, \dots r_n \})$
	Output: Bit String $p$
	Output $p \leftarrow r_1[0: LenPrefix^\beta(\mathcal{R})]$
\end{Pseudocode}%
%
\begin{Pseudocode}[caption={Meteor KeyGen Algorithm, \cite{Meteor2021}}]
algorithm $KeyGen_{\mathcal{M}}^\beta(1^\lambda)$
	Output: Key $k_{prg}$
	Output $k_{prg} \leftarrow PRG.Setup(1^\lambda)$
\end{Pseudocode}

For correctness, the authors argue that a steganographic protocol is correct if encoded messages can be recovered using the $Decode$ algorithm except for a probability negligible in key length, see \autoref{def:correctness-kaptchuk}.
In \autoref{chap:correctness}, we will argue how correctness is in fact not achieved using GPT-2 as an RRRSS scheme and how we can modify the stegosystem described here to achieve correctness while introducing computational overhead exponential in stegotext length.

\begin{definition}[Correctness, \cite{Meteor2021}]
\label{def:correctness-kaptchuk}
We call a steganographic protocol \emph{correct}, if, except with negligible probability, an encoded message can be recovered using the decode algorithm.
Formally, for any $k \leftarrow KeyGen(1^\lambda)$, message $m \in \{0,1\}^*$ and history $h \in \mathcal{H}$,

$$\mathop{Pr}[Decode_{\mathcal{D}}^\beta(k, Encode_{\mathcal{D}}^\beta(k, m, h), h) = m] \geq 1 - \mathop{negl}(\lambda)$$
\end{definition}

Meteor's security proof is also outlined by the authors.
They argue that a steganographic protocol is secure if no ppt. adversary $\mathcal{A}$ is able to distinguish the output of the stegosystem from the output of an oracle $\mathcal{O}$ sampling from the distribution.
More formally, they define steganographic secrecy in \autoref{def:sec-kaptchuk}.
Furthermore, they argue that the Meteor stegosystem is secure by reduction to the PRG real-or-random game. 
In \autoref{chap:security}, we will discuss how \autoref{def:sec-kaptchuk} is weaker than Hopper's definition of steganographic secrecy against chosen hiddentext attackers introduced in \autoref{sec:prov-sec-steg}.
We will also discuss how an attacker can distinguish Meteor's output from a random oracle $\mathcal{O}_{\mathcal{D}}$ using two queries, polynomial time and message length.

\begin{definition}[Steganographic Secrecy, \cite{Meteor2021}]%
	\label{def:sec-kaptchuk}%
	We say that a steganographic scheme $\mathcal{S}_{\mathcal{D}}$ is secure against chosen hiddentext attacks if for all ppt. adversaries $\mathcal{A}_{\mathcal{D}}$
	
	$$k \leftarrow KeyGen_{\mathcal{D}}(1^\lambda)$$
	$$\left| \mathop{Pr}\left[ \mathcal{A}_{\mathcal{D}}^{Encode_{\mathcal{D}}(k, \cdot, \cdot)}=1 \right] - \mathop{Pr}\left[ \mathcal{A}_{\mathcal{D}}^{\mathcal{O}_{\mathcal{D}}(\cdot, \cdot)}=1 \right] \right| < \mathop{negl}(\lambda)$$
\end{definition}
