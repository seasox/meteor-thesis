\chapter{Backgrounds of the Meteor Stegosystem}
\label{chap:previous-work}

\section{Provably Secure Steganography}
\label{sec:prov-sec-steg}

Steganography extends the concepts of cryptography to hide the mere existence of a message.
This makes it robust against censorship and prohibition.
While cryptography can be used to hide the contents of a confidential message, its use is easily discoverable by adversaries.


Steganography is usually modeled using the Prisoners' Problem, as introduced in \cite{Simmons1983}.
The participants Alice and Bob are imprisoned.
While they can exchange messages, their contents are surveilled by a warden we will call Eve.
Alice and Bob now want to craft and exchange escape plans while Eve observes the messages exchanged between Alice and Bob.
Since Eve does not want Alice and Bob to leave the prison prematurely, she tries to distinguish the exchange of the escape plan from innocuous messages.
If Eve succeeds, she suppresses the message.
For an illustration of the Prisoners' Problem see \autoref{fig:prisonersgame}.

We suppose that Alice and Bob communicate using a sequence of \emph{documents}, called the \emph{stegotext}.
In general, the documents $\Delta$ are defined as a set of efficiently recognizable prefix-free bit strings of finite length \cite{Hopper2008}.
However, in this thesis and the Meteor paper, the set of documents is denoted by a set $\mathcal{T}$ (for \emph{tokens}), which are sequences of unicode characters, i.e. $\Delta = \mathcal{T} \subseteq \mathcal{U}^*$. For example, given
$$\mathcal{T} = \{ \textrm{``hello''}, \textrm{``\Vtextvisiblespace world''}, \textrm{``\Vtextvisiblespace stega''}, \textrm{``nography''} \}$$ the documents (or tokens) are ``hello'', ``\Vtextvisiblespace world'', ``\Vtextvisiblespace stega'' and ``nography''.  
The stegotext ``hello steganography'' consists of three tokens.
In \autoref{chap:reliability}, we will see that the tokens used in generative models used with Meteor are not prefix-free, i.e. there exist tokens $t \in \mathcal{T}$ such that there exists a prefix of $t$ which is token as well, which causes issues in reliability we will introduce as \emph{ambiguous tokenization} in \autoref{sec:amb-tok}.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
  \node[block] (alice) {Alice};
  \node[block, right=3cm of alice] (warden) {Eve};
  \node[block, right=3cm of warden] (bob) {Bob};
  \node[left=of alice] (m) {$m$};
  \node[above=of alice] (k) {$k$};
  \node[below=of alice] (h) {history $h$};
  \node[right=of bob] (mb) {$m$};
  \node[above=of bob] (kb) {$k$};
  \node[below=of bob] (hb) {history $h$};
  \node[block, below=of warden] (steganalysis) {Steganalysis: is $s$ stegotext?};
  \node[below left=of steganalysis] (isstego) {suppress message};
  \node[below right=of steganalysis] (notstego) {forward message to recipient};
  \draw[->] (alice) -- node[above] {$s$} (warden);
  \draw[->] (warden) -- node[above] {$s$} (bob);
  \draw[->] (m) -- (alice);
  \draw[->] (k) -- (alice);
  \draw[->] (h) -- (alice);
  \draw[->] (bob) -- (mb);
  \draw[->] (kb) -- (bob);
  \draw[->] (hb) -- (bob);
  \draw[->] (warden) -- (steganalysis);
  \draw[->] (steganalysis) -- node[below] {Yes} (isstego);
  \draw[->] (steganalysis) -- node[below] {No} (notstego);
\end{tikzpicture}
\caption{
An illustration of the Prisoners' Problem \cite{Simmons1983}.
Two accomplices in a crime (Alice and Bob) are imprisoned in widely separated cells.
They want to escape and try to coordinate their escape plans.
They are surveilled by a warden Eve who monitors their communication.
If Eve detects a hidden message by successfully distinguishing it from an innocuous message, she suppresses the message.
Otherwise, the message is forwarded.
It is expected that Alice and Bob have been able to exchange a secret key $k$.
They also have access to the same initial history $h$, which can be public.
}
\label{fig:prisonersgame}
\end{figure}

While the Prisoners' Problem has already been established in the 1980s, the cryptographic community lacked a strict definition of steganographic security until the early 2000s.
In 2002, Hopper et al. introduced a steganographic protocol based on rejection sampling, the first known example of a general provably secure steganographic system \cite{HLA2002}.
On the way, the authors also established a cryptographic point of view on steganographic channels and steganographic protocols as well as a notion of steganographic security against chosen hiddentext attackers.
The definitions and theorems established by Hopper, Langford and von Ahn have succeedingly been advanced in Hopper's dissertation \cite{Hopper2004} and further works by other researchers.


\begin{definition}[Channel]
A \emph{channel} $\mathcal{C}$ is a function that takes a history $h \in \Delta^*$ as input and produces a probability distribution $\mathcal{D}_{\mathcal{C},h}$ on $\Delta$.
We denote the set of histories for $\mathcal{C}$ as $\mathcal{H}$.
\end{definition}

A sequence $s = s_1||s_2||\dots||s_\ell \in \Delta^\ell$ of documents can be generated by repeatedly generating the distribution $\mathcal{D}_{\mathcal{C},h||s_1||s_2||\dots||s_i}$ by querying $\mathcal{C}$ with some initial history $h$ and appending the value $s_i$ sampled from the distribution to $h$.
We will now formally introduce the notion of \emph{marginal channel distribution}, which we will later need for the definition of steganographic security.

\begin{definition}[Marginal Channel Distribution, \cite{Hopper2004}]
Let $\mathcal{C}$ be a channel over $\Delta$ and $h \in \mathcal{H}$ a history.
We let $\mathcal{C}_h = \mathcal{D}_{\mathcal{C},h}$ denote the \emph{marginal channel distribution on a single document from $\Delta$} conditioned on the history $h$ of already drawn documents.
We let $\mathcal{C}_h^\ell$ denote the \emph{marginal channel distribution on a sequence of $\ell$ documents} from $\Delta$ conditioned on the history $h$ of already drawn documents.
\end{definition}
%We will also need the notion of an \emph{always informative channel}.
%An always informative channel is a channel with minimum-entropy bounded by some value $\alpha$.
We will also need the notion of the \emph{support of a distribution}.
The support of a distribution consists of all elements with non-zero probability.

\begin{definition}[Support]
Let $D$ be a distribution.
We denote the \emph{support of $D$} as 
$$\mathop{supp}(D) = \{ x \in D \mid Pr_{D}[x] \neq 0 \}.$$
\end{definition}

Now that we have established the notion of channels, we can commence with the definition of a stegosystem.

\begin{definition}[Stegosystem, \cite{Hopper2004}]
\label{def:stegosystem}
A \emph{steganographic protocol} $\mathcal{S}$, or \emph{stegosystem}, with security parameter $\lambda$ is a pair of probabilistic algorithms:

\begin{itemize}
  \item $\mathcal{S}.KeyGen$ takes as input a security parameter $\lambda$ and returns a key $k \in \{0,1\}^\lambda$.
  \item $\mathcal{S}.Encode$ (abbreviated $SE$) takes as input a key $k \in \{0,1\}^\lambda$, a string $m \in \mathcal{U}^*$ (the hiddentext), and a message history $h \in \mathcal{H}$.
    $SE(k, m, h)$ returns a sequence of documents $s_1||s_2||\dots||s_l$ (the stegotext) from the support of $\mathcal{C}_h^l$.
  \item $\mathcal{S}.Decode$ (abbreviated $SD$) takes as input a key $k \in \{0,1\}^\lambda$, a sequence of documents $s_1||s_2||\dots||s_l$, and a message history $h \in \mathcal{H}$.
    $SD(k, s, h)$ returns a hiddentext $m \in \mathcal{U}^*$
\end{itemize}
\end{definition}

This definition resembles that of a symmetric encryption scheme, except that both $SE$ and $SD$ take a message history $h$ as additional argument.

Now that we have defined what a steganographic protocol is, we will introduce some properties we would like a stegosystem to satisfy.
First of all, we want a stegosystem to be reliable.
The decoding of a given stegotext $s$, given the correct key $k$ and history $h$, should yield the original hiddentext $m$.
To measure this, we define the unreliability of a stegosystem $\mathcal{S}$:

\begin{definition}[Unreliability, \cite{Berndt2017}]
\label{def:unreliability}
The \emph{unreliability} $UnRel_{\mathcal{S}}(\lambda)$ is the maximum probability that the decoder $SD$ fails, i.e.	

$$UnRel_{\mathcal{S}}(\lambda) = \max_{\substack{k \in \{0,1\}^\lambda\\m \in \mathcal{U}^*\\ h \in \mathcal{H}}}\left\{ Pr[SD(k, SE(k,m,h), h) \neq m] \right\}.$$
\end{definition}

\begin{definition}[Reliability]
\label{def:reliability}
A stegosystem $\mathcal{S}$ is \emph{reliable} if its unreliability is negligible in $\lambda$.
\end{definition}

The introduction of \emph{negligible unreliability} allows the stegosystem to sometimes yield wrong results, but only with probability negligible in the security parameter $\lambda$.
This proves helpful for reductions on established cryptographic primitives.
In some works, the term \emph{correctness} of a stegosystem is used \cite{Hopper2004, Meteor2021}.
In this thesis however, we will call a stegosystem reliable instead.

Since we naturally want our stegosystem to be prone to analysis by Eve, we also need a definition of security.
Intuitively, we want Eve to be unable to efficiently detect the use of a stegosystem $\mathcal{S}$ without knowledge of the secret key $k$.
To show that Eve cannot detect the use of $\mathcal{S}$, we establish an oracle $\mathcal{O}$ which does not actually use $SE$ to encode a hidden message $m$, but instead draws a random sequence of documents of length $|SE(k,m,h)|$ from $\mathcal{C}$.
If Eve is allowed to choose the hiddentext $m$ and history $h$, we call her a \emph{chosen hiddentext attacker}.

We can model this attack as a distinguishing game between a challenger and an adversary Eve.
First, the challenger samples a key $k \leftarrowS U_\lambda$.
Next, Eve sends a tuple $(m, h)$ to the challenger.
The challenger now secretly flips a coin.
If the coin shows heads, the challenger uses $SE$ to generate a stegotext $s = SE(k,m,h)$.
Otherwise, the challenger uses $\mathcal{O}$ to generate a covertext $s$.
Either way, the challenger sends $s$ back to Eve.
If Eve is able to guess the result of the coin flip using a distinguisher $W$, she must have been able to distinguish a stegotext from a covertext.

If Eve can distinguish a stegotext from a covertext, we say that she has an advantage against the stegosystem in the distinguishing game.
The naÃ¯ve approach to win this game would be to simply guess the coin flip with a success rate of $\frac{1}{2}$ if they play this game just once.
The challenger and Eve repeat this game $q(\lambda)$ times with up to $l(\lambda)$ bits of hiddentext chosen by Eve.
Additionally, Eve is limited computationally by time $t(\lambda)$.
The insecurity of a stegosystem is the maximum possible advantage any attacker can have.
Formally, Hopper defined a chosen hiddentext attack as follows:

\begin{definition}[Chosen Hiddentext Attack, \cite{Hopper2004}]
In a chosen hiddentext attack, Eve is given access to a ``mystery oracle'' $M$ which is chosen from one of the following distributions:
\begin{enumerate}
\item $ST$: The oracle $ST$ has a uniformly chosen key $k \leftarrowS U_\lambda$ and responds to queries $(m, h)$ with a stegotext drawn from $SE(k, m, h)$.
\item $CT$: The oracle $CT$ has a uniformly chosen $k$ as well, and responds to queries $(m, h)$ with a covertext of length $\mathcal{l} = |SE(k, m, h)|$ drawn from $\mathcal{C}_h^{\mathcal{l}}$.
\end{enumerate}
Eve now has to determine which oracle $M$ she has been given access to.
$W^M(1^\lambda)$ outputs a bit which represents her guess about the type of $M$.
We define Eve's \emph{advantage against a stegosystem $\mathcal{S}$ for channel $\mathcal{C}$} by
$$Adv_{\mathcal{S}, \mathcal{C}, W}^{ss}(\lambda) = |Pr[W^{ST}(1^\lambda)=1] - Pr[W^{CT}(1^\lambda)=1]|$$
where the probability is taken over the randomness of $ST$, $CT$, and $W$.
Define the \emph{insecurity of $\mathcal{S}$ with respect to channel $\mathcal{C}$} by
$$InSec_{\mathcal{S}, \mathcal{C}}^{ss}(t, q, l, \lambda) = \max_{W \in \mathcal{W}(t, q, l)} \left\{ Adv_{\mathcal{S}, \mathcal{C}, W}^{ss}(\lambda) \right\}$$
where $\mathcal{W}(t, q, l)$ denotes the set of all adversaries which make at most $q(\lambda)$ queries totaling at most $l(\lambda)$ bits (of hiddentext) and running in time at most $t(\lambda)$.
\end{definition}

\begin{definition}[Steganographic Security, \cite{Hopper2004}]
\label{def:sec-hopper}
A stegosystem $\mathcal{S}$ with security parameter $\lambda$ is called \emph{$(t,q,l,\epsilon)$ steganographically secret against chosen hiddentext attack for the channel $\mathcal{C}$} ($(t,q,l,\epsilon)$-SS-CHA-$\mathcal{C}$) if $InSec_{\mathcal{S},\mathcal{C}}^{ss}(t,q,l,\lambda) \leq \epsilon$.
\end{definition}

In the past two decades, many researchers built upon those cryptographic definitions of steganographic protocols.
In \cite{DIRR2008}, complexity limitations of black-box steganography were investigated.
In black-box steganography the sending party Alice knows nothing about the underlying distribution except a bound to its min-entropy and the ability to sample from it (but does not have to know the generated distribution) while Bob needs no access to the channel at all.
While a black-box construction provides a definition with the least required knowledge of the channel, exponential sampling complexity is required for the encoding party with respect to the number of message bits.

In contrast, white-box steganography describes the other extreme, where Alice either knows the channel distribution beforehand or can efficiently compute it.
Both black-box and white-box steganography are unrealistic in practice, because Alice is possibly able to gain some knowledge or approximation about the channel, but most of the time cannot perfectly describe the channel's distribution.
Therefore, grey-box steganography has been introduced \cite{LRW2013}.
Grey-box steganography tries to resolve these deficiencies by expecting that Alice has some initial knowledge of the covertext channel.
Alice then uses machine learning techniques to learn about the covertext distribution and generates suitable stegotexts.
This allows the use of steganography on efficiently learnable covertext channels and lowers the required complexity to sample stegotext documents.

\section{Generative Neural Networks}
\label{sec:generative-neural-networks}

Generative Neural Networks (GNN) establish a model which approximate realistic distributions such as natural language.
In the last years the artificial intelligence community achieved tremendous progress in building powerful models based on deep learning, which was proposed in \cite{DeepLearning2015}.

One of the milestones in deep learning based generative models certainly is the development and release of the Generative Pretrained Transformer, or GPT \cite{OpenAI2018}.
Its successors GPT-2 \cite{OpenAI2019} and GPT-3 \cite{OpenAI2020} quickly became two of the industry-standard models in the natural language processing (NLP) community.
While the GPT-n family is mostly known for its capabilities in text generation, it is not specifically designed to generate blog posts or newspaper articles.
It can also be used for a variety of other NLP tasks, such as question answering, sentiment detection or automatic text summarization.
While GPT-3 can only be used via a paid online API, GPT-2 has been open-sourced in 2019 \cite{GPTReleasePlan2019}.
This makes GPT-2 ideal for use in research.

All members of the GPT-n family are attention-based transformer models, which were introduced in \cite{Vaswani2017}.
Before transformers, state of the art machine learning models were often built using recurrent neural networks (RNNs) such as the long-short-term memory model, or LSTM \cite{Hochreiter1997}.

In comparison to RNNs, transformer models are able to achieve significantly higher training and prediction performance.
While both RNNs and transformer models parse their inputs as a vector of so-called tokens, they differ in the way they process them to generate an output.
In RNN, the input tokens are parsed one word at a time, while transformer models are able to parse all input tokens at the same time.
This is due to the architecture of RNNs.
RNNs consist of a loop structure that parses one input token after the other together with a hidden state vector and outputs a hidden state vector for the next iteration.

A transformer model, on the other hand, uses attention and refrains from using a hidden state vector like RNNs do.
Instead, for each output token, the transformer model weighs its attentions on the input, where the sum of attention weights $\omega_i$ over an input of length $n$ must add up to $\sum_{1\leq i \leq n} \omega_i = 1$.
This technique mimics the way human attention works.
For example, when translating a sentence from one language to another, the translator puts their attention to different parts of the input to determine the correct output word at a given position.

\begin{example}[Example of LSTM and attention in translation]
  When translating the english sentence ``The cat likes to eat bananas very much.'' to german ``Die Katze isst sehr gerne Bananen.'', the structure of the output differs significantly from that of the input.
  In an RNN, the model has to remember the token ``eat'' until the token ``isst'' is generated.
  This information has to be encoded in the hidden state vector while parsing the input and be carried throughout the output generation.
  Since each output in the RNN alters the hidden state, the translation has to performed in a serial manner.
    
  In contrast, a transformer model uses attention at each step of output generation.
  When generating the token ``isst'', the models attention will predominantly be weighed to the input token ``eat'', but some attention will also be placed on ``likes'' and ``the cat'' to produce a grammatically correct sentence.
  Meanwhile other tokens such as ``very'' will have low attention since that word is not relevant to generate ``isst''.
\end{example}



When we use GPT-2 in the Meteor stegosystem, we can abstract its prediction functionality as a function $\mathcal{GPT} \colon \mathcal{T}^\zeta \rightarrow \mathbb{Z}^{|\mathcal{T}|}$.
It takes an input vector $v \in \mathcal{T}^\zeta$ of tokens (with $\zeta$ the history length supported by the model) and transforms it into a \emph{logits vector} of unweighted model outputs (the next token prediction) of dimension $|\mathcal{T}|$.

The input vector $v=T(h)$ is a representation of a history $h$ generated using a tokenizer $T$ over tokens $\mathcal{T}$.
Since we work with text, we need to find a way for $T$ to transform our history $h$ into the realm of numbers, i.e. tokenize the history.
To do that, we use a set of tokens $\mathcal{T} = \{ t_1, t_2, \dots, t_\ell \}$ specific to the model.
In the simplest case, the tokens can be ASCII characters, i.e. $\mathcal{T} = ASCII$.
We can then encode a string $h$ with $|h| \leq \zeta$ as a vector $v = T(h) \in \mathcal{T}^{|h|}$, fill the remaining $\zeta - |h|$ entries in the vector with a special padding token $\epsilon$ and pass it to $\mathcal{GPT}$.
  
While the character based approach could work, it is hard for the model to learn the relationship between words solely on the sequence of characters, which badly influences the quality of outputs.
Are ``do'' and ``don't'' more closely related then ``do'' and ``to''?
Probably yes.
But on the character level, ``do'' and ``to'' are very close to each other as they only differ in one character.
Meanwhile, ``do'' and ``don't'' share a common prefix, but differ in three characters (if we pad ``do'' with padding token $\epsilon$).
As we can see, the disadvantages of character based tokenization are long token sequences and meaninglessness of each token for itself.

We could also use words of the english language as tokens, where each word is represented by a unique token.
But this results in large token sets.
It is also unlikely that any token set of full words can be exhaustive for all kinds of texts, especially if we take typographical errors or neologisms into account.
Also, word based tokenization makes it hard for a machine learning algorithm to learn relationships between words.
Similar words -- such as the examples ``do'' and ``to'' from above -- can have vastly different meanings, while the mistyped word ``bananna'' probably means the same as ``banana''.

To tackle the challenges of natural language, most modern text generation models use subword tokenization.
Subword tokenization uses encoding techniques to find common sequences of characters and establishes these as tokens.
One of the commonly used encoding techniques in NLP (and coincidentally the one used in the GPT-n family) is byte-pair encoding (BPE), which was originally introduced as a data compression algorithm in \cite{BPE1994} and proposed as an encoding algorithm in neural machine translation in \cite{BPENMT2016}.
With subword tokenization, a token may not only be a single character or a full word of a given language, but also a substring of a word.
The use of subword tokens allows a model to better predict relationships between words.
For example, the words ``do'' and ``don't'' are probably closely related.
With subword tokens, a tokenizer can parse ``don't'' as ``do$||$n't''. 
An NLP model might then learn that the token ``n't'' can be used to negate a verb.
While this approach makes it easier for the model to learn relationships between different words, there may exist multiple tokenizations for a given word, a pitfall that will be important when we discuss reliability in \autoref{chap:reliability}.

After the tokenizer has encoded the history as a vector of tokens it is passed as input to the generative model.
The model's output is a \emph{logits vector} $z \in \mathbb{Z}^{|\mathcal{T}|}$ with components $z_i$ for each token $t_i$ in the model's token set.
A logits vector is a vector of unweighted activations of the model's underlying layers, where a higher value in a component represents a stronger activation and therefore a higher likelihood of a token to appear as the next token in the sequence.
Since the logits vector is unweighted, it might contain positive and negative values, which don't sum up nicely as they would in a probability distribution we can sample from.
To get a probability distribution from a logits vector $z \in \mathbb{Z}^{|\mathcal{T}|}$ we can use the softmax function $\sigma$, which transforms a logits vector of some dimension $\ell$ into another vector of the same dimension representing the probability mass function for the components of the logits vector:

$$\sigma \colon \mathbb{Z}^{\ell} \rightarrow [0,1]^{\ell},~ \sigma(z) = \left(\frac{e^{z_i}}{\sum_{j=1}^{\ell} e^{z_j}}\right)_{1 \leq i \leq \ell}.$$

When we combine all the previous steps, we get the black box for our ML model as illustrated in \autoref{fig:generative-network}.

\begin{figure}[htpb]
  \centering
  \begin{tikzpicture}
            \node[rectangle, draw=black, minimum width=4cm, minimum height=4cm] (oracle) {$\mathcal{GPT}$};
            \node[right=of oracle, rectangle, draw=black, minimum width=2cm, minimum height=4cm] (sigmoid) {$\sigma$};
            \node[left=of oracle, rectangle, draw=black, minimum width=2cm, minimum height=2cm] (transform) {Tokenizer};
            \node[left=of transform] (history) {$h$};
            \node[above right=-5mm and 1.2cm of sigmoid] (t1) {$t_1$};
            \node[below=5mm of t1] (t2) {$t_2$};
            \node[below=5mm of t2] (t3) {$t_3$};
            \node[below=5mm of t3] (t4) {$\dots$};
            \node[below=5mm of t4] (tn) {$t_{|\mathcal{T}|}$};
            \node[draw,dashed,inner sep=2mm,label={$Next_{\mathcal{GPT}}$},fit=(sigmoid) (oracle) (transform)] {};
            \draw[->] (transform) -- node[midway,above] {$v$} (oracle);
            \draw[->] (history) -- (transform);
            \draw[->] (oracle) -- node[midway,above] {$z$} (sigmoid);
            \draw[->] (sigmoid) -- node[midway,below] {$p_1$} (t1);
            \draw[->] (sigmoid) -- node[midway,below] {$p_2$} (t2);
            \draw[->] (sigmoid) -- node[midway,below] {$p_3$} (t3);
            \draw[->] (sigmoid) -- node[midway,below] {$p_{|\mathcal{T}|}$} (tn);
  \end{tikzpicture}
  \caption{
Generative ML Model as a pipeline $Next_{\mathcal{GPT}}$ as it is used in the Meteor stegosystem, which takes as input a history $h \in \mathcal{H}$ and outputs a tuple $(\mathcal{T}, \mathcal{P})$, where $\mathcal{P} = \{p_1, p_2, \dots, p_{|\mathcal{T}|}\}$ is a probability distribution over tokens $\mathcal{T} = \{ t_1, t_2, \dots, t_{|\mathcal{T}|}\}$. 
The history $h$ is a string of text which is transformed to a vector $v \in \mathcal{T}^\zeta$ (with model-specific $\zeta$) of token indices using a tokenizer.
The prediction model $\mathcal{GPT}$ takes $v$ as input and returns a logits vector $z \in \mathbb{Z}^{|\mathcal{T}|}$ of unscaled outputs.
This logits vector is then transformed to a probability distribution $p \in [0,1]^{|\mathcal{T}|}$ using the softmax function.
  The component $p_i$ represents the probability of token $t_i \in \mathcal{T}$ to be the next token in the sequence $s$ given the history $v$ of length $\zeta$: $p_i = Pr[s_{\zeta+1}=t_i|s=v]$.
}
  \label{fig:generative-network}
\end{figure}

To generate the next token in the sequence, we can sample according to the generated distribution.
After sampling from the distribution, we append the sampled token to the history and repeat the process until sufficiently many tokens have been generated.
We then convert the token outputs to a string using the tokenizer to get the generated text.
For an example of GNN model outputs using GPT-2, see \exampleref{example:gpt2-output-sample}.

\begin{example}[Example of GPT-2 model output for initial history ``What is steganography in computer science?'']
It is a form of digital communication that uses data to hide or obscure messages from a third party. It has become so common that today it is one of the most popular ways of communicating online. Steganography can be used to send photos or videos to another user or to hide messages from someone who will not discover the sender is actually the sender. As a result, it has become a real phenomenon known as "steganography".
  \label{example:gpt2-output-sample}
\end{example}

To use GPT-2 with the Python programming language, we can use the Hugging Face Transformers library together with PyTorch.
PyTorch \cite{PyTorch} is a machine learning framework that implements high performance operations needed for machine learning with support for hardware acceleration.
The Hugging Face Transformers library \cite{HFTransformers} is built upon PyTorch and gives an API for easy access to several pretrained machine learning models.
In its simplest form, the pipeline API generates texts using not only GPT-2 but many other generative models.
A Python program which generates the text in \exampleref{example:gpt2-output-sample} can be found in \autoref{alg:gpt2-demo}.

While the use of the pipeline API is very convenient to develop simple text generation or question answering programs, it abstracts some steps that we will need to intercept for use in a stegosystem.
A more sophisticated way to generate text using GPT-2 can be found in \autoref{alg:gpt2-logits-demo}.
Its output resembles that shown in \exampleref{example:gpt2-output-sample}. 
This code example shows how we can:

\begin{itemize}
  \item use a tokenizer to convert strings to a vector of tokens
  \item use the model to get a logits vector
  \item transform a logits vector into a probability mass function using the softmax function
  \item sample the next token from the distribution
  \item convert a vector of tokens back to a string using the tokenizer
\end{itemize}

As we can see in \autoref{alg:gpt2-logits-demo}, a pseudorandom value \emph{sample} is generated to select the next token \emph{selection} from the distribution.
This is what the Meteor stegosystem will abuse.
There, the sending party will replace the randomness with a crafted value to embed a hidden message.
While that value seems random to an observer, a recipient who possesses the secret key is able to recover the hidden message.


\lstinputlisting[style=code, language=Python, caption={Python text generator using the Hugging Face Transformers Pipeline API}, label={alg:gpt2-demo}]{code/pipeline_gpt2.py}

\lstinputlisting[style=code, language=Python, caption={Python text generator using the Hugging Face Transformers library}, label={alg:gpt2-logits-demo}]{code/logits_demo_gpt2.py}

\newpage

\section{Meteor Stegosystem}
\label{sec:meteor}
In their paper \cite{Meteor2021}, the authors present the Meteor stegosystem, a provably secure steganographic system for realistic distributions approximated by ML models.
The central idea behind Meteor is that an encoding party Alice manipulates the randomness used in a sampling scheme to embed hidden text in a token sequence.
The decoding party Bob receives the generated stegotext from Alice and recovers a prefix of the random value used during sampling.
If the value which replaces the randomness for sampling is indistinguishable from an actually random value, the steganographic protocol is secure.

While it is easily possible for the encoding party to manipulate the sampling because they control the randomness, it is intuitively unclear whether the decoding party can extract a prefix of the randomness from the stegotext to recover parts of the hiddentext message.
Luckily, with many generative neural networks, such as GPT-2, the decoding party can do that.
The Meteor authors have introduced the concept of Ranged Randomness Recoverable Sampling Schemes, or RRRSS.
This definition formally models the desired behavior described above.
\begin{definition}[Ranged Randomness Recoverable Sampling Scheme, \cite{Meteor2021}]
  \label{def:rrrss}
  We call 
  $(Sample_{\mathcal{M}}^\beta, Recover_{\mathcal{M}}^\beta, \mathcal{M}, \beta, \mathcal{H})$ 
  with ppt. algorithms
  $Sample_{\mathcal{M}}^\beta \colon \mathcal{H} \times \{0,1\}^\beta \rightarrow \Delta$ 
  and 
  $Recover_{\mathcal{M}}^\beta \colon \mathcal{H} \times \Delta \rightarrow \mathcal{P}(\{0,1\}^\beta)$ 
  a \emph{Ranged Randomness Recoverable Sampling Scheme} (RRRSS) over distribution $\mathcal{M}$ with precision $\beta \in \mathbb{N}$ and histories $\mathcal{H}$, if:
  
  \begin{enumerate}
    \item $Sample_{\mathcal{M}}^\beta(h, r)$, on history $h \in \mathcal{H}$ and randomness $r \in \{0,1\}^\beta$, samples an output $s \in \Delta$ from the underlying distribution $\mathcal{M}$.
    \item $Recover_{\mathcal{M}}^\beta(h, s)$, on history $h \in \mathcal{H}$ and sample $s \in \Delta$ returned from $Sample_{\mathcal{M}}^\beta$, outputs the set $\mathcal{R} = \{ r \in \{0,1\}^\beta | Sample_{\mathcal{M}}^\beta(\mathcal{H}, r) = s \}$ of possible values for $r$.
  \end{enumerate}
\end{definition}
With the definition of an RRRSS, the authors define sampling and recovery algorithms for the GPT-2 model.
While they define the $Sample$ and $Recover$ algorithms for GPT-2 only, they argue that many generative models satisfy the RRRSS property.

\begin{Pseudocode}[caption={
RRRSS $Sample$ algorithm for GPT-2 \cite{Meteor2021}.
$Sample$ produces, given a history $h$ and a value $r$, the next token sampled according to $r$ from the distribution for the next token generated by $Next_{\mathcal{GPT}}(h)$.
}]
algorithm $Sample_{\mathcal{GPT}}^\beta(  h, r)$
  Output: Token $t$
  $\mathcal{T}, \mathcal{P} \leftarrow Next_{\mathcal{GPT}}(h)$
  $cuml \leftarrow 0$
  for $i \in \{ 0, 1, \dots, | \mathcal{T} - 1 | \}$ do
    $cuml \leftarrow cuml + \mathcal{P}[i]$
    if $cuml > r$ then
      Output $t \leftarrow \mathcal{T}[i]$
  Output $t \leftarrow \mathcal{T}[|\mathcal{T}|-1]$
\end{Pseudocode}

\begin{Pseudocode}[caption={
RRRSS $Recover$ algorithm for GPT-2 \cite{Meteor2021}.
Given a history $h$ and a sample $s$, return a set of possible random values used to generate $s$ with history $h$.
}]
algorithm $Recover_{\mathcal{GPT}}^\beta(h, s)$
  Output: Randomness set $\mathcal{R}$
  $\mathcal{T}, \mathcal{P} \leftarrow Next_{\mathcal{GPT}}(h)$
  $cuml \leftarrow 0$
  for $i \in \{ 0, 1, \dots, | \mathcal{T} - 1 | \}$ do
    if $\mathcal{T}[i] = s$ then
      Output $\mathcal{R} \leftarrow \{ r \in \{ 0, 1\}^\beta | cuml \leq r < cuml + \mathcal{P}[i] \}$
    $cuml \leftarrow cuml + \mathcal{P}[i]$
  Output $\mathcal{R} \leftarrow \emptyset$
\end{Pseudocode}

Here, $Next_{\mathcal{GPT}}$ is the algorithm which generates the probability distribution for the next token given a history $h$ as shown in \autoref{fig:generative-network}.
The tokens are denoted by $\mathcal{T}$ and probabilities are denoted by $\mathcal{P}$.
The $Sample_{\mathcal{GPT}}^\beta$ algorithm cumulates the token probabilities $p_i$ into $cuml$ until the threshold $r$ has been reached.
Then the current token $t_i$ is the token to be sampled from the distribution.
We can see that the order of tokens is relevant for the recovery to succeed, so we expect $Next_{\mathcal{GPT}}$ to return probabilities in a somehow ordered fashion.

Now that a RRRSS scheme for GPT-2 is established, we can commence with the definition of the stegosystem $\mathcal{S} = \left( Encode_{\mathcal{M}}^\beta, Decode_{\mathcal{M}}^\beta \right)$.
The authors described those algorithms as follows while introducing three helper functions $LenPrefix^\beta$, $Prefix^\beta$ and $KeyGen_{\mathcal{M}}^\beta$:


\begin{Pseudocode}[caption={$Encode$ algorithm \cite{Meteor2021}}, label={alg:encode}]
algorithm $Encode_{\mathcal{M}}^\beta(k_{prg}, m, h)$
  Output: Stegotext message $c$
  $c \leftarrow \epsilon,~ n \leftarrow 0,~ i \leftarrow 0$
  while $n < |m|$ do
    $mask \leftarrow PRG.Next(k_{prg})$
    $r \leftarrow m[n:n+\beta] \oplus mask$
    $c_i \leftarrow Sample_{\mathcal{M}}^\beta(h, r)$
    $\mathcal{R} = Recover_{\mathcal{M}}^\beta(h, c_i)$
    $n_i \leftarrow LenPrefix^\beta(\mathcal{R})$
    $c \leftarrow c || c_i, n \leftarrow n+n_i, h \leftarrow h||c_i,~ i \leftarrow i + 1$
  Output $c$
\end{Pseudocode}

$Encode$ uses the PRG to sample a pseudorandom mask and applies that mask to the next $\beta$ bits of $m$ into $r$.
Since $mask$ is pseudorandom, this makes $r$ pseudorandom as well.
We then use $r$ to sample from the RRRSS distribution to get the next token $c_i$.
Afterwards, $Recover_{\mathcal{M}}^\beta$ and $LenPrefix^\beta$ are queried to get the number of bits $n_i$ recoverable from $r$.
This procedure is repeated until $|m|$ bits of hiddentext have been encoded.
Consider that the output of $Encode$ is deterministic, i.e. for a given $m$, $k_{prg}$ and $h$, the same stegotext $c$ will be generated, which will be important when we discuss security in \autoref{chap:security}.


\begin{Pseudocode}[float, caption={$Decode$ algorithm \cite{Meteor2021}}, label={alg:decode}]
algorithm $Decode_{\mathcal{M}}^\beta(k_{prg}, c, h)$
  Output: Plaintext message $m$
  $m \leftarrow \epsilon$
  Parse $c$ as $c_0 || c_1 || \dots || c_{\tau}$
  for $i \in \{ 0, 1, \dots, \tau \}$ do
    $\mathcal{R} = Recover_{\mathcal{M}}^\beta(h, c_i)$
    $m_i \leftarrow Prefix^\beta(\mathcal{R})$
    $mask \leftarrow PRG.Next(k_{prg})$
    $m \leftarrow m || (m_i \oplus mask[0: |m_i|]$
    $h \leftarrow h||c_i$
  Output $m$
\end{Pseudocode}

\begin{Pseudocode}[caption={$LenPrefix$ algorithm \cite{Meteor2021}}]
algorithm $LenPrefix^\beta(\mathcal R = \{ r_1, \dots r_n \})$
  Output: Length $l$
  $l \leftarrow 1$
  while $l < \beta$ do
    if $\exists i, j \in \{ 1, \dots, n \}$ such that $r_i[0: l] \neq r_j[0:  l]$ then
      Output $l-1$
    $l \leftarrow l+1$
  Output $l$
\end{Pseudocode}%
%
\begin{Pseudocode}[caption={$Prefix$ algorithm \cite{Meteor2021}}]
algorithm $Prefix^\beta(  \mathcal R = \{ r_1, \dots r_n \})$
  Output: Bit String $p$
  Output $p \leftarrow r_1[0: LenPrefix^\beta(\mathcal{R})]$
\end{Pseudocode}%
%
\begin{Pseudocode}[caption={$KeyGen$ algorithm \cite{Meteor2021}}]
algorithm $KeyGen_{\mathcal{M}}^\beta(1^\lambda)$
  Output: Key $k_{prg}$
  Output $k_{prg} \leftarrow PRG.Setup(1^\lambda)$
\end{Pseudocode}

$Decode$ takes a stegotext $c$ and decodes it to the hiddentext message.
It repeatedly uses $Recover_{\mathcal{M}}^\beta$ and $Prefix^\beta$ to recover bits from the pseudorandom value used to sample from the distribution.
It then generates a mask from the PRG (which will be the same mask generated in the encoding step) to decrypt the original message.

For reliability, the authors introduce a definition of \emph{steganographic correctness} that is roughly equivalent to our definition of reliability in \autoref{def:reliability} without giving a formal proof for the Meteor stegosystem.
In \autoref{chap:reliability}, we will argue that the Meteor stegosystem is in fact not reliable using GPT-2 as underlying distribution and propose a modification to Meteor that improves reliability while -- in the worst case -- introducing computational overhead exponential in stegotext length.

In their paper, the Meteor authors provide performance statistics for different models and hardware setups.
About 3 bits of hiddentext can be encoded in a stegotext token when using the GPT-2 model.
For other generative models, such as a model that generates HTTP headers or a model trained on Wikipedia articles, the rate is significantly lower at less than 1 bit of hiddentext per token.
The results with the GPT-2 model are consistent with experiments conducted as part of this thesis.

\begin{table}[h!]
  \begin{tabular}{l||c|c|c|c}
    Model & GPU (sec) & CPU (sec) & Stegotext Length (bytes) & Rate (bits/token) \\
    \hline
    GPT-2        & 13.089 & 82.214  & 1976 & 3.09 \\
    Wikipedia    & 19.791 & 46.583  & 2002 & 0.64 \\
    HTTP Headers & 49.380 & 103.280 & 6144 & 0.21 \\
  \end{tabular}
  \caption{Meteor encoding statistics for a 160-byte hiddentext \cite{Meteor2021}.}
\end{table}

Meteor's security proof is also outlined by the authors.
They argue that a steganographic protocol is secure if no ppt. adversary $\mathcal{A}$ is able to distinguish the output of the stegosystem from the output of a random oracle $\mathcal{O}$ that samples from the distribution.
In \autoref{chap:security}, we will show Meteor's security against chosen hiddentext attackers with query complexity of one.
We will also show that Meteor is insecure against chosen hiddentext attackers with two or more queries.
